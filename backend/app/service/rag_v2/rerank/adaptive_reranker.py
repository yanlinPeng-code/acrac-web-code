import time
from typing import List, Dict, Any, Tuple, Optional
from app.schema.IntelligentRecommendation_schemas import PatientInfo, ClinicalContext
from app.service.rag_v2.base import Base
from app.utils.helper.helper import safe_parse_llm_response, safe_process_recommendation_grades
from app.utils.logger.simple_logger import get_logger

from app.service.rag_v2.prompt.base_prompt import BasePrompt

logger=get_logger(__name__)
class AdaptiveThresholdStrategy:
    """åŸºç¡€é˜ˆå€¼ç­–ç•¥"""

    def __init__(self):
        self.threshold_config = {
            'token_threshold':  4096,
            'max_scenarios_single_call': 5,
            'max_total_recommendations': 30,
            'max_avg_recommendations_per_scenario': 8
        }

        self.weights = {
            'token_ratio': 0.5,
            'scenario_ratio': 0.2,
            'total_recommendations_ratio': 0.2,
            'avg_recommendations_ratio': 0.1
        }

    def should_use_concurrent(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            estimated_tokens: int
    ) -> Tuple[bool, Dict[str, Any]]:
        """å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶å‘å¤„ç†"""

        scenario_count = len(confirmed_scenarios)
        total_recommendations = sum(
            len(scenario.get('recommendations', []))
            for scenario in confirmed_scenarios
        )
        avg_recommendations = total_recommendations / max(scenario_count, 1)

        # è®¡ç®—å„ç»´åº¦æ¯”ç‡
        token_ratio = estimated_tokens / self.threshold_config['token_threshold']
        scenario_ratio = scenario_count / self.threshold_config['max_scenarios_single_call']
        total_rec_ratio = total_recommendations / self.threshold_config['max_total_recommendations']
        avg_rec_ratio = avg_recommendations / self.threshold_config['max_avg_recommendations_per_scenario']

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        composite_score = (
                token_ratio * self.weights['token_ratio'] +
                scenario_ratio * self.weights['scenario_ratio'] +
                total_rec_ratio * self.weights['total_recommendations_ratio'] +
                avg_rec_ratio * self.weights['avg_recommendations_ratio']
        )

        # ç¡¬æ€§æ¡ä»¶
        hard_conditions = [
            token_ratio > 1.0,
            scenario_ratio > 1.5,
            total_rec_ratio > 2.0,
            avg_rec_ratio > 1.8
        ]

        use_concurrent = composite_score > 1.0 or any(hard_conditions)

        decision_metrics = {
            'composite_score': composite_score,
            'dimensions': {
                'tokens': {
                    'value': estimated_tokens,
                    'threshold': self.threshold_config['token_threshold'],
                    'ratio': token_ratio
                },
                'scenarios': {
                    'value': scenario_count,
                    'threshold': self.threshold_config['max_scenarios_single_call'],
                    'ratio': scenario_ratio
                },
                'total_recommendations': {
                    'value': total_recommendations,
                    'threshold': self.threshold_config['max_total_recommendations'],
                    'ratio': total_rec_ratio
                },
                'avg_recommendations': {
                    'value': avg_recommendations,
                    'threshold': self.threshold_config['max_avg_recommendations_per_scenario'],
                    'ratio': avg_rec_ratio
                }
            },
            'hard_conditions_triggered': [
                'token_exceeded' if token_ratio > 1.0 else None,
                'scenarios_exceeded' if scenario_ratio > 1.5 else None,
                'total_recommendations_exceeded' if total_rec_ratio > 2.0 else None,
                'avg_recommendations_exceeded' if avg_rec_ratio > 1.8 else None
            ],
            'decision_reason': self._get_decision_reason(composite_score, hard_conditions)
        }

        return use_concurrent, decision_metrics

    def _get_decision_reason(self, composite_score: float, hard_conditions: List[bool]) -> str:
        """ç”Ÿæˆå†³ç­–ç†ç”±"""
        if composite_score > 1.0:
            return f"ç»¼åˆè¯„åˆ†{composite_score:.2f}è¶…è¿‡é˜ˆå€¼1.0"

        triggered = [cond for cond in hard_conditions if cond]
        if triggered:
            return f"è§¦å‘{len(triggered)}ä¸ªç¡¬æ€§æ¡ä»¶"

        return f"ç»¼åˆè¯„åˆ†{composite_score:.2f}æœªè¶…è¿‡é˜ˆå€¼"


class LearningThresholdStrategy(AdaptiveThresholdStrategy):
    """åŸºäºå†å²æ€§èƒ½å­¦ä¹ çš„é˜ˆå€¼ç­–ç•¥"""

    def __init__(self):
        super().__init__()
        self.performance_history = []
        self.learning_enabled = True

    def update_based_on_performance(
            self,
            decision_metrics: Dict[str, Any],
            actual_processing_time: float,
            success: bool,
            strategy_used: str
    ):
        """æ ¹æ®å®é™…æ€§èƒ½æ›´æ–°é˜ˆå€¼"""
        if not self.learning_enabled:
            return

        record = {
            'decision_metrics': decision_metrics,
            'processing_time': actual_processing_time,
            'success': success,
            'strategy_used': strategy_used,
            'timestamp': time.time()
        }

        self.performance_history.append(record)

        # ä¿ç•™æœ€è¿‘100æ¡è®°å½•
        if len(self.performance_history) > 100:
            self.performance_history = self.performance_history[-100:]

        # å®šæœŸè°ƒæ•´é˜ˆå€¼
        if len(self.performance_history) % 20 == 0:
            self._adjust_thresholds_based_on_history()

    def _adjust_thresholds_based_on_history(self):
        """åŸºäºå†å²æ€§èƒ½è°ƒæ•´é˜ˆå€¼"""
        single_call_records = [r for r in self.performance_history if r['strategy_used'] == 'single']
        concurrent_records = [r for r in self.performance_history if r['strategy_used'] == 'concurrent']

        if len(single_call_records) < 5 or len(concurrent_records) < 5:
            return

        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        try:
            avg_single_time = sum(r['processing_time'] for r in single_call_records) / len(single_call_records)
            avg_concurrent_time = sum(r['processing_time'] for r in concurrent_records) / len(concurrent_records)

            # è®¡ç®—æˆåŠŸç‡
            single_success_rate = sum(1 for r in single_call_records if r['success']) / len(single_call_records)
            concurrent_success_rate = sum(1 for r in concurrent_records if r['success']) / len(concurrent_records)

            # æ ¹æ®æ€§èƒ½å·®å¼‚è°ƒæ•´é˜ˆå€¼
            time_ratio = avg_single_time / avg_concurrent_time if avg_concurrent_time > 0 else 1.0
            success_ratio = single_success_rate / concurrent_success_rate if concurrent_success_rate > 0 else 1.0

            # å¦‚æœå•æ¬¡è°ƒç”¨æ€§èƒ½æ›´å¥½ï¼Œé€‚å½“æé«˜é˜ˆå€¼
            if time_ratio < 0.8 and success_ratio > 0.9:
                self.threshold_config['token_threshold'] = min(
                    self.threshold_config['token_threshold'] * 1.1,
                    8000
                )
                logger.info(f"ğŸ“ˆ åŸºäºæ€§èƒ½æ•°æ®æé«˜tokené˜ˆå€¼è‡³: {self.threshold_config['token_threshold']}")

            # å¦‚æœå¹¶å‘è°ƒç”¨æ€§èƒ½æ›´å¥½ï¼Œé€‚å½“é™ä½é˜ˆå€¼
            elif time_ratio > 1.2 or success_ratio < 0.8:
                self.threshold_config['token_threshold'] = max(
                    self.threshold_config['token_threshold'] * 0.9,
                    2000
                )
                logger.info(f"ğŸ“‰ åŸºäºæ€§èƒ½æ•°æ®é™ä½tokené˜ˆå€¼è‡³: {self.threshold_config['token_threshold']}")
        except Exception as e:
            logger.warning(f"è°ƒæ•´é˜ˆå€¼æ—¶å‡ºé”™: {e}")
class AdaptiveReranker(BasePrompt):
    """è‡ªé€‚åº”æ¨èå¼•æ“"""

    def __init__(self, environment: str = "production", use_adaptive: bool = True):
        super().__init__()
        self.environment = environment
        self.use_adaptive = use_adaptive


        # åˆå§‹åŒ–ç­–ç•¥
        if use_adaptive:
            self.strategy = LearningThresholdStrategy()
            logger.info("ğŸ”„ å¯ç”¨è‡ªé€‚åº”ç­–ç•¥")
        else:
            self.strategy = AdaptiveThresholdStrategy()
            logger.info("âš¡ ä½¿ç”¨å›ºå®šé˜ˆå€¼ç­–ç•¥")

        self._initialize_strategy()

    def _initialize_strategy(self):
        """åˆå§‹åŒ–ç­–ç•¥é…ç½®"""
        env_config = self.get_environment_specific_config(self.environment)
        self.strategy.threshold_config.update(env_config)
        logger.info(f"âœ… ç­–ç•¥åˆå§‹åŒ–å®Œæˆï¼Œç¯å¢ƒ: {self.environment}")
    def get_environment_specific_config(self, environment: str) -> Dict[str, Any]:
        """è·å–ç¯å¢ƒç‰¹å®šé…ç½®"""
        configs = {
            'development': {
                'token_threshold': 4000,
                'max_scenarios_single_call': 3,
                'max_total_recommendations': 20,
                'max_avg_recommendations_per_scenario': 6,
            },
            'production': {
                'token_threshold': 4000,
                'max_scenarios_single_call': 5,
                'max_total_recommendations': 30,
                'max_avg_recommendations_per_scenario': 10,
            },
            'local-qwen': {
                'token_threshold': 4000,
                'max_scenarios_single_call': 4,
                'max_total_recommendations': 25,
                'max_avg_recommendations_per_scenario': 7,
            }
        }
        return configs.get(environment, configs['production'])

    def estimate_tokens_with_tiktoken(self, text: str, model_name: str = "cl100k_base") -> int:
        """ä½¿ç”¨tiktokenè®¡ç®—tokenæ•°é‡"""
        try:
            try:
                return len(self.tokenizer.encode(text))
            except KeyError:
                import qwen_token_counter
                encoding = qwen_token_counter.get_token_count(text)
                return encoding
        except ImportError:
            logger.warning("tiktokenæœªå®‰è£…ï¼Œä½¿ç”¨å›é€€ä¼°ç®—æ–¹æ³•")
            return self._estimate_tokens_fallback(text)

    def _estimate_tokens_fallback(self, text: str) -> int:
        """tiktokenä¸å¯ç”¨æ—¶çš„å›é€€ä¼°ç®—æ–¹æ³•"""
        import re
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
        numbers = len(re.findall(r'\d+', text))
        punctuation = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text))
        spaces = len(re.findall(r'\s', text))

        estimated_tokens = (
                chinese_chars * 2.3 +
                english_words * 1.3 +
                numbers * 0.8 +
                punctuation * 0.5 +
                spaces * 0.1
        )
        return int(estimated_tokens)

    def _build_single_call_prompt(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_scenarios:int,
            max_recommendations_per_scenario: int,
            direct_return: bool,

    ) -> str:
        """æ„å»ºå•æ¬¡è°ƒç”¨æç¤ºè¯"""

        patient_info_content = self.build_patient_context(patient_info)
        clinical_context_content = self.build_clinical_context(clinical_context)
        scenarios_content = self.build_scenarios_with_recommend(confirmed_scenarios)
        task_instruction = self.build_task_instruction(
            max_scenarios, max_recommendations_per_scenario, direct_return
        )

        return f"{patient_info_content}\n{clinical_context_content}\n{scenarios_content}\n{task_instruction}"

    async def get_recommendations(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_scenarios:int=3,
            max_recommendations_per_scenario: int = 3,
            direct_return:bool=False,
            use_adaptive: Optional[bool] = None,  # å¯è¦†ç›–åˆå§‹è®¾ç½®
            max_concurrent: int = 3,
    ) -> List[Dict[str, Any]]:
        """ä¸»å…¥å£å‡½æ•° - è·å–æ¨èç»“æœ"""

        # ç¡®å®šæ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”ç­–ç•¥
        adaptive_mode = use_adaptive if use_adaptive is not None else self.use_adaptive
        # 1. è®¡ç®—tokenæ•°
        single_prompt = self._build_single_call_prompt(
            confirmed_scenarios,
            patient_info,
            clinical_context,
            max_scenarios,
            max_recommendations_per_scenario,
            direct_return
        )



        if adaptive_mode:
            return await self._get_recommendations_adaptive(
                confirmed_scenarios, patient_info, clinical_context,
                max_recommendations_per_scenario, max_concurrent,single_prompt,direct_return
            )
        else:
            # éè‡ªé€‚åº”æ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨å•æ¬¡è°ƒç”¨
            return await self._get_recommendations_single_call(
                confirmed_scenarios, patient_info, clinical_context,
                max_recommendations_per_scenario, len(confirmed_scenarios),single_prompt,direct_return
            )
    async def _get_recommendations_adaptive(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int,
            max_concurrent: int,
            single_prompt: str,
            direct_return:bool,
    ) -> List[Dict[str, Any]]:
        """è‡ªé€‚åº”æ¨¡å¼å¤„ç†"""

        start_time = time.time()
        estimated_tokens = self.estimate_tokens_with_tiktoken(single_prompt)
        if not direct_return:
            # 2. ä½¿ç”¨ç­–ç•¥å†³ç­–
            use_concurrent, decision_metrics = self.strategy.should_use_concurrent(
                confirmed_scenarios, estimated_tokens
            )

            # 3. è®°å½•å†³ç­–è¯¦æƒ…
            self._log_decision_metrics(decision_metrics, estimated_tokens)

            # 4. æ‰§è¡Œç›¸åº”ç­–ç•¥å¹¶è®°å½•æ€§èƒ½
            try:
                if use_concurrent:
                    logger.info("âš¡ ä½¿ç”¨å¹¶å‘å¤„ç†ç­–ç•¥")
                    results = await self._get_recommendations_for_confirmed_scenarios_concurrent(
                        confirmed_scenarios, patient_info, clinical_context,
                        max_recommendations_per_scenario, max_concurrent
                    )
                    strategy_used = 'concurrent'
                else:
                    logger.info("ğŸ”„ ä½¿ç”¨å•æ¬¡è°ƒç”¨ç­–ç•¥")
                    results = await self._get_recommendations_single_call(
                        confirmed_scenarios, patient_info, clinical_context,
                        max_recommendations_per_scenario, len(confirmed_scenarios),single_prompt
                    )
                    strategy_used = 'single'

                processing_time = time.time() - start_time
                success = True

            except Exception as e:
                logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                processing_time = time.time() - start_time
                results = self._fallback_for_confirmed_scenarios(confirmed_scenarios)
                success = False
                strategy_used = 'single' if not use_concurrent else 'concurrent'

            # 5. å¦‚æœæ˜¯å­¦ä¹ ç­–ç•¥ï¼Œæ›´æ–°æ€§èƒ½æ•°æ®
            if isinstance(self.strategy, LearningThresholdStrategy):
                self.strategy.update_based_on_performance(
                    decision_metrics=decision_metrics,
                    actual_processing_time=processing_time,
                    success=success,
                    strategy_used=strategy_used
                )

            return results
        else:
             total_token=self.strategy.threshold_config["token_threshold"]
             if total_token-1500<estimated_tokens:
                 #é‡æ–°è§„æ•´ç°æœ‰æ•°æ®ç»“æ„é‡æ–°æ„æˆæç¤ºè¯
                 results = await self._get_recommendations_single_call(
                     confirmed_scenarios, patient_info, clinical_context,
                     max_recommendations_per_scenario, len(confirmed_scenarios), single_prompt,direct_return
                 )
                 return results
             else:
                 # 2. ä½¿ç”¨ç­–ç•¥å†³ç­–
                 use_concurrent, decision_metrics = self.strategy.should_use_concurrent(
                     confirmed_scenarios, estimated_tokens
                 )

                 # 3. è®°å½•å†³ç­–è¯¦æƒ…
                 self._log_decision_metrics(decision_metrics, estimated_tokens)

                 # 4. æ‰§è¡Œç›¸åº”ç­–ç•¥å¹¶è®°å½•æ€§èƒ½
                 try:
                     if use_concurrent:
                         logger.info("âš¡ ä½¿ç”¨å¹¶å‘å¤„ç†ç­–ç•¥")
                         results = await self._get_recommendations_for_confirmed_scenarios_concurrent(
                             confirmed_scenarios, patient_info, clinical_context,
                             max_recommendations_per_scenario, max_concurrent
                         )
                         strategy_used = 'concurrent'
                     else:
                         logger.info("ğŸ”„ ä½¿ç”¨å•æ¬¡è°ƒç”¨ç­–ç•¥")
                         results = await self._get_recommendations_single_call(
                             confirmed_scenarios, patient_info, clinical_context,
                             max_recommendations_per_scenario, len(confirmed_scenarios), single_prompt
                         )
                         strategy_used = 'single'

                     processing_time = time.time() - start_time
                     success = True

                 except Exception as e:
                     logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                     processing_time = time.time() - start_time
                     results = self._fallback_for_confirmed_scenarios(confirmed_scenarios)
                     success = False
                     strategy_used = 'single' if not use_concurrent else 'concurrent'

                 # 5. å¦‚æœæ˜¯å­¦ä¹ ç­–ç•¥ï¼Œæ›´æ–°æ€§èƒ½æ•°æ®
                 if isinstance(self.strategy, LearningThresholdStrategy):
                     self.strategy.update_based_on_performance(
                         decision_metrics=decision_metrics,
                         actual_processing_time=processing_time,
                         success=success,
                         strategy_used=strategy_used
                     )

                 return results
    def _log_decision_metrics(self, decision_metrics: Dict[str, Any], estimated_tokens: int):
        """è®°å½•å†³ç­–æŒ‡æ ‡"""
        metrics = decision_metrics['dimensions']

        logger.info("ğŸ“Š å†³ç­–åˆ†æ:")
        logger.info(
            f"  Tokenæ•°: {estimated_tokens}/{metrics['tokens']['threshold']} ({metrics['tokens']['ratio'] * 100:.1f}%)")
        logger.info(
            f"  åœºæ™¯æ•°: {metrics['scenarios']['value']}/{metrics['scenarios']['threshold']} ({metrics['scenarios']['ratio'] * 100:.1f}%)")
        logger.info(
            f"  æ€»æ¨èæ•°: {metrics['total_recommendations']['value']}/{metrics['total_recommendations']['threshold']} ({metrics['total_recommendations']['ratio'] * 100:.1f}%)")
        logger.info(
            f"  å¹³å‡æ¨èæ•°: {metrics['avg_recommendations']['value']:.1f}/{metrics['avg_recommendations']['threshold']} ({metrics['avg_recommendations']['ratio'] * 100:.1f}%)")
        logger.info(f"  ç»¼åˆè¯„åˆ†: {decision_metrics['composite_score']:.2f}")
        logger.info(f"  å†³ç­–ç†ç”±: {decision_metrics['decision_reason']}")


    async def _get_recommendations_for_confirmed_scenarios_concurrent(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int,
            max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """å¹¶å‘å¤„ç†"""
        """å¹¶å‘å¤„ç†å·²ç¡®è®¤åœºæ™¯çš„æ¨èé¡¹ç›®åˆ†çº§
    
            Args:
                confirmed_scenarios: å·²ç¡®è®¤çš„åœºæ™¯åˆ—è¡¨
                patient_info: æ‚£è€…ä¿¡æ¯
                clinical_context: ä¸´åºŠä¸Šä¸‹æ–‡
                max_recommendations_per_scenario: æ¯ä¸ªåœºæ™¯æœ€å¤§æ¨èé¡¹ç›®æ•°
                max_concurrent: æœ€å¤§å¹¶å‘æ•°
    
            Returns:
                æ ¼å¼åŒ–çš„æ¨èç»“æœï¼Œä¸åŸå‡½æ•°æ ¼å¼ç›¸åŒ
            """
        import asyncio
        from typing import List, Dict, Any

        async def process_single_scenario(scenario_data: Dict[str, Any], scenario_index: int) -> Dict[str, Any]:
            """å¤„ç†å•ä¸ªåœºæ™¯çš„æ¨èé¡¹ç›®åˆ†çº§"""
            try:
                # æ„å»ºå•ä¸ªåœºæ™¯çš„æç¤ºè¯
                prompt = self._build_single_scenario_prompt(
                    scenario_data,
                    scenario_index,
                    patient_info,
                    clinical_context,
                    max_recommendations_per_scenario
                )

                # è°ƒç”¨LLM
                response = await self.ai_service._call_llm(prompt)

                # è§£æJSONç»“æœ
                import re
                import json

                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if not json_match:
                    logger.error(f"åœºæ™¯{scenario_index} LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
                    return self._fallback_single_scenario(scenario_data, scenario_index)

                try:
                    result = json.loads(json_match.group())
                except json.JSONDecodeError as e:
                    logger.error(f"åœºæ™¯{scenario_index} JSONè§£æé”™è¯¯: {e}")
                    return self._fallback_single_scenario(scenario_data, scenario_index)

                # å¤„ç†åˆ†çº§æ¨èç»“æœ
                return self._process_single_scenario_result(result, scenario_data, scenario_index,
                                                            max_recommendations_per_scenario)

            except Exception as e:
                logger.error(f"å¤„ç†åœºæ™¯{scenario_index}æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                return self._fallback_single_scenario(scenario_data, scenario_index)

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)

        async def bounded_process(scenario_data, scenario_index):
            async with semaphore:
                return await process_single_scenario(scenario_data, scenario_index)

        # å¹¶å‘å¤„ç†æ‰€æœ‰åœºæ™¯
        tasks = [
            bounded_process(scenario_data, idx + 1)
            for idx, scenario_data in enumerate(confirmed_scenarios)
        ]

        single_scenario_results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        final_results = []
        for result in single_scenario_results:
            if isinstance(result, Exception):
                logger.error(f"åœºæ™¯å¤„ç†å¼‚å¸¸: {result}")
                continue
            if result:  # åªæ·»åŠ æœ‰æ•ˆç»“æœ
                final_choices=result.get("final_choices",[])
                if not final_choices:
                    procedures=result.get('graded_recommendations')["highly_recommended"]
                    res=[ p['procedure_details']['name_zh'] for p in procedures]
                    final_choices=res
                final_results.append(final_choices)

        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº

        # ç”Ÿæˆæ€»ä½“æ¨ç†
        res =await self._generate_overall_reasoning(patient_info,clinical_context,max_recommendations_per_scenario,final_results)

        # ä¸ºæ‰€æœ‰ç»“æœæ·»åŠ æ€»ä½“æ¨ç†


        # è®°å½•è¯¦ç»†çš„åˆ†çº§ç»Ÿè®¡
        logger.info(f"âœ… å¹¶å‘åœºæ™¯æ¨èåˆ†çº§å®Œæˆï¼Œå¤„ç†äº†{len(final_results)}ä¸ªåœºæ™¯")


        return res
    def _build_single_scenario_prompt(
            self,
            scenario_data: Dict[str, Any],
            scenario_index: int,
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int
    ) -> str:
        """ä¸ºå•ä¸ªåœºæ™¯æ„å»ºæç¤ºè¯"""

        scenario = scenario_data['scenario']
        recommendations = scenario_data.get('recommendations', [])

        patient_info_content = self.build_patient_context(patient_info)
        clinical_context_content = self.build_clinical_context(clinical_context)
        scenario_content = self._build_single_scenario_content(scenario_data, scenario_index)
        task_instruction = self._build_single_scenario_task_instruction(
            scenario_index,
            len(recommendations),
            max_recommendations_per_scenario
        )

        return f"""{patient_info_content}

        {clinical_context_content}

        {scenario_content}

        {task_instruction}"""
    async def _get_recommendations_single_call(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int,
            expected_scenario_count: int,
            single_prompt: str,
            direct_return:bool=False
    ) -> List[Dict[str, Any]]:
        """å•æ¬¡è°ƒç”¨å¤„ç†"""
        # è¿™é‡Œå®ç°å•æ¬¡LLMè°ƒç”¨é€»è¾‘
        # è¿”å›æ ¼å¼åŒ–çš„ç»“æœ
        # 2. æ ¹æ®tokenæ•°é€‰æ‹©ç­–ç•¥
        return await self._get_recommendations_single_call_by_llm(
                confirmed_scenarios,single_prompt,direct_return
            )
    async def _get_recommendations_single_call_by_llm(self, confirmed_scenarios,
                                                       single_prompt,direct_return):

            try:

                response = await self.ai_service._call_llm(single_prompt)

                if not direct_return:


                    # ä½¿ç”¨å¢å¼ºçš„JSONè§£æ
                    result = safe_parse_llm_response(response=response, expected_scenario_count=len(confirmed_scenarios))

                    if result is None:
                        logger.error("JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
                        return self._fallback_for_confirmed_scenarios(confirmed_scenarios)

                    # å¤„ç†é€‰ä¸­çš„åœºæ™¯æ•°æ®
                    selected_scenarios_data = result.get('selected_scenarios', [])
                    final_results = []

                    for selected_data in selected_scenarios_data:
                        scenario_index = selected_data.get('scenario_index')
                        scenario_id = selected_data.get('scenario_id')
                        grading_data = selected_data.get('recommendation_grades', {})
                        final_choices=selected_data.get("final_choices",[])
                        # éªŒè¯åœºæ™¯ç´¢å¼•èŒƒå›´
                        if not (1 <= scenario_index <= len(confirmed_scenarios)):
                            logger.warning(f"æ— æ•ˆçš„åœºæ™¯ç´¢å¼•: {scenario_index}ï¼Œè·³è¿‡è¯¥åœºæ™¯")
                            continue

                        # è·å–åŸå§‹åœºæ™¯æ•°æ®
                        original_scenario_data = confirmed_scenarios[scenario_index - 1]
                        original_recommendations = original_scenario_data.get('recommendations', [])
                        scenario = original_scenario_data['scenario']

                        # å®‰å…¨å¤„ç†æ¨èåˆ†çº§
                        graded_recommendations = safe_process_recommendation_grades(
                            grading_data, original_recommendations, scenario_index
                        )

                        # æ„å»ºè¿”å›ç»“æœ
                        final_result = {
                            'comprehensive_score': selected_data.get('comprehensive_score', 0),
                            'scenario_reasoning': selected_data.get('scenario_reasoning', ''),
                            'grading_reasoning': selected_data.get('grading_reasoning', ''),
                            'overall_reasoning': result.get('overall_reasoning', ''),
                            'graded_recommendations': graded_recommendations,
                            'recommendation_summary': {
                                'highly_recommended_count': len(graded_recommendations['highly_recommended']),
                                'recommended_count': len(graded_recommendations['recommended']),
                                'less_recommended_count': len(graded_recommendations['less_recommended']),
                                'total_recommendations': len(original_recommendations)
                            },
                            "final_choices":final_choices,
                            'scenario_metadata': {
                                'scenario_id': scenario_id or scenario.semantic_id,
                                'description': scenario.description_zh,
                                'panel': scenario.panel.name_zh if hasattr(scenario, 'panel') else 'æœªçŸ¥',
                                'patient_population': scenario.patient_population,
                                'clinical_context': scenario.clinical_context,
                                'original_index': scenario_index
                            }
                        }

                        final_results.append(final_result)

                    # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
                    final_results.sort(key=lambda x: x['comprehensive_score'], reverse=True)

                    logger.info(f"âœ… å•æ¬¡è°ƒç”¨å®Œæˆï¼ŒæˆåŠŸå¤„ç†{len(final_results)}ä¸ªåœºæ™¯")
                    return final_results
                else:
                    return response
            except Exception as e:
                logger.error(f"âŒ å•æ¬¡è°ƒç”¨å¤±è´¥: {str(e)}")
                if not direct_return:
                    return self._fallback_for_confirmed_scenarios(confirmed_scenarios)
                return "æ‰§è¡Œå‡ºé”™"
    def _fallback_for_confirmed_scenarios(self, confirmed_scenarios: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é™çº§æ–¹æ¡ˆ"""
        logger.info("ä½¿ç”¨é™çº§æ–¹æ¡ˆ...")
        return []
    async def _generate_overall_reasoning(self,
                                    patient_info,
                                    clinical_context,
                                    max_recommendations_per_scenario,
                                    final_results: List[Dict[str, Any]]) :
        """ç”Ÿæˆæ€»ä½“æ¨ç†è¯´æ˜"""
        patient_info_content = self.build_patient_context(patient_info)
        clinical_context_content = self.build_clinical_context(clinical_context)
        choices_content=""
        a=[]
        for choices in final_results:
            if isinstance(choices,list) and choices:
                a.extend(choices)
        choices_content="\n".join(a)
        task_content = f"""
                          ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦å½±åƒä¸“å®¶ï¼Œè¯·ä½ æ ¹æ®æä¾›ç»™ä½ çš„åŒ»å­¦å½±åƒæ¨èé¡¹ç›®ä»¥åŠæ‚£è€…çš„ä¿¡æ¯å’Œä¸´åºŠä¸Šä¸‹æ–‡ï¼Œé€‰æ‹©{max_recommendations_per_scenario}ä¸ªæœ€é€‚åˆè¯¥ç—…äººçš„åŒ»å­¦å½±åƒæ¨èé¡¹ç›®
                          è¿™æ˜¯æ‚£è€…çš„ä¿¡æ¯ï¼š
                                 {patient_info_content}
                          è¿™æ˜¯ä¸´åºŠä¸Šä¸‹æ–‡ï¼š
                                 {clinical_context_content}
                          è¿™æ˜¯å¯¹åº”çš„åŒ»å­¦å½±åƒæ¨èï¼š
                                  {choices_content}               

                          è¯·ä½ åŠ¡å¿…é€‰æ‹©{max_recommendations_per_scenario}ä¸ªæ¨èé¡¹ç›®ã€‚
                          è¿™æ˜¯è¾“å‡ºæ ¼å¼
                          {{
                             "final_choices":[è¿™é‡Œæ˜¯é€‰æ‹©çš„{max_recommendations_per_scenario}ä¸ªåŒ»å­¦å½±åƒæ¨èé¡¹ç›®]
                             "overall_reason":..

                          }}
                          åŠ¡å¿…ä»¥jsonæ ¼å¼è¾“å‡ºï¼


                """
        response = await self.ai_service._call_llm(task_content)
        try:
            res=safe_parse_llm_response(response)
        except Exception as e:
            logger.info(f"è§£æjsonå‡ºé”™ï¼š{e}")

        choices=res.get("final_choices",[])
        reason=res.get( "overall_reason","")
        return [{"final_choices":choices,"overall_reason":reason}]





    def _fallback_single_scenario(
            self,
            scenario_data: Dict[str, Any],
            scenario_index: int,
            top_k: int = 3  # æ–°å¢top_kå‚æ•°
    ) -> Dict[str, Any]:
        """å•ä¸ªåœºæ™¯å¤„ç†çš„é™çº§æ–¹æ¡ˆ"""

        scenario = scenario_data['scenario']
        original_recommendations = scenario_data.get('recommendations', [])

        # æ„å»ºç©ºçš„æ¨èåˆ†çº§
        graded_recommendations = {
            "highly_recommended": [],
            "recommended": [],
            "less_recommended": []
        }

        # å°†æ‰€æœ‰æ¨èé¡¹ç›®æ ‡è®°ä¸º"æ¨è"ä½œä¸ºé™çº§æ–¹æ¡ˆ
        for rec_data in original_recommendations:
            rec_copy = rec_data.copy()
            rec_copy['recommendation_level'] = 'recommended'
            rec_copy['recommendation_level_zh'] = 'æ¨è'

            # æ·»åŠ è¯¦ç»†ä¿¡æ¯çš„é™çº§å¤„ç†
            procedure = rec_copy['procedure']
            recommendation = rec_copy['recommendation']

            rec_copy['procedure_details'] = {
                'semantic_id': procedure.semantic_id,
                'name_zh': procedure.name_zh,
                'name_en': procedure.name_en,
                'modality': procedure.modality,
                'body_part': procedure.body_part,
                'contrast_used': procedure.contrast_used,
                'radiation_level': procedure.radiation_level,
                'exam_duration': procedure.exam_duration,
                'preparation_required': procedure.preparation_required,
                'standard_code': procedure.standard_code,
                'description_zh': procedure.description_zh
            }

            rec_copy['recommendation_details'] = {
                'appropriateness_rating': recommendation.appropriateness_rating,
                'appropriateness_category_zh': recommendation.appropriateness_category_zh,
                'evidence_level': recommendation.evidence_level,
                'consensus_level': recommendation.consensus_level,
                'adult_radiation_dose': recommendation.adult_radiation_dose,
                'pediatric_radiation_dose': recommendation.pediatric_radiation_dose,
                'pregnancy_safety': recommendation.pregnancy_safety,
                'contraindications': recommendation.contraindications,
                'reasoning_zh': recommendation.reasoning_zh,
                'special_considerations': recommendation.special_considerations
            }

            graded_recommendations['recommended'].append(rec_copy)

        # æ„å»º final_choices é™çº§æ–¹æ¡ˆï¼Œé€‰æ‹©å‰top_kä¸ª
        final_choices = []
        if original_recommendations:
            # é€‰æ‹©å‰top_kä¸ªæ¨èé¡¹ç›®ä½œä¸ºæœ€ç»ˆé€‰æ‹©
            selected_count = min(top_k, len(original_recommendations))
            final_choices = [original_recommendations[i]['procedure'].name_zh
                             for i in range(selected_count)]

        return {
            'comprehensive_score': 50,  # é»˜è®¤ä¸­ç­‰è¯„åˆ†
            'scenario_reasoning': 'ç³»ç»Ÿé™çº§å¤„ç†ï¼šæ— æ³•è·å–è¯¦ç»†åˆ†æ',
            'grading_reasoning': 'ç³»ç»Ÿé™çº§å¤„ç†ï¼šæ‰€æœ‰æ¨èé¡¹ç›®æ ‡è®°ä¸ºæ¨èçº§åˆ«',
            'overall_reasoning': '',
            'graded_recommendations': graded_recommendations,
            'recommendation_summary': {
                'highly_recommended_count': 0,
                'recommended_count': len(original_recommendations),
                'less_recommended_count': 0,
                'total_recommendations': len(original_recommendations)
            },
            'final_choices': final_choices,
            'scenario_metadata': {
                'scenario_id': scenario.semantic_id,
                'description': scenario.description_zh,
                'panel': scenario.panel.name_zh if hasattr(scenario, 'panel') else 'æœªçŸ¥',
                'patient_population': scenario.patient_population,
                'clinical_context': scenario.clinical_context,
                'original_index': scenario_index
            }
        }
    def _process_single_scenario_result(
            self,
            result: Dict[str, Any],
            scenario_data: Dict[str, Any],
            scenario_index: int,
            top_k: int = 3  # æ–°å¢ top_k å‚æ•°ï¼Œé»˜è®¤é€‰æ‹©å‰3ä¸ª
    ) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªåœºæ™¯çš„LLMè¿”å›ç»“æœ"""

        scenario = scenario_data['scenario']
        original_recommendations = scenario_data.get('recommendations', [])
        grading_data = result.get('recommendation_grades', {})

        # æŒ‰æ¨èç­‰çº§ç»„ç»‡æ¨èé¡¹ç›®
        graded_recommendations = {
            "highly_recommended": [],
            "recommended": [],
            "less_recommended": []
        }

        # å¤„ç†å„ç­‰çº§æ¨èé¡¹ç›®
        recommendation_levels = [
            ('highly_recommended', 'æå…¶æ¨è'),
            ('recommended', 'æ¨è'),
            ('less_recommended', 'ä¸å¤ªæ¨è')
        ]

        for level_key, level_zh in recommendation_levels:
            for rec_idx in grading_data.get(level_key, []):
                if 1 <= rec_idx <= len(original_recommendations):
                    rec_data = original_recommendations[rec_idx - 1].copy()
                    rec_data['recommendation_level'] = level_key
                    rec_data['recommendation_level_zh'] = level_zh

                    # æ·»åŠ å®Œæ•´çš„æ£€æŸ¥é¡¹ç›®ä¿¡æ¯
                    procedure = rec_data['procedure']
                    recommendation = rec_data['recommendation']

                    # æ„å»ºè¯¦ç»†çš„æ£€æŸ¥é¡¹ç›®ä¿¡æ¯
                    rec_data['procedure_details'] = {
                        'semantic_id': procedure.semantic_id,
                        'name_zh': procedure.name_zh,
                        'name_en': procedure.name_en,
                        'modality': procedure.modality,
                        'body_part': procedure.body_part,
                        'contrast_used': procedure.contrast_used,
                        'radiation_level': procedure.radiation_level,
                        'exam_duration': procedure.exam_duration,
                        'preparation_required': procedure.preparation_required,
                        'standard_code': procedure.standard_code,
                        'description_zh': procedure.description_zh
                    }

                    # æ„å»ºè¯¦ç»†çš„æ¨èä¿¡æ¯
                    rec_data['recommendation_details'] = {
                        'appropriateness_rating': recommendation.appropriateness_rating,
                        'appropriateness_category_zh': recommendation.appropriateness_category_zh,
                        'evidence_level': recommendation.evidence_level,
                        'consensus_level': recommendation.consensus_level,
                        'adult_radiation_dose': recommendation.adult_radiation_dose,
                        'pediatric_radiation_dose': recommendation.pediatric_radiation_dose,
                        'pregnancy_safety': recommendation.pregnancy_safety,
                        'contraindications': recommendation.contraindications,
                        'reasoning_zh': recommendation.reasoning_zh,
                        'special_considerations': recommendation.special_considerations
                    }

                    graded_recommendations[level_key].append(rec_data)
                else:
                    logger.warning(f"åœºæ™¯{scenario_index}çš„æ— æ•ˆ{level_zh}ç´¢å¼•: {rec_idx}")

        # è·å– final_choicesï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºé€‰æ‹© top_k ä¸ªé¡¹ç›®
        final_choices = []

        # æ–¹æ³•1: å¦‚æœLLMè¿”å›äº†final_choicesï¼Œä½¿ç”¨å®ƒï¼ˆä½†é™åˆ¶æ•°é‡ï¼‰
        llm_final_choices = result.get('final_choices', [])
        if llm_final_choices:
            final_choices = llm_final_choices[:top_k]  # é™åˆ¶ä¸ºtop_kä¸ª
        else:
            # æ–¹æ³•2: é™çº§æ–¹æ¡ˆ - æŒ‰æ¨èç­‰çº§ä¼˜å…ˆçº§é€‰æ‹©top_kä¸ªé¡¹ç›®
            selected_recommendations = []

            # ä¼˜å…ˆé€‰æ‹©æå…¶æ¨èçš„é¡¹ç›®
            if graded_recommendations['highly_recommended']:
                selected_count = min(top_k, len(graded_recommendations['highly_recommended']))
                selected_recommendations.extend(graded_recommendations['highly_recommended'][:selected_count])

            # å¦‚æœè¿˜ä¸å¤Ÿï¼Œè¡¥å……æ¨èçš„é¡¹ç›®
            if len(selected_recommendations) < top_k and graded_recommendations['recommended']:
                remaining_slots = top_k - len(selected_recommendations)
                additional_count = min(remaining_slots, len(graded_recommendations['recommended']))
                selected_recommendations.extend(graded_recommendations['recommended'][:additional_count])

            # å¦‚æœè¿˜ä¸å¤Ÿï¼Œè¡¥å……ä¸å¤ªæ¨èçš„é¡¹ç›®ï¼ˆé€šå¸¸ä¸æ¨èï¼Œä½†ä½œä¸ºå¤‡é€‰ï¼‰
            if len(selected_recommendations) < top_k and graded_recommendations['less_recommended']:
                remaining_slots = top_k - len(selected_recommendations)
                additional_count = min(remaining_slots, len(graded_recommendations['less_recommended']))
                selected_recommendations.extend(graded_recommendations['less_recommended'][:additional_count])

            # æå–æ£€æŸ¥é¡¹ç›®åç§°
            final_choices = [rec['procedure_details']['name_zh'] for rec in selected_recommendations]

            # å¦‚æœæ²¡æœ‰é€‰æ‹©ä»»ä½•é¡¹ç›®ï¼Œæ·»åŠ æç¤º
            if not final_choices:
                final_choices = ["æ— åˆé€‚æ¨èé¡¹ç›®"]

        # æ„å»ºè¿”å›ç»“æœ - ç»Ÿä¸€æ ¼å¼
        return {
            'comprehensive_score': result.get('comprehensive_score', 0),
            'scenario_reasoning': result.get('scenario_reasoning', ''),
            'grading_reasoning': result.get('grading_reasoning', ''),
            'overall_reasoning': '',  # å°†åœ¨å¤–å±‚ç»Ÿä¸€è®¾ç½®
            'graded_recommendations': graded_recommendations,
            'recommendation_summary': {
                'highly_recommended_count': len(graded_recommendations['highly_recommended']),
                'recommended_count': len(graded_recommendations['recommended']),
                'less_recommended_count': len(graded_recommendations['less_recommended']),
                'total_recommendations': len(original_recommendations)
            },
            'final_choices': final_choices,  # ç°åœ¨åŒ…å«æœ€å¤štop_kä¸ªé¡¹ç›®
            'scenario_metadata': {
                'scenario_id': result.get('scenario_id') or scenario.semantic_id,
                'description': scenario.description_zh,
                'panel': scenario.panel.name_zh if hasattr(scenario, 'panel') else 'æœªçŸ¥',
                'patient_population': scenario.patient_population,
                'clinical_context': scenario.clinical_context,
                'original_index': scenario_index
            }
        }

