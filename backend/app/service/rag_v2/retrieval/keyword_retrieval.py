import asyncio
import hashlib
import json
import math
from typing import Optional, List, Dict

from sqlalchemy.orm import selectinload
from sqlmodel import select, and_, or_

from app.config.redis_config import redis_manager
from app.model import ClinicalScenario
from app.service.rag_v2.ai_service import AiService
from app.service.rag_v2.retrieval.base_retrieval import BaseRetrieval
from app.utils.logger.simple_logger import get_logger

logger=get_logger(__name__)
class KeywordRetrieval(BaseRetrieval):


      def __init__(self):
          super().__init__()
          self.ai_service=AiService()
          self.redis_client=redis_manager


      async def aretrieval(self,
                           query_text: str,
                           medical_dict: Optional[List] = None,
                           top_p: int = 50,
                           top_k: int = 10):
          """
                  jiebaåˆ†è¯ + æ¨¡ç³ŠåŒ¹é…æ£€ç´¢ï¼ˆé«˜å¹¶å‘ä¼˜åŒ–ï¼šä½¿ç”¨ç‹¬ç«‹ sessionï¼‰
                  """
          # æš‚æ—¶ä¸ä½¿ç”¨
          return []
          # 1. ä½¿ç”¨æ··åˆåˆ†è¯ï¼ˆjieba + LLMå¹¶å‘éªŒè¯ï¼‰
          keywords, new_terms = await self._hybrid_tokenize_with_llm_verification(query_text, medical_dict)
          logger.info(f"ğŸ” æ··åˆåˆ†è¯æå–åˆ° {len(keywords)} ä¸ªå…³é”®è¯: {keywords[:10]}")
          if new_terms:
              logger.info(f"âœ¨ æœ¬æ¬¡æ–°å‘ç° {len(new_terms)} ä¸ªåŒ»å­¦æœ¯è¯­: {new_terms}")
              logger.info(f"âœ… è¿™äº›æ–°è¯å·²åŠ¨æ€æ·»åŠ åˆ°jiebaå†…ç½®è¯å…¸ï¼Œåç»­åˆ†è¯ä¼šè‡ªåŠ¨ä½¿ç”¨")

          if not keywords:
              logger.warning("jiebaåˆ†è¯æœªæå–åˆ°å…³é”®è¯ï¼Œè¿”å›ç©ºç»“æœ")
              return []

          # 2. æ„å»ºSQLæ¨¡ç³ŠåŒ¹é…æ¡ä»¶ï¼ˆä½¿ç”¨LIKEï¼‰
          top_keywords = keywords
          like_conditions = [
              ClinicalScenario.description_zh.contains(keyword)
              for keyword in top_keywords
          ]

          # 3. é«˜å¹¶å‘ä¼˜åŒ–ï¼šä½¿ç”¨ç‹¬ç«‹ session æ‰§è¡Œæ¨¡ç³ŠåŒ¹é…æŸ¥è¯¢
          session = await self._get_independent_session()
          try:
              statement = (
                  select(ClinicalScenario)
                  .options(
                      selectinload(ClinicalScenario.panel),
                      selectinload(ClinicalScenario.topic)
                  )
                  .where(
                      and_(
                          ClinicalScenario.is_active == True,
                          or_(*like_conditions)
                      )
                  )
                  .limit(top_p)
              )

              result = await session.exec(statement)
              scenarios = result.all()
              logger.info(f"æ¨¡ç³ŠåŒ¹é…æ£€ç´¢åˆ° {len(scenarios)} æ¡åœºæ™¯")
          except Exception as e:
              logger.error(f"æ•°æ®åº“æŸ¥è¯¢å¤±è´¥: {e}")
              return []
          finally:
              await session.close()

          if not scenarios:
              return []

          # 4. è®¡ç®—æ¯ä¸ªåœºæ™¯çš„jiebaåˆ†è¯é‡å åº¦å¾—åˆ†
          query_keywords_set = set(keywords)
          candidates_with_scores = []

          for scenario in scenarios:
              scenario_keywords = set(self._jieba_tokenize(
                  scenario.description_zh or "",
                  medical_dict,
                  new_terms
              ))

              overlap = query_keywords_set.intersection(scenario_keywords)
              union = query_keywords_set.union(scenario_keywords)

              if len(union) > 0:
                  jieba_score = len(overlap) / len(union)
              else:
                  jieba_score = 0.0

              candidates_with_scores.append({
                  'scenario': scenario,
                  'scenario_id': scenario.id,
                  'score': jieba_score,
                  'matched_keywords': list(overlap),
                  'source': "jieba"
              })

          logger.info(f"âœ… åˆ†è¯è¯„åˆ†å®Œæˆï¼Œå…± {len(candidates_with_scores)} ä¸ªç»“æœ")

          # 5. æ£€æŸ¥æ˜¯å¦éœ€è¦å½’ä¸€åŒ–å¹¶å¤„ç†
          if candidates_with_scores:
              max_score = max(candidate['jieba_score'] for candidate in candidates_with_scores)
              logger.info(f"ğŸ“Š å½’ä¸€åŒ–å‰æœ€å¤§åˆ†æ•°: {max_score:.4f}")

              if max_score < 0.7:
                  logger.info("ğŸ“ˆ æœ€å¤§åˆ†æ•°ä½äº0.7ï¼Œè¿›è¡Œéçº¿æ€§å½’ä¸€åŒ–å¤„ç†")
                  candidates_with_scores = self._normalize_scores_nonlinear(candidates_with_scores,
                                                                            method="power"
                                                                            )
              else:
                  logger.info("âœ… æœ€å¤§åˆ†æ•°è¾¾åˆ°0.7ï¼Œä¿æŒåŸå§‹åˆ†æ•°")

          # 6. æŒ‰jieba_scoreæ’åº
          candidates_with_scores.sort(key=lambda x: x['jieba_score'], reverse=True)
          logger.info(f"ğŸ“Š æ’åºåå‰3ååˆ†æ•°: {[r['jieba_score'] for r in candidates_with_scores[:3]]}")

          # 7. è¿”å›top_k
          final_results = candidates_with_scores[:top_k]
          logger.info(f"âœ… è¿”å› {len(final_results)} æ¡jiebaæ£€ç´¢ç»“æœ")
          return final_results

      async def _hybrid_tokenize_with_llm_verification(
              self,
              text: str,
              medical_dict: list
      ) -> tuple[List[str], List[str]]:
          """Run jieba and LLM keyword extraction in parallel and update the dictionary dynamically."""
          import jieba

          cached_keywords = await self._get_cached_keywords(text)
          if cached_keywords and cached_keywords["keywords"]:
              cached_new_terms = cached_keywords.get("new_terms") or []
              if cached_new_terms:
                  for term in cached_new_terms:
                      if len(term) >= 2:
                          jieba.add_word(term, freq=10000, tag="medical_dynamic")
                  logger.info("keywords cache hit; restored %s new terms", len(cached_new_terms))
              logger.info("reusing %s cached keywords", len(cached_keywords["keywords"]))
              return cached_keywords["keywords"], cached_new_terms

          logger.info("starting parallel jieba + LLM keyword extraction")
          jieba_task = asyncio.get_event_loop().run_in_executor(
              None,
              self._jieba_tokenize,
              text,
              medical_dict,
              None
          )
          llm_task = self.ai_service.extract_medical_keywords_by_llm(text, top_k=20)
          try:
              jieba_keywords, llm_keywords = await asyncio.gather(
                  jieba_task,
                  llm_task,
                  return_exceptions=True
              )
              if isinstance(jieba_keywords, Exception):
                  logger.error("jieba keyword extraction failed: %s", jieba_keywords)
                  jieba_keywords = []
              if isinstance(llm_keywords, Exception):
                  logger.error("LLM keyword extraction failed: %s", llm_keywords)
                  llm_keywords = []

              jieba_set = set(jieba_keywords)
              llm_set = set(llm_keywords)
              new_terms = list(llm_set - jieba_set)

              if new_terms:
                  logger.info("LLM discovered %s new medical terms", len(new_terms))
                  for term in new_terms:
                      if len(term) >= 2:
                          jieba.add_word(term, freq=10000, tag="medical_dynamic")
                          logger.debug("added dynamic term: %s", term)
              else:
                  logger.info("jieba and LLM keywords are identical; dictionary unchanged")

              merged_keywords = list(jieba_set | llm_set)
              merged_keywords.sort(key=len, reverse=True)

              logger.info(
                  "merged keywords=%s (jieba=%s, llm=%s, new=%s)",
                  len(merged_keywords),
                  len(jieba_keywords),
                  len(llm_keywords),
                  len(new_terms)
              )

              await self._cache_keywords(text, merged_keywords, new_terms)
              return merged_keywords, new_terms

          except Exception as exc:
              logger.error("hybrid tokenization failed: %s", exc)
              fallback_keywords = self._jieba_tokenize(text, medical_dict, None)
              await self._cache_keywords(text, fallback_keywords, [])
              return fallback_keywords, []

      async def _cache_keywords(
              self,
              text: str,
              keywords: List[str],
              new_terms: List[str],
              ttl: int = 12 * 60 * 60,
      ) -> None:
          """å°†å…³é”®è¯ç»“æœå†™å…¥Redisç¼“å­˜"""
          if not keywords or not self.redis_client:
              return
          cache_key = f"medical_keywords:{hashlib.md5(text.encode('utf-8')).hexdigest()}"
          payload = json.dumps({'keywords': keywords, 'new_terms': new_terms}, ensure_ascii=False)
          try:
              await self.redis_client.set(cache_key, payload, ex=ttl)
          except Exception as exc:
              logger.warning(f"å†™å…¥å…³é”®è¯ç¼“å­˜å¤±è´¥: {exc}")

      def _jieba_tokenize(self, text: str, medical_dict: list, new_item: list = None) -> List[str]:
          """
          ä½¿ç”¨jiebaè¿›è¡Œä¸­æ–‡åˆ†è¯å’Œå…³é”®è¯æå–

          ç‰¹æ€§ï¼š
          - è‡ªåŠ¨åŠ è½½å¤–éƒ¨åŒ»å­¦è¯å…¸ï¼ˆdictç›®å½•ä¸‹çš„æ–‡ä»¶ï¼‰
          - å†…ç½®200+åŒ»å­¦æœ¯è¯­ä½œä¸ºè¡¥å……
          - TextRank + TF-IDFåŒç®—æ³•æå–å…³é”®è¯
          - æ™ºèƒ½åœç”¨è¯è¿‡æ»¤
          - ä¼˜å…ˆçº§æ’åºï¼ˆåŒ»å­¦æœ¯è¯­>é•¿è¯>çŸ­è¯ï¼‰
          """
          # project_root = Path(__file__).parent.parent.parent.parent
          # dict_dir = project_root / "dict"

          import jieba
          import jieba.analyse
          # jieba.analyse.set_stop_words(dict_dir / "stops.txt")
          # å†…ç½®åŒ»å­¦æœ¯è¯­ä½œä¸ºè¡¥å……ï¼ˆä»¥é˜²å¤–éƒ¨è¯å…¸åŠ è½½å¤±è´¥ï¼‰
          # è¿™äº›æœ¯è¯­ä¼šä¸å¤–éƒ¨è¯å…¸åˆå¹¶ä½¿ç”¨
          builtin_medical_terms = [
              'å† å¿ƒç—…', 'æ€¥æ€§å† è„‰ç»¼åˆå¾', 'å¿ƒè‚Œæ¢—æ­»', 'å¿ƒç»ç—›', 'é«˜è¡€å‹',
              'ç³–å°¿ç—…', 'è„‘å’ä¸­', 'è‚ºæ “å¡', 'ä¸»åŠ¨è„‰å¤¹å±‚', 'å¿ƒåŠ›è¡°ç«­',
              'è‚ºç‚', 'æ”¯æ°”ç®¡ç‚', 'å“®å–˜', 'æ…¢é˜»è‚º', 'è‚ºç»“æ ¸',
              'é˜‘å°¾ç‚', 'èƒ†å›Šç‚', 'èƒ°è…ºç‚', 'è‚ æ¢—é˜»', 'æ¶ˆåŒ–é“å‡ºè¡€',
              'è‚¾ç»“çŸ³', 'å°¿è·¯æ„ŸæŸ“', 'è‚¾åŠŸèƒ½ä¸å…¨', 'è‚¾ç‚',
              'éª¨æŠ˜', 'è„±ä½', 'éŸ§å¸¦æŸä¼¤', 'è½¯ç»„ç»‡æŒ«ä¼¤',
              'ç”²çŠ¶è…ºåŠŸèƒ½äº¢è¿›', 'ç”²çŠ¶è…ºåŠŸèƒ½å‡é€€', 'ç”²çŠ¶è…ºç»“èŠ‚',
              'å¦Šå¨ é«˜è¡€å‹', 'å¦Šå¨ ç³–å°¿ç—…', 'å®«å¤–å­•', 'å…ˆå…†æµäº§',
              'å‹æ¦¨æ€§ç–¼ç—›', 'å‘¼å¸å›°éš¾', 'å’³å—½å’³ç—°', 'èƒ¸é—·æ°”çŸ­',
              'è…¹ç—›è…¹æ³»', 'æ¶å¿ƒå‘•å', 'å¤´ç—›å¤´æ™•', 'å‘çƒ­ç•å¯’',
              'CT', 'MRI', 'è¶…å£°', 'Xçº¿', 'å¿ƒç”µå›¾', 'å† çŠ¶åŠ¨è„‰é€ å½±',
              "éå¦Šå¨ ", "éå¦Šå¨ æœŸ", "éå¦Šå¨ çŠ¶æ€"
          ]
          if new_item:
              builtin_medical_terms.extend(new_item)

          # è¡¥å……æ·»åŠ å†…ç½®è¯æ±‡ï¼ˆå¤–éƒ¨è¯å…¸å·²åœ¨åˆå§‹åŒ–æ—¶åŠ è½½ï¼‰
          for term in set(builtin_medical_terms):
              jieba.add_word(term, freq=10000, tag='medical')

          # æ–¹æ³•1: ä½¿ç”¨TextRankç®—æ³•æå–å…³é”®è¯ï¼ˆæ¨èï¼‰
          keywords_textrank = jieba.analyse.textrank(
              text,
              topK=20,  # æå–å‰20ä¸ªå…³é”®è¯
              withWeight=False,
              allowPOS=('n', 'nr', 'nt', 'nz', 'v', 'a',
                        "f", "ns", "ad", "q", 'u', 's', 'vd', 'r', 'xc', 't',
                        'vn'

                        ),
              # åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯
          )

          # æ–¹æ³•2: ä½¿ç”¨TF-IDFç®—æ³•æå–å…³é”®è¯ï¼ˆä½œä¸ºè¡¥å……ï¼‰
          keywords_tfidf = jieba.analyse.extract_tags(
              text,
              topK=15,
              withWeight=False
          )

          all_words = set(builtin_medical_terms)
          for suggest in all_words:
              jieba.suggest_freq(suggest, True)
          # æ–¹æ³•3: åŸºç¡€åˆ†è¯ï¼ˆä¿ç•™æ‰€æœ‰åŒ»å­¦ç›¸å…³è¯ï¼‰
          words = jieba.lcut(text, cut_all=False)

          # åœç”¨è¯åˆ—è¡¨ï¼ˆæ‰©å±•ç‰ˆï¼‰
          stop_words = {
              # é€šç”¨åœç”¨è¯
              'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª',
              'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½',
              'è‡ªå·±', 'è¿™', 'é‚£', 'é‡Œ', 'å•Š', 'å—', 'å‘¢', 'å§', 'å“¦', 'å—¯', 'å“ˆ',
              # ä¸´åºŠå¸¸è§è™šè¯
              'æ‚£è€…', 'ç—…äºº', 'ç—…å²', 'å¹´', 'å²', 'æ¬¡', 'å¤©', 'å°æ—¶', 'åˆ†é’Ÿ',
              'ä¸»è¯‰', 'ç°ç—…å²', 'æ—¢å¾€å²', 'è¯Šæ–­', 'ç—‡çŠ¶', 'è¡¨ç°'
          }

          # è¿‡æ»¤åœç”¨è¯å’Œå•å­—
          words_filtered = [
              w for w in words
              if w not in stop_words and len(w) >= 2  # ä¿ç•™é•¿åº¦>=2çš„è¯
          ]

          # åˆå¹¶ä¸‰ç§æ–¹æ³•çš„ç»“æœ
          all_keywords = list(set(keywords_textrank + keywords_tfidf + words_filtered))

          # è·å–æ‰€æœ‰å·²åŠ è½½çš„åŒ»å­¦æœ¯è¯­ï¼ˆå¤–éƒ¨è¯å…¸ + å†…ç½®è¯å…¸ï¼‰
          all_medical_terms = set(builtin_medical_terms)
          try:
              all_medical_terms.update(medical_dict)
          except:
              pass  # å¦‚æœè·å–å¤±è´¥ï¼Œä½¿ç”¨å†…ç½®è¯å…¸å³å¯

          # ä¼˜å…ˆçº§æ’åºï¼šåŒ»å­¦æœ¯è¯­ > é•¿è¯ > å…¶ä»–
          medical_keywords = [w for w in all_keywords if w in all_medical_terms]
          long_keywords = [w for w in all_keywords if len(w) >= 3 and w not in medical_keywords]
          other_keywords = [w for w in all_keywords if len(w) == 2 and w not in medical_keywords]

          return medical_keywords + long_keywords + other_keywords

      async def _get_cached_keywords(self, text: str) -> Optional[Dict[str, List[str]]]:
          """å°è¯•ä»Redisç¼“å­˜è¯»å–å…³é”®è¯ï¼Œé¿å…é‡å¤è§¦å‘LLMè°ƒç”¨"""
          if not text or not self.redis_client:
              return None
          cache_key = f"medical_keywords:{hashlib.md5(text.encode('utf-8')).hexdigest()}"
          try:
              cached_value = await self.redis_client.get(cache_key)
          except Exception as exc:
              logger.warning(f"è·å–å…³é”®è¯ç¼“å­˜å¤±è´¥: {exc}")
              return None
          if not cached_value:
              return None
          if isinstance(cached_value, bytes):
              try:
                  cached_value = cached_value.decode('utf-8')
              except Exception as exc:
                  logger.warning(f"å…³é”®è¯ç¼“å­˜è§£ç å¤±è´¥: {exc}")
                  return None
          try:
              cached_data = json.loads(cached_value)
          except json.JSONDecodeError as exc:
              logger.warning(f"å…³é”®è¯ç¼“å­˜JSONè§£æå¤±è´¥: {exc}")
              return None
          return {
              'keywords': cached_data.get('keywords') or [],
              'new_terms': cached_data.get('new_terms') or []
          }

      def _normalize_scores_nonlinear(self, candidates: List[Dict], method: str = "sigmoid") -> List[Dict]:
          """
          éçº¿æ€§å½’ä¸€åŒ–åˆ†æ•°åˆ°0.5~0.95èŒƒå›´

          Args:
              candidates: åŒ…å«jieba_scoreçš„å€™é€‰åˆ—è¡¨
              method: å½’ä¸€åŒ–æ–¹æ³•ï¼Œå¯é€‰ "sigmoid", "power", "log", "exponential"

          Returns:
              å½’ä¸€åŒ–åçš„å€™é€‰åˆ—è¡¨
          """

          if not candidates:
              return candidates

          # æå–åŸå§‹åˆ†æ•°
          scores = [candidate['jieba_score'] for candidate in candidates]
          min_score = min(scores)
          max_score = max(scores)

          logger.info(f"ğŸ“ˆ {method}å½’ä¸€åŒ–å‰åˆ†æ•°èŒƒå›´: [{min_score:.4f}, {max_score:.4f}]")

          # å¦‚æœæ‰€æœ‰åˆ†æ•°ç›¸åŒï¼Œç›´æ¥è®¾ç½®åˆ°ä¸­é—´å€¼
          if abs(max_score - min_score) < 1e-6:
              for candidate in candidates:
                  candidate['jieba_score'] = 0.8
              logger.info("ğŸ“Š æ‰€æœ‰åˆ†æ•°ç›¸åŒï¼Œè®¾ç½®ä¸ºä¸­é—´å€¼0.725")
              return candidates

          for candidate in candidates:
              # å…ˆçº¿æ€§å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
              x = (candidate['jieba_score'] - min_score) / (max_score - min_score)

              if method == "sigmoid":
                  # Sigmoidå‡½æ•°å½’ä¸€åŒ– - å¼ºåŒ–ä¸­é—´åŒºåŸŸ
                  normalized_score = self._sigmoid_normalize(x)
              elif method == "power":
                  # å¹‚å‡½æ•°å½’ä¸€åŒ– - å¯ä»¥å¼ºåŒ–é«˜åˆ†æˆ–ä½åˆ†åŒºåŸŸ
                  normalized_score = self._power_normalize(x, power=0.6)
              elif method == "log":
                  # å¯¹æ•°å½’ä¸€åŒ– - å‹ç¼©é«˜åˆ†åŒºåŸŸï¼Œæ‹‰ä¼¸ä½åˆ†åŒºåŸŸ
                  normalized_score = self._log_normalize(x)
              elif method == "exponential":
                  # æŒ‡æ•°å½’ä¸€åŒ– - æ‹‰ä¼¸é«˜åˆ†åŒºåŸŸï¼Œå‹ç¼©ä½åˆ†åŒºåŸŸ
                  normalized_score = self._exponential_normalize(x)
              elif method == "tanh":
                  # åŒæ›²æ­£åˆ‡å½’ä¸€åŒ– - æ¸©å’Œçš„éçº¿æ€§
                  normalized_score = self._tanh_normalize(x)
              else:
                  # é»˜è®¤ä½¿ç”¨çº¿æ€§å½’ä¸€åŒ–
                  normalized_score = 0.5 + 0.45 * x

              candidate['jieba_score'] = normalized_score

          # éªŒè¯å½’ä¸€åŒ–ç»“æœ
          normalized_scores = [candidate['jieba_score'] for candidate in candidates]
          logger.info(f"ğŸ“ˆ {method}å½’ä¸€åŒ–ååˆ†æ•°èŒƒå›´: [{min(normalized_scores):.4f}, {max(normalized_scores):.4f}]")

          return candidates

      def _sigmoid_normalize(self, x: float) -> float:
          """Sigmoidå‡½æ•°å½’ä¸€åŒ– - å¼ºåŒ–ä¸­é—´åŒºåŸŸ"""
          # å°†è¾“å…¥è°ƒæ•´åˆ°æ›´é€‚åˆsigmoidçš„èŒƒå›´
          x_scaled = (x - 0.5) * 6  # è°ƒæ•´ç¼©æ”¾å› å­æ¥æ§åˆ¶æ›²çº¿é™¡å³­ç¨‹åº¦
          sigmoid = 1 / (1 + math.exp(-x_scaled))
          # æ˜ å°„åˆ°0.5-0.95èŒƒå›´
          return 0.5 + 0.45 * sigmoid

      def _power_normalize(self, x: float, power: float = 0.7) -> float:
          """å¹‚å‡½æ•°å½’ä¸€åŒ– - power<1å¼ºåŒ–é«˜åˆ†ï¼Œpower>1å¼ºåŒ–ä½åˆ†"""
          powered = x ** power
          return 0.5 + 0.45 * powered

      def _log_normalize(self, x: float) -> float:
          """å¯¹æ•°å½’ä¸€åŒ– - å‹ç¼©é«˜åˆ†åŒºåŸŸ"""
          # é¿å…log(0)
          if x < 0.001:
              x = 0.001
          log_norm = math.log(x + 1) / math.log(2)  # log2(x+1) å½’ä¸€åŒ–åˆ°0-1
          return 0.5 + 0.45 * log_norm

      def _exponential_normalize(self, x: float) -> float:
          """æŒ‡æ•°å½’ä¸€åŒ– - æ‹‰ä¼¸é«˜åˆ†åŒºåŸŸ"""
          exp_norm = (math.exp(x) - 1) / (math.e - 1)
          return 0.5 + 0.45 * exp_norm

      def _tanh_normalize(self, x: float) -> float:
          """åŒæ›²æ­£åˆ‡å½’ä¸€åŒ– - æ¸©å’Œçš„éçº¿æ€§"""
          x_scaled = (x - 0.5) * 3  # è°ƒæ•´ç¼©æ”¾å› å­
          tanh_norm = (math.tanh(x_scaled) + 1) / 2
          return 0.5 + 0.45 * tanh_norm

