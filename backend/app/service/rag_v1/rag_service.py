import asyncio
import hashlib
import json
from typing import Any, Dict, List, Optional
import time
from langchain_core.messages import SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from sqlmodel.ext.asyncio.session import AsyncSession
from app.core.language_model.model_client_wrapper import ChatClientSDK, EmbeddingClientSDK, RerankerClientSDK
from app.entity.model_entity import DEFAULT_APP_CONFIG
from app.entity.retrieval_entity import RerankingStrategy
from app.response.exception.exceptions import ValidationException, InternalServerException
from app.schema.IntelligentRecommendation_schemas import (
    IntelligentRecommendationRequest,
    IntelligentRecommendationResponse,
    SearchStrategy, RetrievalRequest
)
from app.service.rag_v1.model_service import ModelService
from app.service.rag_v1.retrieval_service import RetrievalService
from app.service.rag_v1.simple_retrieval_service import SimpleRetrievalService
from app.service.rag_v1.adaptive_recommend_service import AdaptiveRecommendationEngineService
from app.utils.logger.simple_logger import get_logger
from app.config.config import settings
from app.entity.retrieval_entity import  StructOutputPatient
from app.config.redis_config import redis_manager
logger = get_logger(__name__)





class RagService:

      def __init__(self,
                   session: AsyncSession,
                   model_service: ModelService,
                   retrieval_service:RetrievalService,
                   simple_retrieval_service:SimpleRetrievalService,
                   ):
          self.session = session
          self.model_service = model_service
          self.retrieval_service = retrieval_service
          self.simple_retrieval_service=simple_retrieval_service
          self.redis_client = redis_manager.async_client
      async def get_struct_output(self, standard_query: str, timeout: float = 300):
          """è·å–ç»“æ„åŒ–è¾“å‡ºï¼Œå¸¦Redisç¼“å­˜å’Œè¶…æ—¶æ§åˆ¶

          Args:
              standard_query: æ ‡å‡†æŸ¥è¯¢æ–‡æœ¬
              timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10ç§’
          """
          # ç”Ÿæˆç¼“å­˜é”®
          cache_key = f"struct_output:{hashlib.md5(standard_query.encode()).hexdigest()}"

          try:
              # å°è¯•ä»ç¼“å­˜è·å–
              cached_result = await self.redis_client.get(cache_key)
              if cached_result:
                  logger.info(f"âœ… ä»ç¼“å­˜è·å–struct_output")
                  cached_data = json.loads(cached_result)
                  return StructOutputPatient(**cached_data)
          except Exception as e:
              logger.warning(f"âš ï¸  Redisç¼“å­˜è¯»å–å¤±è´¥: {e}")

          # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨LLMï¼ˆå¸¦è¶…æ—¶æ§åˆ¶ï¼‰
          try:
              prompt = ChatPromptTemplate.from_messages(
                  [SystemMessage("ä½ æ˜¯ä¸€ä¸ªæŒ‰ç…§æŒ‡ä»¤ç”Ÿæˆå›å¤çš„åŠ©æ‰‹ï¼Œè¯·ä½ æ ¹æ®è¾“å…¥çš„æ–‡æœ¬ç”Ÿæˆç›¸åº”çš„æ•°æ®ï¼Œè¯·ä½ æ³¨æ„ï¼Œå¦‚æœæ–‡æœ¬é‡Œæ²¡æœ‰ç›¸åº”å­—æ®µçš„æ•°æ®ï¼Œç½®ä¸ºæœªçŸ¥"),
                   ("user", "è¿™æ˜¯è¾“å…¥çš„æ–‡æœ¬:{text}")
                  ]
              )

              model = ChatOpenAI(
                  model=settings.OLLAMA_LLM_MODEL,
                  api_key=settings.SILICONFLOW_API_KEY,
                  base_url=settings.OLLAMA_BASE_URL,
                  temperature=1,
                  max_tokens=1024,
                  streaming=False,
              ).with_structured_output(StructOutputPatient)

              chain = prompt | model

              # ä½¿ç”¨asyncio.wait_foræ·»åŠ é¢å¤–è¶…æ—¶ä¿æŠ¤
              response = await asyncio.wait_for(
                  chain.ainvoke({"text": standard_query}),
                  timeout=timeout
              )

              # å­˜å…¥ç¼“å­˜ï¼ˆTTL: 1å°æ—¶ = 3600ç§’ï¼‰
              try:
                  await self.redis_client.setex(
                      cache_key,
                      3600,
                      json.dumps(response.model_dump())
                  )
                  logger.info(f"âœ… struct_outputå·²ç¼“å­˜")
              except Exception as e:
                  logger.warning(f"âš ï¸  Redisç¼“å­˜å†™å…¥å¤±è´¥: {e}")

              return response

          except asyncio.TimeoutError:
              logger.error(f"âŒ struct_outputè°ƒç”¨è¶…æ—¶ ({timeout}ç§’)")
              # è¿”å›ç©ºç»“æ„ï¼Œé¿å…é˜»å¡æ•´ä¸ªæµç¨‹
              return StructOutputPatient(patient_info={}, clinical_context={})


      async def _process_struct_output_if_needed(
              self,
              request: IntelligentRecommendationRequest
      ) -> None:
          """å¤„ç†æ ‡å‡†æŸ¥è¯¢å¹¶æ›´æ–°requestå¯¹è±¡"""
          if request.standard_query and isinstance(request.standard_query, str) and request.standard_query!="":
              res = await self.get_struct_output(request.standard_query)
              request.patient_info = res.patient_info
              request.clinical_context = res.clinical_context

      def _build_response(
              self,
              request: IntelligentRecommendationRequest,
              best_recommendations: List[Dict[str, Any]],
              processing_time_ms: int,
              retrieval_config: RetrievalRequest,
              strategy: RerankingStrategy,
              error_message: Optional[str] = None
      ) -> IntelligentRecommendationResponse:
          """æ„å»ºå“åº”å¯¹è±¡"""
          return IntelligentRecommendationResponse(
              query=f"{request.clinical_context.chief_complaint} | {request.clinical_context.diagnosis or ''}",
              best_recommendations=best_recommendations,
              processing_time_ms=processing_time_ms,
              similarity_threshold=retrieval_config.similarity_threshold,
              strategy_used=strategy.value,
              error_message=error_message
          )



      def _initialize_retrieval_config(
                  self,
                  request: IntelligentRecommendationRequest
          ) -> tuple[SearchStrategy, RetrievalRequest, RerankingStrategy, int]:
              """åˆå§‹åŒ–æ£€ç´¢é…ç½®"""
              search_strategy = request.search_strategy or SearchStrategy()
              retrieval_config = request.retrieval_strategy or RetrievalRequest()
              strategy = retrieval_config.reranking_strategy
              initial_top_k = self._calculate_initial_top_k(retrieval_config)

              logger.info(f"ğŸš€ å¼€å§‹æ™ºèƒ½æ¨èï¼Œä½¿ç”¨ç­–ç•¥: {strategy.value}")
              logger.info(f"   è§„åˆ™è¿‡æ»¤: {retrieval_config.apply_rule_filter}, "
                          f"LLMé‡æ’åº: {retrieval_config.enable_reranking}, "
                          f"LLMæ¨è: {retrieval_config.need_llm_recommendations}")

              return search_strategy, retrieval_config, strategy, initial_top_k



      async def generate_intelligent_recommendation(
              self,
              request: IntelligentRecommendationRequest,
              medical_dict: Dict[str, Any]
              ) -> IntelligentRecommendationResponse:
          """
          ç”Ÿæˆæ™ºèƒ½æ¨è - ä½¿ç”¨ç­–ç•¥æšä¸¾ç»Ÿä¸€å¤„ç†

          æµç¨‹ï¼š
          1. æ‰§è¡Œå››é˜¶æ®µæ··åˆæ£€ç´¢
          2. æ ¹æ®ç­–ç•¥æšä¸¾æ‰§è¡Œç›¸åº”çš„é‡æ’åºé€»è¾‘
          3. è¿”å›ç»“æ„åŒ–ç»“æœ
          """
          start_time = time.time()
          # å¤„ç†æ ‡å‡†æŸ¥è¯¢
          await self._process_struct_output_if_needed(request)




          try:
              # ========== 1. æ‰§è¡Œå››é˜¶æ®µæ··åˆæ£€ç´¢ ==========
              search_strategy, retrieval_config, strategy, initial_top_k = self._initialize_retrieval_config(request)

              final_scenarios = await self.retrieval_service.retrieve_clinical_scenarios(
                  patient_info=request.patient_info,
                  clinical_context=request.clinical_context,
                  search_strategy=search_strategy,
                  top_k=initial_top_k,
                  similarity_threshold=retrieval_config.similarity_threshold,
                  medical_dict=medical_dict
              )

              # ========== 2. æ ¹æ®ç­–ç•¥æ‰§è¡Œé‡æ’åº ==========
              best_recommendations = await self.retrieval_service.llm_rank_all_scenarios(
                  all_scenarios=final_scenarios,
                  patient_info=request.patient_info,
                  clinical_context=request.clinical_context,
                  strategy=strategy,
                  min_rating=retrieval_config.min_appropriateness_rating or 5,
                  direct_return=request.direct_return,
                  max_scenarios=retrieval_config.top_scenarios,
                  max_recommendations_per_scenario=retrieval_config.top_recommendations_per_scenario
              )
              if isinstance(best_recommendations,dict):
                  best_recommendations=[best_recommendations]

              # ========== 3. è®¡ç®—å¤„ç†æ—¶é—´ ==========
              processing_time_ms = int((time.time() - start_time) * 1000)

              # ========== 4. è¿”å›ç»“æ„åŒ–å“åº” ==========
              return self._build_response(request, best_recommendations, processing_time_ms, retrieval_config, strategy)


          except Exception as e:
              logger.error(f"âŒ æ™ºèƒ½æ¨èå¤±è´¥: {str(e)}")
              processing_time_ms = int((time.time() - start_time) * 1000)
              return IntelligentRecommendationResponse(
                  best_recommendations=[],
                  processing_time_ms=processing_time_ms,
                  error_message=str(e)
              )



      def _calculate_initial_top_k(self, retrieval_config: RetrievalRequest) -> int:
          """è®¡ç®—åˆå§‹æ£€ç´¢æ•°é‡"""
          strategy = retrieval_config.reranking_strategy
          base_k = retrieval_config.top_scenarios

          # æ ¹æ®ç­–ç•¥å†³å®šåˆå§‹æ£€ç´¢æ•°é‡
          strategy_multipliers = {
              RerankingStrategy.NONE: 1,
              RerankingStrategy.RULE_ONLY: 3,
              RerankingStrategy.LLM_SCENARIO_ONLY: 4,
              RerankingStrategy.LLM_RECOMMENDATION_ONLY: 1,
              RerankingStrategy.RULE_AND_LLM_SCENARIO: 5,
              RerankingStrategy.RULE_AND_LLM_RECOMMENDATION: 4,
              RerankingStrategy.LLM_SCENARIO_AND_RECOMMENDATION: 4,
              RerankingStrategy.ALL: 6
          }

          multiplier = strategy_multipliers.get(strategy, 3)
          return max(30, base_k * multiplier)

      async  def generate_simple_recommendation(self,
           request: IntelligentRecommendationRequest,
           medical_dict: Dict[str, Any]):
          standard_query=""
          start_time = time.time()
          try:
              await self._process_struct_output_if_needed(request)

              # åœºæ™¯æ•°é‡éªŒè¯
              if request.retrieval_strategy.top_scenarios >= 5:
                  raise ValidationException(message="è¯·æ±‚çš„æœ€å¤§åœºæ™¯æ•°ä¸èƒ½è¶…è¿‡5ä¸ªï¼")

              # ========== 1. åˆå§‹åŒ–æ£€ç´¢é…ç½® ==========
              search_strategy, retrieval_config, strategy, initial_top_k = self._initialize_retrieval_config(request)


              final_scenarios = await self.retrieval_service.retrieve_clinical_scenarios(
                  patient_info=request.patient_info,
                  clinical_context=request.clinical_context,
                  search_strategy=search_strategy,
                  top_k=initial_top_k,
                  similarity_threshold=retrieval_config.similarity_threshold,
                  medical_dict=medical_dict
              )
              # ========== 2. æ ¹æ®ç­–ç•¥æ‰§è¡Œé‡æ’åº ==========
              best_recommendations = await self.simple_retrieval_service.simple_rank_all_scenarios(
                  all_scenarios=final_scenarios,
                  patient_info=request.patient_info,
                  clinical_context=request.clinical_context,
                  strategy=strategy,
                  min_rating=retrieval_config.min_appropriateness_rating or 5,
                  direct_return=request.direct_return,
                  max_scenarios=retrieval_config.top_scenarios,
                  max_recommendations_per_scenario=retrieval_config.top_recommendations_per_scenario
              )

              # ========== 3. è®¡ç®—å¤„ç†æ—¶é—´ ==========
              processing_time_ms = int((time.time() - start_time) * 1000)
              if isinstance(best_recommendations, dict):
                  best_recommendations = [best_recommendations]
              # ========== 4. è¿”å›ç»“æ„åŒ–å“åº” ==========
              return self._build_response(request, best_recommendations, processing_time_ms, retrieval_config, strategy)


          except Exception as e:
               raise InternalServerException(
                   message=str(e)
               )







      async def _process_and_validate_model_config(self, origin_model_config: dict[str, Any]) -> dict[str, Any]:
          """
          å¤„ç†å¹¶æ ¡éªŒæ¨¡å‹é…ç½®ï¼ˆæ”¯æŒchat/embedding/rerankerä¸‰ç§å­æ¨¡å‹ï¼‰ï¼Œè¿”å›æ ¡éªŒåçš„é…ç½®

          æ ¡éªŒé€»è¾‘ï¼š
          1. æ•´ä½“ç»“æ„æ ¡éªŒï¼ˆå¿…é¡»ä¸ºå­—å…¸ï¼‰
          2. ä¸‰ç±»å­æ¨¡å‹ï¼ˆchat_model/embedding_model/reranker_modelï¼‰åˆ†åˆ«æ ¡éªŒï¼š
             - ç±»å‹åˆæ³•æ€§ï¼ˆtypeå¿…é¡»åŒ¹é…ï¼‰
             - æ¨¡å‹åç§°ï¼ˆnameï¼‰æœ‰æ•ˆæ€§ï¼ˆä½¿ç”¨ModelManageréªŒè¯ï¼‰
             - å‚æ•°ï¼ˆparametersï¼‰ç»“æ„æ ¡éªŒ
          
          æ³¨æ„ï¼šä¸å†ä¾èµ–language_model_managerï¼Œä½¿ç”¨ModelManagerè¿›è¡Œæ¨¡å‹éªŒè¯
          """
          # 1. åŸºç¡€æ ¡éªŒï¼šè¾“å…¥å¿…é¡»ä¸ºå­—å…¸ï¼Œå¦åˆ™è¿”å›é»˜è®¤é…ç½®
          if not isinstance(origin_model_config, dict):
              return DEFAULT_APP_CONFIG["model_config"]

          # 2. æå–åŸºç¡€é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼å…œåº•
          validated_config = {
              "chat_model": origin_model_config.get("chat_model", {}),
              "embedding_model": origin_model_config.get("embedding_model", {}),
              "reranker_model": origin_model_config.get("reranker_model", {})  # æå–rerankeræ¨¡å‹é…ç½®
          }

          # 3. å­æ¨¡å‹æ ¡éªŒé€šç”¨é€»è¾‘ï¼ˆä½¿ç”¨ModelManageréªŒè¯ï¼‰
          def _validate_sub_model(
                  sub_model_key: str,  # å­æ¨¡å‹é”®åï¼ˆå¦‚"chat_model"ï¼‰
                  expected_type: str  # é¢„æœŸç±»å‹ï¼ˆå¦‚"chat"ï¼‰
          ) -> Dict[str, Any]:
              """æ ¡éªŒå•ä¸ªå­æ¨¡å‹çš„é…ç½®ï¼ˆæ”¯æŒä»»æ„ç±»å‹çš„å­æ¨¡å‹ï¼Œä¿æŒé€šç”¨æ€§ï¼‰"""
              user_sub_config = validated_config[sub_model_key]
              if not isinstance(user_sub_config, dict):
                  user_sub_config = {}

              # ä»é»˜è®¤é…ç½®ä¸­è·å–è¯¥å­æ¨¡å‹çš„åŸºå‡†é…ç½®
              default_sub_config = DEFAULT_APP_CONFIG["model_config"][sub_model_key]

              # æ ¡éªŒtype
              sub_type = user_sub_config.get("type", default_sub_config["type"])
              if not isinstance(sub_type, str) or sub_type != expected_type:
                  sub_type = default_sub_config["type"]

              # æ ¡éªŒmodel nameï¼ˆä½¿ç”¨ModelManagerï¼‰
              sub_name = user_sub_config.get("name", default_sub_config["name"])
              if not isinstance(sub_name, str) or not sub_name:
                  sub_name = default_sub_config["name"]
              else:
                  # ä½¿ç”¨ModelManageréªŒè¯æ¨¡å‹æ˜¯å¦å­˜åœ¨ä¸”å¯ç”¨
                  is_valid, error_msg = self.model_service.model_manager.validate_model(sub_name)
                  if not is_valid:
                      # æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
                      sub_name = default_sub_config["name"]

              # æ ¡éªŒparametersï¼ˆåªæ ¡éªŒæ˜¯å¦ä¸ºå­—å…¸ï¼Œä¸æ ¡éªŒå…·ä½“å‚æ•°ï¼‰
              user_params = user_sub_config.get("parameters", {})
              if not isinstance(user_params, dict):
                  user_params = default_sub_config.get("parameters", {})

              return {
                  "type": sub_type,
                  "name": sub_name,
                  "parameters": user_params  # ç›´æ¥ä½¿ç”¨ç”¨æˆ·å‚æ•°ï¼Œç”±model_serviceéªŒè¯
              }

          # 4. æ‰§è¡Œå­æ¨¡å‹æ ¡éªŒ
          if validated_config["chat_model"]:
              validated_config["chat_model"] = _validate_sub_model("chat_model", "chat")
          if validated_config["embedding_model"]:
              validated_config["embedding_model"] = _validate_sub_model("embedding_model", "embedding")
          if validated_config["reranker_model"]:
              validated_config["reranker_model"] = _validate_sub_model("reranker_model", "reranker")
          # validated_config["chat_model"] = _validate_sub_model("chat_model", "chat")
          # validated_config["embedding_model"] = _validate_sub_model("embedding_model", "embedding")
          # validated_config["reranker_model"] = _validate_sub_model("reranker_model", "reranker")

          return validated_config

      async def _generate_llm_explanation(
          self,
          chat_model: Any,
          patient_info,
          clinical_context,
          scenarios_with_recommendations: List[Dict]
      ) -> Dict[str, Any]:
          """
          ä½¿ç”¨LLMç”Ÿæˆæ¨èè¯´æ˜
          
          åŸºäºæ£€ç´¢åˆ°çš„ä¸´åºŠåœºæ™¯å’Œæ¨èé¡¹ç›®ï¼Œç”Ÿæˆäººæ€§åŒ–çš„æ¨èè§£é‡Š
          """
          # æ„å»ºæç¤ºè¯
          prompt = self._build_recommendation_prompt(
              patient_info,
              clinical_context,
              scenarios_with_recommendations
          )
          
          try:
              # è°ƒç”¨èŠå¤©æ¨¡å‹
              response = await self._call_chat_model(chat_model, prompt)
              return {
                  'explanation': response,
                  'generated': True
              }
          except Exception as e:
              return {
                  'explanation': f"LLMç”Ÿæˆå¤±è´¥: {str(e)}",
                  'generated': False
              }
      
      def _build_recommendation_prompt(
          self,
          patient_info,
          clinical_context,
          scenarios_with_recommendations: List[Dict]
      ) -> str:
          """
          æ„å»ºLLMæç¤ºè¯
          """
          prompt_parts = [
              "# ä¸´åºŠæ¨èä»»åŠ¡",
              "",
              "## æ‚£è€…ä¿¡æ¯",
              f"- å¹´é¾„: {patient_info.age}å²" if patient_info.age else "",
              f"- æ€§åˆ«: {patient_info.gender}" if patient_info.gender else "",
              f"- å¦Šå¨ çŠ¶æ€: {patient_info.pregnancy_status}" if patient_info.pregnancy_status else "",
              "",
              "## ä¸´åºŠä¸Šä¸‹æ–‡",
              f"- ä¸»è¯‰: {clinical_context.chief_complaint}",
              f"- æ—¢å¾€ç—…å²: {clinical_context.medical_history}" if clinical_context.medical_history else "",
              f"- è¯Šæ–­: {clinical_context.diagnosis}" if clinical_context.diagnosis else "",
              "",
              "## æ£€ç´¢åˆ°çš„ä¸´åºŠåœºæ™¯ä¸æ¨è",
              ""
          ]
          
          # æ·»åŠ åœºæ™¯å’Œæ¨è
          for i, scenario in enumerate(scenarios_with_recommendations[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
              prompt_parts.append(f"### åœºæ™¯ {i}: {scenario['scenario_description']}")
              prompt_parts.append(f"åŒ¹é…åˆ†æ•°: {scenario['matching_scores']['final_score']:.2f}")
              prompt_parts.append("")
              prompt_parts.append("æ¨èæ£€æŸ¥é¡¹ç›®:")
              for rec in scenario['recommendations'][:5]:  # æ¯ä¸ªåœºæ™¯æ˜¾ç¤ºå‰5ä¸ªæ¨è
                  prompt_parts.append(
                      f"- {rec['procedure_name']} (é€‚å®œæ€§: {rec['appropriateness_rating']}/9)"
                  )
              prompt_parts.append("")
          
          prompt_parts.extend([
              "",
              "## ä»»åŠ¡è¦æ±‚",
              "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´ã€ä¸“ä¸šçš„ä¸´åºŠæ£€æŸ¥æ¨èè¯´æ˜ï¼ŒåŒ…æ‹¬ï¼š",
              "1. æœ€é€‚åˆçš„æ£€æŸ¥é¡¹ç›®ï¼ˆå‰3é¡¹ï¼‰",
              "2. é€‰æ‹©ç†ç”±ï¼ˆç»“åˆæ‚£è€…æƒ…å†µå’Œä¸´åºŠåœºæ™¯ï¼‰",
              "3. æ³¨æ„äº‹é¡¹ï¼ˆå¦‚æœ‰ï¼‰",
              "",
              "è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸šã€‚"
          ])
          
          return "\n".join(filter(None, prompt_parts))
      
      async def _call_chat_model(self, chat_model: Any, prompt: str) -> str:
          """
          è°ƒç”¨èŠå¤©æ¨¡å‹
          """
          # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„èŠå¤©æ¨¡å‹æ¥å£å®ç°
          if hasattr(chat_model, 'chat'):
              result = await chat_model.chat(prompt)
              return result
          elif hasattr(chat_model, '__call__'):
              result = await chat_model(prompt)
              return result
          else:
              raise NotImplementedError("èŠå¤©æ¨¡å‹æ¥å£æœªå®ç°")














