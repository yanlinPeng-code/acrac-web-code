from typing import Any, Dict, List
import time
from sqlmodel.ext.asyncio.session import AsyncSession
from app.core.language_model.model_client_wrapper import ChatClientSDK, EmbeddingClientSDK, RerankerClientSDK
from app.entity.model_entity import DEFAULT_APP_CONFIG
from app.entity.retrieval_entity import RerankingStrategy
from app.response.exception.exceptions import ValidationException, InternalServerException
from app.schema.IntelligentRecommendation_schemas import (
    IntelligentRecommendationRequest,
    IntelligentRecommendationResponse,
    SearchStrategy, RetrievalRequest
)
from app.service.rag_v1.model_service import ModelService
from app.service.rag_v1.retrieval_service import RetrievalService
from app.service.rag_v1.simple_retrieval_service import SimpleRetrievalService
from app.service.rag_v1.adaptive_recommend_service import AdaptiveRecommendationEngineService
from app.utils.logger.simple_logger import get_logger
logger = get_logger(__name__)
"""
ç§‘å®¤: å¿ƒè¡€ç®¡å†…ç§‘ ,
ä¸´åºŠåœºæ™¯: 35å²ç”·æ€§ï¼Œé«˜è¡€å‹ç—…å²3å¹´ï¼Œè§„å¾‹æœè¯ï¼Œé’éœ‰ç´ è¿‡æ•ï¼Œä¸»è¯‰åå¤å¤´ç—›ã€å¤´æ™•ä¼´è€³é¸£1å‘¨ï¼Œ
ç—‡çŠ¶ä¸­åº¦ï¼Œä½“æ¸©36.8â„ƒï¼Œè¡€å‹130/85 mmHgï¼Œå¿ƒè‚ºå¬è¯Šæ­£å¸¸ï¼Œè€ƒè™‘åŸå‘æ€§é«˜è¡€å‹ã€‚
"""

class RagService:

      def __init__(self,
                   session: AsyncSession,
                   model_service: ModelService,
                   retrieval_service:RetrievalService,
                   simple_retrieval_service:SimpleRetrievalService,
                   ):
          self.session = session
          self.model_service = model_service
          self.retrieval_service = retrieval_service
          self.simple_retrieval_service=simple_retrieval_service

      async def generate_intelligent_recommendation(
              self,
              request: IntelligentRecommendationRequest,
              medical_dict: Dict[str, Any]
              ) -> IntelligentRecommendationResponse:
          """
          ç”Ÿæˆæ™ºèƒ½æ¨è - ä½¿ç”¨ç­–ç•¥æšä¸¾ç»Ÿä¸€å¤„ç†

          æµç¨‹ï¼š
          1. æ‰§è¡Œå››é˜¶æ®µæ··åˆæ£€ç´¢
          2. æ ¹æ®ç­–ç•¥æšä¸¾æ‰§è¡Œç›¸åº”çš„é‡æ’åºé€»è¾‘
          3. è¿”å›ç»“æ„åŒ–ç»“æœ
          """
          standard_query=""
          start_time = time.time()
          if request.standard_query and isinstance(request.standard_query, str):
              standard_query = request.standard_query
          try:
              # ========== 1. æ‰§è¡Œå››é˜¶æ®µæ··åˆæ£€ç´¢ ==========


              search_strategy = request.search_strategy or SearchStrategy()

              # è·å–æ£€ç´¢ç­–ç•¥é…ç½®
              retrieval_config = request.retrieval_strategy or RetrievalRequest()
              strategy = retrieval_config.reranking_strategy

              logger.info(f"ğŸš€ å¼€å§‹æ™ºèƒ½æ¨èï¼Œä½¿ç”¨ç­–ç•¥: {strategy.value}")
              logger.info(f"   è§„åˆ™è¿‡æ»¤: {retrieval_config.apply_rule_filter}, "
                          f"LLMé‡æ’åº: {retrieval_config.enable_reranking}, "
                          f"LLMæ¨è: {retrieval_config.need_llm_recommendations}")

              # è®¡ç®—åˆå§‹æ£€ç´¢æ•°é‡
              initial_top_k = self._calculate_initial_top_k(retrieval_config)

              final_scenarios = await self.retrieval_service.retrieve_clinical_scenarios(
                  patient_info=request.patient_info,
                  clinical_context=request.clinical_context,
                  standard_query=standard_query,
                  search_strategy=search_strategy,
                  top_k=initial_top_k,
                  similarity_threshold=retrieval_config.similarity_threshold,
                  medical_dict=medical_dict
              )

              # ========== 2. æ ¹æ®ç­–ç•¥æ‰§è¡Œé‡æ’åº ==========
              best_recommendations = await self.retrieval_service.llm_rank_all_scenarios(
                  all_scenarios=final_scenarios,
                  patient_info=request.patient_info,
                  clinical_context=request.clinical_context,
                  strategy=strategy,
                  min_rating=retrieval_config.min_appropriateness_rating or 5,
                  direct_return=request.direct_return,
                  max_scenarios=retrieval_config.top_scenarios,
                  max_recommendations_per_scenario=retrieval_config.top_recommendations_per_scenario
              )

              # ========== 3. è®¡ç®—å¤„ç†æ—¶é—´ ==========
              processing_time_ms = int((time.time() - start_time) * 1000)

              # ========== 4. è¿”å›ç»“æ„åŒ–å“åº” ==========
              return IntelligentRecommendationResponse(
                  query=f"{request.clinical_context.chief_complaint} | {request.clinical_context.diagnosis or ''}",
                  best_recommendations=best_recommendations,
                  processing_time_ms=processing_time_ms,
                  similarity_threshold=retrieval_config.similarity_threshold,
                  strategy_used=strategy.value
              )

          except Exception as e:
              logger.error(f"âŒ æ™ºèƒ½æ¨èå¤±è´¥: {str(e)}")
              processing_time_ms = int((time.time() - start_time) * 1000)
              return IntelligentRecommendationResponse(
                  best_recommendations=[],
                  processing_time_ms=processing_time_ms,
                  error_message=str(e)
              )

      async def stream_direct_recommendation(
              self,
              request: IntelligentRecommendationRequest,
              medical_dict: Dict[str, Any]
      ):
          """æµå¼è¿”å›LLMç”Ÿæˆçš„ç›´æ¥æ¨èæ–‡æœ¬ï¼ˆå…ˆæ¨èé¡¹ç›®ï¼Œå†æ¨èç†ç”±ï¼‰ã€‚"""
          search_strategy = request.search_strategy or SearchStrategy()
          retrieval_config = request.retrieval_strategy or RetrievalRequest()
          initial_top_k = self._calculate_initial_top_k(retrieval_config)

          scenarios = await self.retrieval_service.retrieve_clinical_scenarios(
              patient_info=request.patient_info,
              clinical_context=request.clinical_context,
              standard_query=request.standard_query or "",
              search_strategy=search_strategy,
              need_optimize_query=request.need_optimize_query,
              top_k=initial_top_k,
              similarity_threshold=retrieval_config.similarity_threshold,
              medical_dict=medical_dict
          )
          scenarios_with_recs = await self.retrieval_service.get_scenarios_with_recommends(
              scenarios,
              max_scenarios=retrieval_config.top_scenarios,
              max_recommendations_per_scenario=retrieval_config.top_recommendations_per_scenario,
              min_rating=retrieval_config.min_appropriateness_rating or 5
          )
          engine = AdaptiveRecommendationEngineService()
          prompt = engine._build_single_call_prompt(
              confirmed_scenarios=scenarios_with_recs,
              patient_info=request.patient_info,
              clinical_context=request.clinical_context,
              max_recommendations_per_scenario=retrieval_config.top_recommendations_per_scenario,
              direct_return=True
          )
          async for chunk in self.retrieval_service.ai_service._stream_llm(prompt):
              yield chunk

      def _calculate_initial_top_k(self, retrieval_config: RetrievalRequest) -> int:
          """è®¡ç®—åˆå§‹æ£€ç´¢æ•°é‡"""
          strategy = retrieval_config.reranking_strategy
          base_k = retrieval_config.top_scenarios

          # æ ¹æ®ç­–ç•¥å†³å®šåˆå§‹æ£€ç´¢æ•°é‡
          strategy_multipliers = {
              RerankingStrategy.NONE: 1,
              RerankingStrategy.RULE_ONLY: 3,
              RerankingStrategy.LLM_SCENARIO_ONLY: 4,
              RerankingStrategy.LLM_RECOMMENDATION_ONLY: 1,
              RerankingStrategy.RULE_AND_LLM_SCENARIO: 5,
              RerankingStrategy.RULE_AND_LLM_RECOMMENDATION: 4,
              RerankingStrategy.LLM_SCENARIO_AND_RECOMMENDATION: 4,
              RerankingStrategy.ALL: 6
          }

          multiplier = strategy_multipliers.get(strategy, 3)
          return max(30, base_k * multiplier)

      async  def generate_simple_recommendation(self,
           request: IntelligentRecommendationRequest,
           medical_dict: Dict[str, Any]):
          standard_query=""
          start_time = time.time()
          try:
              if request.standard_query and isinstance(request.standard_query,str):
                  standard_query=request.standard_query


              if request.retrieval_strategy.top_scenarios>=5:
                  raise ValidationException(message="è¯·æ±‚çš„æœ€å¤§åœºæ™¯æ•°ä¸èƒ½è¶…è¿‡5ä¸ªï¼")
             
              # ========== 1. æ‰§è¡Œå››é˜¶æ®µæ··åˆæ£€ç´¢ ==========
              search_strategy = request.search_strategy or SearchStrategy()

              # è·å–æ£€ç´¢ç­–ç•¥é…ç½®
              retrieval_config = request.retrieval_strategy or RetrievalRequest()
              strategy = retrieval_config.reranking_strategy

              logger.info(f"ğŸš€ å¼€å§‹æ™ºèƒ½æ¨èï¼Œä½¿ç”¨ç­–ç•¥: {strategy.value}")
              logger.info(f"   è§„åˆ™è¿‡æ»¤: {retrieval_config.apply_rule_filter}, "
                          f"LLMé‡æ’åº: {retrieval_config.enable_reranking}, "
                          f"LLMæ¨è: {retrieval_config.need_llm_recommendations}")




              # è®¡ç®—åˆå§‹æ£€ç´¢æ•°é‡
              initial_top_k = self._calculate_initial_top_k(retrieval_config)

              final_scenarios = await self.retrieval_service.retrieve_clinical_scenarios(
                  patient_info=request.patient_info,
                  clinical_context=request.clinical_context,
                  standard_query=standard_query,
                  search_strategy=search_strategy,
                  top_k=initial_top_k,
                  similarity_threshold=retrieval_config.similarity_threshold,
                  medical_dict=medical_dict
              )
              # ========== 2. æ ¹æ®ç­–ç•¥æ‰§è¡Œé‡æ’åº ==========
              best_recommendations = await self.simple_retrieval_service.simple_rank_all_scenarios(
                  all_scenarios=final_scenarios,
                  patient_info=request.patient_info,
                  clinical_context=request.clinical_context,
                  strategy=strategy,
                  min_rating=retrieval_config.min_appropriateness_rating or 5,
                  direct_return=request.direct_return,
                  max_scenarios=retrieval_config.top_scenarios,
                  max_recommendations_per_scenario=retrieval_config.top_recommendations_per_scenario
              )

              # ========== 3. è®¡ç®—å¤„ç†æ—¶é—´ ==========
              processing_time_ms = int((time.time() - start_time) * 1000)

              # ========== 4. è¿”å›ç»“æ„åŒ–å“åº” ==========
              return IntelligentRecommendationResponse(
                  query=f"{request.clinical_context.chief_complaint} | {request.clinical_context.diagnosis or ''}",
                  best_recommendations=best_recommendations,
                  processing_time_ms=processing_time_ms,
                  similarity_threshold=retrieval_config.similarity_threshold,
                  strategy_used=strategy.value
              )

          except Exception as e:
               raise InternalServerException(
                   message=str(e)
               )










      async def _process_and_validate_model_config(self, origin_model_config: dict[str, Any]) -> dict[str, Any]:
          """
          å¤„ç†å¹¶æ ¡éªŒæ¨¡å‹é…ç½®ï¼ˆæ”¯æŒchat/embedding/rerankerä¸‰ç§å­æ¨¡å‹ï¼‰ï¼Œè¿”å›æ ¡éªŒåçš„é…ç½®

          æ ¡éªŒé€»è¾‘ï¼š
          1. æ•´ä½“ç»“æ„æ ¡éªŒï¼ˆå¿…é¡»ä¸ºå­—å…¸ï¼‰
          2. ä¸‰ç±»å­æ¨¡å‹ï¼ˆchat_model/embedding_model/reranker_modelï¼‰åˆ†åˆ«æ ¡éªŒï¼š
             - ç±»å‹åˆæ³•æ€§ï¼ˆtypeå¿…é¡»åŒ¹é…ï¼‰
             - æ¨¡å‹åç§°ï¼ˆnameï¼‰æœ‰æ•ˆæ€§ï¼ˆä½¿ç”¨ModelManageréªŒè¯ï¼‰
             - å‚æ•°ï¼ˆparametersï¼‰ç»“æ„æ ¡éªŒ
          
          æ³¨æ„ï¼šä¸å†ä¾èµ–language_model_managerï¼Œä½¿ç”¨ModelManagerè¿›è¡Œæ¨¡å‹éªŒè¯
          """
          # 1. åŸºç¡€æ ¡éªŒï¼šè¾“å…¥å¿…é¡»ä¸ºå­—å…¸ï¼Œå¦åˆ™è¿”å›é»˜è®¤é…ç½®
          if not isinstance(origin_model_config, dict):
              return DEFAULT_APP_CONFIG["model_config"]

          # 2. æå–åŸºç¡€é…ç½®ï¼Œä½¿ç”¨é»˜è®¤å€¼å…œåº•
          validated_config = {
              "chat_model": origin_model_config.get("chat_model", {}),
              "embedding_model": origin_model_config.get("embedding_model", {}),
              "reranker_model": origin_model_config.get("reranker_model", {})  # æå–rerankeræ¨¡å‹é…ç½®
          }

          # 3. å­æ¨¡å‹æ ¡éªŒé€šç”¨é€»è¾‘ï¼ˆä½¿ç”¨ModelManageréªŒè¯ï¼‰
          def _validate_sub_model(
                  sub_model_key: str,  # å­æ¨¡å‹é”®åï¼ˆå¦‚"chat_model"ï¼‰
                  expected_type: str  # é¢„æœŸç±»å‹ï¼ˆå¦‚"chat"ï¼‰
          ) -> Dict[str, Any]:
              """æ ¡éªŒå•ä¸ªå­æ¨¡å‹çš„é…ç½®ï¼ˆæ”¯æŒä»»æ„ç±»å‹çš„å­æ¨¡å‹ï¼Œä¿æŒé€šç”¨æ€§ï¼‰"""
              user_sub_config = validated_config[sub_model_key]
              if not isinstance(user_sub_config, dict):
                  user_sub_config = {}

              # ä»é»˜è®¤é…ç½®ä¸­è·å–è¯¥å­æ¨¡å‹çš„åŸºå‡†é…ç½®
              default_sub_config = DEFAULT_APP_CONFIG["model_config"][sub_model_key]

              # æ ¡éªŒtype
              sub_type = user_sub_config.get("type", default_sub_config["type"])
              if not isinstance(sub_type, str) or sub_type != expected_type:
                  sub_type = default_sub_config["type"]

              # æ ¡éªŒmodel nameï¼ˆä½¿ç”¨ModelManagerï¼‰
              sub_name = user_sub_config.get("name", default_sub_config["name"])
              if not isinstance(sub_name, str) or not sub_name:
                  sub_name = default_sub_config["name"]
              else:
                  # ä½¿ç”¨ModelManageréªŒè¯æ¨¡å‹æ˜¯å¦å­˜åœ¨ä¸”å¯ç”¨
                  is_valid, error_msg = self.model_service.model_manager.validate_model(sub_name)
                  if not is_valid:
                      # æ¨¡å‹ä¸å­˜åœ¨æˆ–ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å€¼
                      sub_name = default_sub_config["name"]

              # æ ¡éªŒparametersï¼ˆåªæ ¡éªŒæ˜¯å¦ä¸ºå­—å…¸ï¼Œä¸æ ¡éªŒå…·ä½“å‚æ•°ï¼‰
              user_params = user_sub_config.get("parameters", {})
              if not isinstance(user_params, dict):
                  user_params = default_sub_config.get("parameters", {})

              return {
                  "type": sub_type,
                  "name": sub_name,
                  "parameters": user_params  # ç›´æ¥ä½¿ç”¨ç”¨æˆ·å‚æ•°ï¼Œç”±model_serviceéªŒè¯
              }

          # 4. æ‰§è¡Œå­æ¨¡å‹æ ¡éªŒ
          if validated_config["chat_model"]:
              validated_config["chat_model"] = _validate_sub_model("chat_model", "chat")
          if validated_config["embedding_model"]:
              validated_config["embedding_model"] = _validate_sub_model("embedding_model", "embedding")
          if validated_config["reranker_model"]:
              validated_config["reranker_model"] = _validate_sub_model("reranker_model", "reranker")
          # validated_config["chat_model"] = _validate_sub_model("chat_model", "chat")
          # validated_config["embedding_model"] = _validate_sub_model("embedding_model", "embedding")
          # validated_config["reranker_model"] = _validate_sub_model("reranker_model", "reranker")

          return validated_config

      async def _generate_llm_explanation(
          self,
          chat_model: Any,
          patient_info,
          clinical_context,
          scenarios_with_recommendations: List[Dict]
      ) -> Dict[str, Any]:
          """
          ä½¿ç”¨LLMç”Ÿæˆæ¨èè¯´æ˜
          
          åŸºäºæ£€ç´¢åˆ°çš„ä¸´åºŠåœºæ™¯å’Œæ¨èé¡¹ç›®ï¼Œç”Ÿæˆäººæ€§åŒ–çš„æ¨èè§£é‡Š
          """
          # æ„å»ºæç¤ºè¯
          prompt = self._build_recommendation_prompt(
              patient_info,
              clinical_context,
              scenarios_with_recommendations
          )
          
          try:
              # è°ƒç”¨èŠå¤©æ¨¡å‹
              response = await self._call_chat_model(chat_model, prompt)
              return {
                  'explanation': response,
                  'generated': True
              }
          except Exception as e:
              return {
                  'explanation': f"LLMç”Ÿæˆå¤±è´¥: {str(e)}",
                  'generated': False
              }
      
      def _build_recommendation_prompt(
          self,
          patient_info,
          clinical_context,
          scenarios_with_recommendations: List[Dict]
      ) -> str:
          """
          æ„å»ºLLMæç¤ºè¯
          """
          prompt_parts = [
              "# ä¸´åºŠæ¨èä»»åŠ¡",
              "",
              "## æ‚£è€…ä¿¡æ¯",
              f"- å¹´é¾„: {patient_info.age}å²" if patient_info.age else "",
              f"- æ€§åˆ«: {patient_info.gender}" if patient_info.gender else "",
              f"- å¦Šå¨ çŠ¶æ€: {patient_info.pregnancy_status}" if patient_info.pregnancy_status else "",
              "",
              "## ä¸´åºŠä¸Šä¸‹æ–‡",
              f"- ä¸»è¯‰: {clinical_context.chief_complaint}",
              f"- æ—¢å¾€ç—…å²: {clinical_context.medical_history}" if clinical_context.medical_history else "",
              f"- è¯Šæ–­: {clinical_context.diagnosis}" if clinical_context.diagnosis else "",
              "",
              "## æ£€ç´¢åˆ°çš„ä¸´åºŠåœºæ™¯ä¸æ¨è",
              ""
          ]
          
          # æ·»åŠ åœºæ™¯å’Œæ¨è
          for i, scenario in enumerate(scenarios_with_recommendations[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ª
              prompt_parts.append(f"### åœºæ™¯ {i}: {scenario['scenario_description']}")
              prompt_parts.append(f"åŒ¹é…åˆ†æ•°: {scenario['matching_scores']['final_score']:.2f}")
              prompt_parts.append("")
              prompt_parts.append("æ¨èæ£€æŸ¥é¡¹ç›®:")
              for rec in scenario['recommendations'][:5]:  # æ¯ä¸ªåœºæ™¯æ˜¾ç¤ºå‰5ä¸ªæ¨è
                  prompt_parts.append(
                      f"- {rec['procedure_name']} (é€‚å®œæ€§: {rec['appropriateness_rating']}/9)"
                  )
              prompt_parts.append("")
          
          prompt_parts.extend([
              "",
              "## ä»»åŠ¡è¦æ±‚",
              "è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯ï¼Œç”Ÿæˆä¸€ä»½ç®€æ´ã€ä¸“ä¸šçš„ä¸´åºŠæ£€æŸ¥æ¨èè¯´æ˜ï¼ŒåŒ…æ‹¬ï¼š",
              "1. æœ€é€‚åˆçš„æ£€æŸ¥é¡¹ç›®ï¼ˆå‰3é¡¹ï¼‰",
              "2. é€‰æ‹©ç†ç”±ï¼ˆç»“åˆæ‚£è€…æƒ…å†µå’Œä¸´åºŠåœºæ™¯ï¼‰",
              "3. æ³¨æ„äº‹é¡¹ï¼ˆå¦‚æœ‰ï¼‰",
              "",
              "è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€ç®€æ´ä¸“ä¸šã€‚"
          ])
          
          return "\n".join(filter(None, prompt_parts))
      
      async def _call_chat_model(self, chat_model: Any, prompt: str) -> str:
          """
          è°ƒç”¨èŠå¤©æ¨¡å‹
          """
          # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…çš„èŠå¤©æ¨¡å‹æ¥å£å®ç°
          if hasattr(chat_model, 'chat'):
              result = await chat_model.chat(prompt)
              return result
          elif hasattr(chat_model, '__call__'):
              result = await chat_model(prompt)
              return result
          else:
              raise NotImplementedError("èŠå¤©æ¨¡å‹æ¥å£æœªå®ç°")


if __name__ == '__main__':
    import asyncio
    from app.schema.IntelligentRecommendation_schemas import PatientInfo, ClinicalContext
    
    async def test():
        # è¿™é‡Œéœ€è¦å®é™…çš„æ•°æ®åº“ä¼šè¯
        # rag_service = RagService(
        #     session=session,
        #     model_service=LanguageModelService(),
        #     language_model_manager=LanguageModelManager()
        # )
        # 
        # request = IntelligentRecommendationRequest(
        #     patient_info=PatientInfo(
        #         age=45,
        #         gender="female",
        #         pregnancy_status="not_applicable"
        #     ),
        #     clinical_context=ClinicalContext(
        #         chief_complaint="èƒ¸ç—›",
        #         diagnosis="ç–‘ä¼¼å† å¿ƒç—…"
        #     )
        # )
        # 
        # response = await rag_service.generate_intelligent_recommendation(request)
        # print(response)
        pass
    
    asyncio.run(test())











