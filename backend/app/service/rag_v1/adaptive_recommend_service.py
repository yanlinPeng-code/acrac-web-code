
import time
import logging
from typing import List, Dict, Any, Tuple, Optional
import dashscope
from app.schema.IntelligentRecommendation_schemas import PatientInfo, ClinicalContext
from app.service.rag_v1.ai_service import AiService
from app.utils.helper.helper import safe_parse_llm_response, safe_process_recommendation_grades

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)




class AdaptiveThresholdStrategy:
    """åŸºç¡€é˜ˆå€¼ç­–ç•¥"""

    def __init__(self):
        self.threshold_config = {
            'token_threshold':  4096,
            'max_scenarios_single_call': 5,
            'max_total_recommendations': 30,
            'max_avg_recommendations_per_scenario': 8
        }

        self.weights = {
            'token_ratio': 0.5,
            'scenario_ratio': 0.2,
            'total_recommendations_ratio': 0.2,
            'avg_recommendations_ratio': 0.1
        }

    def should_use_concurrent(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            estimated_tokens: int
    ) -> Tuple[bool, Dict[str, Any]]:
        """å†³å®šæ˜¯å¦ä½¿ç”¨å¹¶å‘å¤„ç†"""

        scenario_count = len(confirmed_scenarios)
        total_recommendations = sum(
            len(scenario.get('recommendations', []))
            for scenario in confirmed_scenarios
        )
        avg_recommendations = total_recommendations / max(scenario_count, 1)

        # è®¡ç®—å„ç»´åº¦æ¯”ç‡
        token_ratio = estimated_tokens / self.threshold_config['token_threshold']
        scenario_ratio = scenario_count / self.threshold_config['max_scenarios_single_call']
        total_rec_ratio = total_recommendations / self.threshold_config['max_total_recommendations']
        avg_rec_ratio = avg_recommendations / self.threshold_config['max_avg_recommendations_per_scenario']

        # è®¡ç®—ç»¼åˆè¯„åˆ†
        composite_score = (
                token_ratio * self.weights['token_ratio'] +
                scenario_ratio * self.weights['scenario_ratio'] +
                total_rec_ratio * self.weights['total_recommendations_ratio'] +
                avg_rec_ratio * self.weights['avg_recommendations_ratio']
        )

        # ç¡¬æ€§æ¡ä»¶
        hard_conditions = [
            token_ratio > 1.0,
            scenario_ratio > 1.5,
            total_rec_ratio > 2.0,
            avg_rec_ratio > 1.8
        ]

        use_concurrent = composite_score > 1.0 or any(hard_conditions)

        decision_metrics = {
            'composite_score': composite_score,
            'dimensions': {
                'tokens': {
                    'value': estimated_tokens,
                    'threshold': self.threshold_config['token_threshold'],
                    'ratio': token_ratio
                },
                'scenarios': {
                    'value': scenario_count,
                    'threshold': self.threshold_config['max_scenarios_single_call'],
                    'ratio': scenario_ratio
                },
                'total_recommendations': {
                    'value': total_recommendations,
                    'threshold': self.threshold_config['max_total_recommendations'],
                    'ratio': total_rec_ratio
                },
                'avg_recommendations': {
                    'value': avg_recommendations,
                    'threshold': self.threshold_config['max_avg_recommendations_per_scenario'],
                    'ratio': avg_rec_ratio
                }
            },
            'hard_conditions_triggered': [
                'token_exceeded' if token_ratio > 1.0 else None,
                'scenarios_exceeded' if scenario_ratio > 1.5 else None,
                'total_recommendations_exceeded' if total_rec_ratio > 2.0 else None,
                'avg_recommendations_exceeded' if avg_rec_ratio > 1.8 else None
            ],
            'decision_reason': self._get_decision_reason(composite_score, hard_conditions)
        }

        return use_concurrent, decision_metrics

    def _get_decision_reason(self, composite_score: float, hard_conditions: List[bool]) -> str:
        """ç”Ÿæˆå†³ç­–ç†ç”±"""
        if composite_score > 1.0:
            return f"ç»¼åˆè¯„åˆ†{composite_score:.2f}è¶…è¿‡é˜ˆå€¼1.0"

        triggered = [cond for cond in hard_conditions if cond]
        if triggered:
            return f"è§¦å‘{len(triggered)}ä¸ªç¡¬æ€§æ¡ä»¶"

        return f"ç»¼åˆè¯„åˆ†{composite_score:.2f}æœªè¶…è¿‡é˜ˆå€¼"


class LearningThresholdStrategy(AdaptiveThresholdStrategy):
    """åŸºäºå†å²æ€§èƒ½å­¦ä¹ çš„é˜ˆå€¼ç­–ç•¥"""

    def __init__(self):
        super().__init__()
        self.performance_history = []
        self.learning_enabled = True

    def update_based_on_performance(
            self,
            decision_metrics: Dict[str, Any],
            actual_processing_time: float,
            success: bool,
            strategy_used: str
    ):
        """æ ¹æ®å®é™…æ€§èƒ½æ›´æ–°é˜ˆå€¼"""
        if not self.learning_enabled:
            return

        record = {
            'decision_metrics': decision_metrics,
            'processing_time': actual_processing_time,
            'success': success,
            'strategy_used': strategy_used,
            'timestamp': time.time()
        }

        self.performance_history.append(record)

        # ä¿ç•™æœ€è¿‘100æ¡è®°å½•
        if len(self.performance_history) > 100:
            self.performance_history = self.performance_history[-100:]

        # å®šæœŸè°ƒæ•´é˜ˆå€¼
        if len(self.performance_history) % 20 == 0:
            self._adjust_thresholds_based_on_history()

    def _adjust_thresholds_based_on_history(self):
        """åŸºäºå†å²æ€§èƒ½è°ƒæ•´é˜ˆå€¼"""
        single_call_records = [r for r in self.performance_history if r['strategy_used'] == 'single']
        concurrent_records = [r for r in self.performance_history if r['strategy_used'] == 'concurrent']

        if len(single_call_records) < 5 or len(concurrent_records) < 5:
            return

        # è®¡ç®—å¹³å‡å¤„ç†æ—¶é—´
        try:
            avg_single_time = sum(r['processing_time'] for r in single_call_records) / len(single_call_records)
            avg_concurrent_time = sum(r['processing_time'] for r in concurrent_records) / len(concurrent_records)

            # è®¡ç®—æˆåŠŸç‡
            single_success_rate = sum(1 for r in single_call_records if r['success']) / len(single_call_records)
            concurrent_success_rate = sum(1 for r in concurrent_records if r['success']) / len(concurrent_records)

            # æ ¹æ®æ€§èƒ½å·®å¼‚è°ƒæ•´é˜ˆå€¼
            time_ratio = avg_single_time / avg_concurrent_time if avg_concurrent_time > 0 else 1.0
            success_ratio = single_success_rate / concurrent_success_rate if concurrent_success_rate > 0 else 1.0

            # å¦‚æœå•æ¬¡è°ƒç”¨æ€§èƒ½æ›´å¥½ï¼Œé€‚å½“æé«˜é˜ˆå€¼
            if time_ratio < 0.8 and success_ratio > 0.9:
                self.threshold_config['token_threshold'] = min(
                    self.threshold_config['token_threshold'] * 1.1,
                    8000
                )
                logger.info(f"ğŸ“ˆ åŸºäºæ€§èƒ½æ•°æ®æé«˜tokené˜ˆå€¼è‡³: {self.threshold_config['token_threshold']}")

            # å¦‚æœå¹¶å‘è°ƒç”¨æ€§èƒ½æ›´å¥½ï¼Œé€‚å½“é™ä½é˜ˆå€¼
            elif time_ratio > 1.2 or success_ratio < 0.8:
                self.threshold_config['token_threshold'] = max(
                    self.threshold_config['token_threshold'] * 0.9,
                    2000
                )
                logger.info(f"ğŸ“‰ åŸºäºæ€§èƒ½æ•°æ®é™ä½tokené˜ˆå€¼è‡³: {self.threshold_config['token_threshold']}")
        except Exception as e:
            logger.warning(f"è°ƒæ•´é˜ˆå€¼æ—¶å‡ºé”™: {e}")


class AdaptiveRecommendationEngineService:
    """è‡ªé€‚åº”æ¨èå¼•æ“"""

    def __init__(self, environment: str = "production", use_adaptive: bool = True):
        self.environment = environment
        self.use_adaptive = use_adaptive
        self.tokenizer=dashscope.get_tokenizer("qwen-7b-chat")
        self.ai_service = AiService()


        # åˆå§‹åŒ–ç­–ç•¥
        if use_adaptive:
            self.strategy = LearningThresholdStrategy()
            logger.info("ğŸ”„ å¯ç”¨è‡ªé€‚åº”ç­–ç•¥")
        else:
            self.strategy = AdaptiveThresholdStrategy()
            logger.info("âš¡ ä½¿ç”¨å›ºå®šé˜ˆå€¼ç­–ç•¥")

        self._initialize_strategy()

    def _initialize_strategy(self):
        """åˆå§‹åŒ–ç­–ç•¥é…ç½®"""
        env_config = self.get_environment_specific_config(self.environment)
        self.strategy.threshold_config.update(env_config)
        logger.info(f"âœ… ç­–ç•¥åˆå§‹åŒ–å®Œæˆï¼Œç¯å¢ƒ: {self.environment}")

    def get_environment_specific_config(self, environment: str) -> Dict[str, Any]:
        """è·å–ç¯å¢ƒç‰¹å®šé…ç½®"""
        configs = {
            'development': {
                'token_threshold': 4096,
                'max_scenarios_single_call': 3,
                'max_total_recommendations': 20,
                'max_avg_recommendations_per_scenario': 6,
            },
            'production': {
                'token_threshold': 4096,
                'max_scenarios_single_call': 5,
                'max_total_recommendations': 50,
                'max_avg_recommendations_per_scenario': 10,
            },
            'local-qwen': {
                'token_threshold': 4096,
                'max_scenarios_single_call': 4,
                'max_total_recommendations': 25,
                'max_avg_recommendations_per_scenario': 7,
            }
        }
        return configs.get(environment, configs['production'])

    def estimate_tokens_with_tiktoken(self, text: str, model_name: str = "cl100k_base") -> int:
        """ä½¿ç”¨tiktokenè®¡ç®—tokenæ•°é‡"""
        try:
            try:
                return len(self.tokenizer.encode(text))
            except KeyError:
                import qwen_token_counter
                encoding = qwen_token_counter.get_token_count(text)
                return encoding
        except ImportError:
            logger.warning("tiktokenæœªå®‰è£…ï¼Œä½¿ç”¨å›é€€ä¼°ç®—æ–¹æ³•")
            return self._estimate_tokens_fallback(text)

    def _estimate_tokens_fallback(self, text: str) -> int:
        """tiktokenä¸å¯ç”¨æ—¶çš„å›é€€ä¼°ç®—æ–¹æ³•"""
        import re
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
        numbers = len(re.findall(r'\d+', text))
        punctuation = len(re.findall(r'[^\w\s\u4e00-\u9fff]', text))
        spaces = len(re.findall(r'\s', text))

        estimated_tokens = (
                chinese_chars * 2.3 +
                english_words * 1.3 +
                numbers * 0.8 +
                punctuation * 0.5 +
                spaces * 0.1
        )
        return int(estimated_tokens)

    def _build_single_call_prompt(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int,
            direct_return:bool,

    ) -> str:
        """æ„å»ºå•æ¬¡è°ƒç”¨æç¤ºè¯"""

        patient_info_content = self.build_patient_context(patient_info)
        clinical_context_content = self.build_clinical_context(clinical_context)
        scenarios_content = self._build_optimized_scenarios_content(confirmed_scenarios)
        task_instruction = self._build_optimized_task_instruction(
            len(confirmed_scenarios), max_recommendations_per_scenario,direct_return
        )

        return f"{patient_info_content}\n{clinical_context_content}\n{scenarios_content}\n{task_instruction}"

    def build_patient_context(self, patient_info: PatientInfo) -> str:
          """æ„å»ºæ‚£è€…ä¿¡æ¯"""
          # æ‚£è€…å’Œä¸´åºŠä¿¡æ¯
          patient_context = f"""
             ## æ‚£è€…åŸºæœ¬ä¿¡æ¯
             - **å¹´é¾„**: {patient_info.age}å²
             - **æ€§åˆ«**: {patient_info.gender}
             - **å¦Šå¨ çŠ¶æ€**: {patient_info.pregnancy_status or 'éå¦Šå¨ æœŸ'}
             - **è¿‡æ•å²**: {', '.join(patient_info.allergies) if patient_info.allergies else 'æ— '}
             - **åˆå¹¶ç—‡**: {', '.join(patient_info.comorbidities) if patient_info.comorbidities else 'æ— '}
            """
          return patient_context

    def build_clinical_context(self, clinical_context: ClinicalContext) -> str:
        """æ„å»ºä¸´åºŠä¸Šä¸‹æ–‡ï¼ˆç¤ºä¾‹å®ç°ï¼‰"""
        ## ä¸´åºŠä¸Šä¸‹æ–‡
        clinical_context_content = f"""
                   ### ä¸´åºŠä¿¡æ¯
                   - **å°±è¯Šç§‘å®¤**: {clinical_context.department}
                   - **ä¸»è¯‰**: {clinical_context.chief_complaint}
                   - **æ—¢å¾€ç—…å²**: {clinical_context.medical_history or 'æ— '}
                   - **ç°ç—…å²**: {clinical_context.present_illness or 'æ— '}
                   - **ä¸»è¯Šæ–­**: {clinical_context.diagnosis or 'å¾…è¯Šæ–­'}
                   - **ç—‡çŠ¶ä¸¥é‡ç¨‹åº¦**: {clinical_context.symptom_severity or 'æœªçŸ¥'}
                   - **ç—‡çŠ¶æŒç»­æ—¶é—´**: {clinical_context.symptom_duration or 'æœªçŸ¥'}
                   """
        return clinical_context_content

    def _build_optimized_scenarios_content(self, confirmed_scenarios: List[Dict[str, Any]]) -> str:
        """æ„å»ºä¼˜åŒ–çš„åœºæ™¯å†…å®¹"""
        # æ‰€æœ‰åœºæ™¯å’Œæ¨èé¡¹ç›®ï¼ˆåˆ©ç”¨å®Œæ•´å­—æ®µä¿¡æ¯ï¼‰
        scenarios_text = "## å¯é€‰ä¸´åºŠåœºæ™¯åŠæ¨èé¡¹ç›®\n\n"

        for scenario_idx, scenario_data in enumerate(confirmed_scenarios, 1):
            scenario = scenario_data['scenario']
            recommendations = scenario_data.get('recommendations', [])

            scenarios_text += f"### åœºæ™¯{scenario_idx}: {scenario.description_zh}\n"
            scenarios_text += f"- **åœºæ™¯ID**: {scenario.semantic_id}\n"
            scenarios_text += f"- **é€‚ç”¨ç§‘å®¤**: {scenario.panel.name_zh if hasattr(scenario, 'panel') else 'æœªçŸ¥'}\n"
            scenarios_text += f"- **é€‚ç”¨äººç¾¤**: {scenario.patient_population or 'æœªçŸ¥'}\n"
            scenarios_text += f"- **ä¸´åºŠèƒŒæ™¯**: {scenario.clinical_context or 'æ— '}\n\n"

            if not recommendations:
                scenarios_text += "  æš‚æ— æ¨èé¡¹ç›®\n\n"
                continue

            scenarios_text += "#### æ¨èé¡¹ç›®æ¸…å•:\n"
            for rec_idx, rec_data in enumerate(recommendations, 1):
                recommendation = rec_data['recommendation']
                procedure = rec_data['procedure']

                # æ£€æŸ¥é¡¹ç›®åŸºæœ¬ä¿¡æ¯
                scenarios_text += f"{rec_idx}. **{procedure.name_zh}** ({procedure.name_en})\n"

                # æ£€æŸ¥æŠ€æœ¯ç»†èŠ‚
                tech_details = []
                if procedure.modality:
                    tech_details.append(f"æ£€æŸ¥æ–¹å¼: {procedure.modality}")
                if procedure.body_part:
                    tech_details.append(f"æ£€æŸ¥éƒ¨ä½: {procedure.body_part}")
                # if procedure.exam_duration:
                #     tech_details.append(f"æ£€æŸ¥æ—¶é•¿: {procedure.exam_duration}åˆ†é’Ÿ")
                # if tech_details:
                #     scenarios_text += f"   - æŠ€æœ¯ç»†èŠ‚: {', '.join(tech_details)}\n"

                # å®‰å…¨æ€§å’Œå‡†å¤‡ä¿¡æ¯
                safety_info = []
                if procedure.contrast_used:
                    safety_info.append("ä½¿ç”¨å¯¹æ¯”å‰‚")
                if procedure.radiation_level:
                    safety_info.append(f"è¾å°„ç­‰çº§: {procedure.radiation_level}")
                # if procedure.preparation_required:
                #     safety_info.append("éœ€è¦å‡†å¤‡")
                if safety_info:
                    scenarios_text += f"   - å®‰å…¨ä¿¡æ¯: {', '.join(safety_info)}\n"

                # ACRæ¨èä¿¡æ¯
                scenarios_text += f"   - **ACRé€‚å®œæ€§è¯„åˆ†**: {recommendation.appropriateness_rating}/9\n"
                if recommendation.appropriateness_category_zh:
                    scenarios_text += f"   - é€‚å®œæ€§ç±»åˆ«: {recommendation.appropriateness_category_zh}\n"

                # è¯æ®å’Œå…±è¯†
                evidence_info = []
                if recommendation.evidence_level:
                    evidence_info.append(f"è¯æ®å¼ºåº¦: {recommendation.evidence_level}")
                # if recommendation.consensus_level:
                #     evidence_info.append(f"å…±è¯†æ°´å¹³: {recommendation.consensus_level}")
                # if recommendation.median_rating:
                #     evidence_info.append(f"ä¸­ä½æ•°è¯„åˆ†: {recommendation.median_rating}")
                # if evidence_info:
                #     scenarios_text += f"   - è¯æ®è´¨é‡: {', '.join(evidence_info)}\n"

                # è¾å°„å‰‚é‡ä¿¡æ¯
                dose_info = []
                if recommendation.adult_radiation_dose:
                    dose_info.append(f"æˆäººå‰‚é‡: {recommendation.adult_radiation_dose}")
                if recommendation.pediatric_radiation_dose:
                    dose_info.append(f"å„¿ç«¥å‰‚é‡: {recommendation.pediatric_radiation_dose}")
                if dose_info:
                    scenarios_text += f"   - è¾å°„å‰‚é‡: {', '.join(dose_info)}\n"

                # å®‰å…¨æ€§ä¿¡æ¯
                safety_info = []
                if recommendation.pregnancy_safety:
                    safety_info.append(f"å¦Šå¨ å®‰å…¨: {recommendation.pregnancy_safety}")
                if recommendation.contraindications:
                    contra = recommendation.contraindications[:50] + "..." if len(
                        recommendation.contraindications) > 50 else recommendation.contraindications
                    safety_info.append(f"ç¦å¿Œç—‡: {contra}")
                if safety_info:
                    scenarios_text += f"   - å®‰å…¨è€ƒè™‘: {', '.join(safety_info)}\n"

                # æ¨èç†ç”±
                # if recommendation.reasoning_zh:
                #     reasoning = recommendation.reasoning_zh[:80] + "..." if len(
                #         recommendation.reasoning_zh) > 80 else recommendation.reasoning_zh
                #     scenarios_text += f"   - æ¨èç†ç”±: {reasoning}\n"


                if recommendation.special_considerations:
                    special = recommendation.special_considerations[:80] + "..." if len(
                        recommendation.special_considerations) > 80 else recommendation.special_considerations
                    scenarios_text += f"   - ç‰¹æ®Šè€ƒè™‘: {special}\n"

                # æ ‡å‡†ç¼–ç ï¼ˆå¦‚æœ‰ï¼‰
                # code_info = []
                # if procedure.standard_code:
                #     code_info.append(f"æ ‡å‡†ç : {procedure.standard_code}")
                # if procedure.icd10_code:
                #     code_info.append(f"ICD10: {procedure.icd10_code}")
                # if procedure.cpt_code:
                #     code_info.append(f"CPT: {procedure.cpt_code}")
                # if code_info:
                #     scenarios_text += f"   - æ ‡å‡†ç¼–ç : {', '.join(code_info)}\n"

                scenarios_text += "\n"

            scenarios_text += "---\n\n"
        return scenarios_text

    def _build_optimized_task_instruction(self, scenario_count: int, max_recommendations_per_scenario: int,direct_return:bool) -> str:
        """æ„å»ºä¼˜åŒ–çš„ä»»åŠ¡æŒ‡ä»¤"""
        prompt=f"""
              ## ä»»åŠ¡è¯´æ˜
    
              åŸºäºæ‚£è€…ä¿¡æ¯å’Œä¸´åºŠä¸Šä¸‹æ–‡ï¼Œå¯¹{scenario_count}ä¸ªå·²ç¡®è®¤ä¸´åºŠåœºæ™¯çš„æ‰€æœ‰æ¨èé¡¹ç›®è¿›è¡Œ**ä¸‰çº§æ¨èç­‰çº§åˆ’åˆ†**ã€‚
    
              ### åˆ†çº§æ ‡å‡†
                      - **æå…¶æ¨è (Highly Recommended)**: è¯„åˆ†é«˜ï¼Œè¯æ®å……åˆ†ï¼Œä¸æ‚£è€…æƒ…å†µå®Œç¾åŒ¹é…ï¼Œå®‰å…¨æ€§å’Œè¯Šæ–­ä»·å€¼ä¿±ä½³ï¼Œæ— æ˜æ˜¾ç¦å¿Œ
                      - **æ¨è (Recommended)**: è¯„åˆ†ä¸­ç­‰ï¼Œä¸´åºŠé€‚ç”¨æ€§è‰¯å¥½ï¼Œé£é™©æ”¶ç›Šæ¯”åˆç†ï¼Œå¯èƒ½å­˜åœ¨è½»å¾®é™åˆ¶
                      - **ä¸å¤ªæ¨è (Less Recommended)**: è¯„åˆ†ä½ï¼Œæˆ–å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œæˆ–æœ‰æ˜ç¡®ç¦å¿Œç—‡ï¼Œæˆ–ä¸å½“å‰ä¸´åºŠéœ€æ±‚åŒ¹é…åº¦ä¸é«˜
              ##æ³¨æ„
                  - æ¯ä¸ªåœºæ™¯çš„æœ€ç»ˆæ¨èé¡¹ç›®å¿…é¡»ä¸º{max_recommendations_per_scenario}ä¸ªã€‚
                  - æ¯ä¸ªåœºæ™¯ä½ éƒ½è¦åšæ¨èçš„è¯„çº§ï¼Œä¸èƒ½æ è¿‡ã€‚
              ### è¾“å‡ºæ ¼å¼
              {{
                  "selected_scenarios": [
                      {{
                          "scenario_index": è¿™é‡Œæ˜¯ç´¢å¼•id(ä¾‹å¦‚ï¼š1),
                          "scenario_id": "åœºæ™¯è¯­ä¹‰ID",
                          "comprehensive_score": "0-100ç»¼åˆè¯„åˆ†",
                          "scenario_reasoning": "åœºæ™¯åŒ¹é…åº¦åˆ†æ",
                          "recommendation_grades": {{
                              "highly_recommended": [1, 3],
                              "recommended": [2, 4],
                              "less_recommended": [5]
                          }},
                          "grading_reasoning": "åˆ†çº§ä¸´åºŠç†ç”±"
                      }},
                      {{
                          "scenario_index": è¿™é‡Œæ˜¯ç´¢å¼•id(ä¾‹å¦‚ï¼š2),
                          "scenario_id": "åœºæ™¯è¯­ä¹‰ID",
                          "comprehensive_score": "0-100ç»¼åˆè¯„åˆ†",
                          "scenario_reasoning": "åœºæ™¯åŒ¹é…åº¦åˆ†æ",
                          "recommendation_grades": {{
                              "highly_recommended": [1, 3],
                              "recommended": [2, 4],
                              "less_recommended": [5]
                          }},
                          "grading_reasoning": "åˆ†çº§ä¸´åºŠç†ç”±"
                      }},
                  ],
                  "overall_choices":[è¿™æ˜¯æ€»ä½“çš„é€‰æ‹©é¡¹ç›®ï¼Œæ³¨æ„ï¼å¡«æ¨èé¡¹ç›®çš„åå­—,è¦æ±‚ä½ ç»¼åˆæ€§çš„è€ƒé‡ä¹‹åï¼Œé€‰æ‹©æœ€ç¬¦åˆæ‚£è€…ä¿¡æ¯å’Œä¸´åºŠåœºæ™¯çš„æ¨èé¡¹ç›®ï¼å¿…é¡»ä¸º{max_recommendations_per_scenario}ä¸ª]
                  "overall_reasoning": "æ€»ä½“ç­–ç•¥è¯´æ˜"
              }}
              **é‡è¦ï¼šè¯·åªè¾“å‡ºçº¯JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€è¯´æ˜æˆ–Markdownæ ‡è®°ï¼ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®ã€‚**
              """

        if not direct_return:
            return  prompt




        else:
            return f"""
            ## ä»»åŠ¡è¯´æ˜
            åŸºäºæ‚£è€…ä¿¡æ¯ä¸ä¸´åºŠä¸Šä¸‹æ–‡ï¼Œä»¥åŠç»™å®šçš„åœºæ™¯ä¸‹å¯ä¾›é€‰æ‹©çš„æ¨èé¡¹ç›®ï¼Œç›´æ¥ç»™å‡ºæœ€ç»ˆæ¨èåŠå…¶åŸå› ã€‚
    
            ### è¾“å‡ºè¦æ±‚ï¼ˆçº¯æ–‡æœ¬ï¼Œä¸­æ–‡ï¼‰
            - ä»…è¾“å‡ºæ–‡æœ¬ï¼Œä¸è¦JSONæˆ–å…¶ä»–æ ‡è®°ï¼Œä¸è¦åŒ…å«é¢å¤–çš„è§£é‡Šæ€§æ®µè½ã€‚
            - 
              1) å…ˆè¾“å‡ºâ€œæ¨èé¡¹ç›®â€ï¼šåˆ—å‡ºæœ€é€‚åˆæ‚£è€…ä¿¡æ¯å’Œä¸´åºŠä¸Šä¸‹æ–‡{max_recommendations_per_scenario} ä¸ªé¡¹ç›®ï¼ŒæŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼Œä»…å†™é¡¹ç›®åç§°ï¼Œç”¨é¡¿å·æˆ–é€—å·åˆ†éš”ã€‚
              2) å†è¾“å‡ºâ€œæ¨èç†ç”±â€ï¼šç®€è¦è¯´æ˜é€‰æ‹©ä¾æ®ï¼Œç»“åˆæ‚£è€…ä¸åœºæ™¯ä¿¡æ¯ï¼Œè¯­è¨€ç²¾ç‚¼ã€‚
            - ä¸¥æ ¼éµå®ˆâ€œå…ˆæ¨èé¡¹ç›®ï¼Œå†æ¨èç†ç”±â€çš„é¡ºåºã€‚
    
            ### æ–‡æœ¬ç¤ºä¾‹ï¼ˆç¤ºæ„ï¼‰ï¼š
            æ¨èé¡¹ç›®ï¼šé¡¹ç›®Aï¼Œé¡¹ç›®Bï¼Œé¡¹ç›®C
            æ¨èç†ç”±ï¼šâ€¦â€¦
            """

    async def get_recommendations(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int = 10,
            direct_return:bool=False,
            use_adaptive: Optional[bool] = None,  # å¯è¦†ç›–åˆå§‹è®¾ç½®
            max_concurrent: int = 3,
    ) -> List[Dict[str, Any]]:
        """ä¸»å…¥å£å‡½æ•° - è·å–æ¨èç»“æœ"""

        # ç¡®å®šæ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”ç­–ç•¥
        adaptive_mode = use_adaptive if use_adaptive is not None else self.use_adaptive
        # 1. è®¡ç®—tokenæ•°
        single_prompt = self._build_single_call_prompt(
            confirmed_scenarios, patient_info, clinical_context, max_recommendations_per_scenario,direct_return
        )



        if adaptive_mode:
            return await self._get_recommendations_adaptive(
                confirmed_scenarios, patient_info, clinical_context,
                max_recommendations_per_scenario, max_concurrent,single_prompt,direct_return
            )
        else:
            # éè‡ªé€‚åº”æ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨å•æ¬¡è°ƒç”¨
            return await self._get_recommendations_single_call(
                confirmed_scenarios, patient_info, clinical_context,
                max_recommendations_per_scenario, len(confirmed_scenarios),single_prompt,direct_return
            )

    async def _get_recommendations_adaptive(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int,
            max_concurrent: int,
            single_prompt: str,
            direct_return:bool,
    ) -> List[Dict[str, Any]]:
        """è‡ªé€‚åº”æ¨¡å¼å¤„ç†"""

        start_time = time.time()
        estimated_tokens = self.estimate_tokens_with_tiktoken(single_prompt)
        if not direct_return:
            # 2. ä½¿ç”¨ç­–ç•¥å†³ç­–
            use_concurrent, decision_metrics = self.strategy.should_use_concurrent(
                confirmed_scenarios, estimated_tokens
            )

            # 3. è®°å½•å†³ç­–è¯¦æƒ…
            self._log_decision_metrics(decision_metrics, estimated_tokens)

            # 4. æ‰§è¡Œç›¸åº”ç­–ç•¥å¹¶è®°å½•æ€§èƒ½
            try:
                if use_concurrent:
                    logger.info("âš¡ ä½¿ç”¨å¹¶å‘å¤„ç†ç­–ç•¥")
                    results = await self._get_recommendations_for_confirmed_scenarios_concurrent(
                        confirmed_scenarios, patient_info, clinical_context,
                        max_recommendations_per_scenario, max_concurrent
                    )
                    strategy_used = 'concurrent'
                else:
                    logger.info("ğŸ”„ ä½¿ç”¨å•æ¬¡è°ƒç”¨ç­–ç•¥")
                    results = await self._get_recommendations_single_call(
                        confirmed_scenarios, patient_info, clinical_context,
                        max_recommendations_per_scenario, len(confirmed_scenarios),single_prompt
                    )
                    strategy_used = 'single'

                processing_time = time.time() - start_time
                success = True

            except Exception as e:
                logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                processing_time = time.time() - start_time
                results = self._fallback_for_confirmed_scenarios(confirmed_scenarios)
                success = False
                strategy_used = 'single' if not use_concurrent else 'concurrent'

            # 5. å¦‚æœæ˜¯å­¦ä¹ ç­–ç•¥ï¼Œæ›´æ–°æ€§èƒ½æ•°æ®
            if isinstance(self.strategy, LearningThresholdStrategy):
                self.strategy.update_based_on_performance(
                    decision_metrics=decision_metrics,
                    actual_processing_time=processing_time,
                    success=success,
                    strategy_used=strategy_used
                )

            return results
        else:
             total_token=self.strategy.threshold_config["token_threshold"]
             if total_token-1500<estimated_tokens:
                 #é‡æ–°è§„æ•´ç°æœ‰æ•°æ®ç»“æ„é‡æ–°æ„æˆæç¤ºè¯
                 prompt=self._build_comprehensive_prompt_with_grading(confirmed_scenarios,patient_info,clinical_context,direct_return,len(confirmed_scenarios),max_recommendations_per_scenario)
                 results = await self._get_recommendations_single_call(
                     confirmed_scenarios, patient_info, clinical_context,
                     max_recommendations_per_scenario, len(confirmed_scenarios),  prompt,direct_return
                 )
                 return results
             else:
                 # 2. ä½¿ç”¨ç­–ç•¥å†³ç­–
                 use_concurrent, decision_metrics = self.strategy.should_use_concurrent(
                     confirmed_scenarios, estimated_tokens
                 )

                 # 3. è®°å½•å†³ç­–è¯¦æƒ…
                 self._log_decision_metrics(decision_metrics, estimated_tokens)

                 # 4. æ‰§è¡Œç›¸åº”ç­–ç•¥å¹¶è®°å½•æ€§èƒ½
                 try:
                     if use_concurrent:
                         logger.info("âš¡ ä½¿ç”¨å¹¶å‘å¤„ç†ç­–ç•¥")
                         results = await self._get_recommendations_for_confirmed_scenarios_concurrent(
                             confirmed_scenarios, patient_info, clinical_context,
                             max_recommendations_per_scenario, max_concurrent
                         )
                         strategy_used = 'concurrent'
                     else:
                         logger.info("ğŸ”„ ä½¿ç”¨å•æ¬¡è°ƒç”¨ç­–ç•¥")
                         results = await self._get_recommendations_single_call(
                             confirmed_scenarios, patient_info, clinical_context,
                             max_recommendations_per_scenario, len(confirmed_scenarios), single_prompt
                         )
                         strategy_used = 'single'

                     processing_time = time.time() - start_time
                     success = True

                 except Exception as e:
                     logger.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                     processing_time = time.time() - start_time
                     results = self._fallback_for_confirmed_scenarios(confirmed_scenarios)
                     success = False
                     strategy_used = 'single' if not use_concurrent else 'concurrent'

                 # 5. å¦‚æœæ˜¯å­¦ä¹ ç­–ç•¥ï¼Œæ›´æ–°æ€§èƒ½æ•°æ®
                 if isinstance(self.strategy, LearningThresholdStrategy):
                     self.strategy.update_based_on_performance(
                         decision_metrics=decision_metrics,
                         actual_processing_time=processing_time,
                         success=success,
                         strategy_used=strategy_used
                     )

                 return results




    def _build_comprehensive_prompt_with_grading(
                         self,
                         all_scenarios: List[Dict[str, Any]],
                         patient_info: PatientInfo,
                         clinical_context: ClinicalContext,
                         direct_return: bool,
                         max_scenarios: int,
                         max_recommendations_per_scenario: int
                 ) -> str:
                     """æ„å»ºå®Œæ•´çš„æç¤ºè¯ï¼Œç¡®ä¿æ€»tokenæ•°ä¸è¶…è¿‡3600"""
                     # æ„å»ºå„ä¸ªéƒ¨åˆ†
                     try:

                         patient_info_content = self.build_patient_context(patient_info)
                         clinical_context_content = self.build_clinical_context(clinical_context)

                         # è®¡ç®—å›ºå®šéƒ¨åˆ†çš„tokenæ•°
                         fixed_parts = patient_info_content + clinical_context_content
                         fixed_tokens = len(self.tokenizer.encode(fixed_parts))

                         # ä¸ºä»»åŠ¡æŒ‡ä»¤é¢„ç•™ç©ºé—´ï¼ˆä¼°è®¡çº¦500-800 tokenï¼‰
                         task_reserve_tokens = 900
                         available_scenario_tokens = \
                             self.strategy.threshold_config[
                                 "token_threshold"] - 1500 - fixed_tokens - task_reserve_tokens
                         logger.info(f"å¯ç”¨çš„æç¤ºè¯tokenæ•°{available_scenario_tokens}")
                         # æ„å»ºåœºæ™¯å†…å®¹ï¼Œé™åˆ¶åœ¨å¯ç”¨tokenæ•°å†…
                         scenarios_content = self.build_scenarios_with_recommend(
                             all_scenarios,
                             patient_info,
                             max_tokens=available_scenario_tokens
                         )

                         # æ„å»ºä»»åŠ¡æŒ‡ä»¤ï¼Œä½¿ç”¨å®é™…æ˜¾ç¤ºçš„åœºæ™¯æ•°é‡
                         task_instruction = self.build_task_instruction(
                             direct_return=direct_return,
                             max_scenarios=max_scenarios,
                             max_recommendations_per_scenario=max_recommendations_per_scenario
                         )

                         # ç»„åˆå®Œæ•´æç¤ºè¯
                         comprehensive_prompt = (
                                 patient_info_content +
                                 clinical_context_content +
                                 scenarios_content +
                                 task_instruction
                         )

                         # æœ€ç»ˆtokenè®¡æ•°éªŒè¯
                         total_tokens = len(self.tokenizer.encode(comprehensive_prompt))
                         if total_tokens > self.strategy.threshold_config[
                             "token_threshold"] - 1500:
                             logger.info(f"ä»ç„¶è¶…å‡º{4096 - 1500 - total_tokens}ä¸ªtoken,è¿›è¡Œæˆªæ–­")
                             # å¦‚æœä»ç„¶è¶…å‡ºï¼Œè¿›ä¸€æ­¥æˆªæ–­åœºæ™¯éƒ¨åˆ†
                             scenarios_content = self._truncate_scenarios_further(scenarios_content,
                                                                                  available_scenario_tokens - fixed_tokens - task_reserve_tokens)
                             comprehensive_prompt = (
                                     patient_info_content +
                                     clinical_context_content +
                                     scenarios_content +
                                     task_instruction
                             )

                         return comprehensive_prompt
                     except Exception as e:
                         logger.info(f"æ„å»ºæç¤ºè¯é”™è¯¯ï¼š{e}")
                         return ""

    def _truncate_scenarios_further(self, scenarios_content: str, max_tokens: int) -> str:
        """è¿›ä¸€æ­¥æˆªæ–­åœºæ™¯å†…å®¹"""
        current_tokens = len(self.tokenizer.encode(scenarios_content))
        if current_tokens <= max_tokens:
            return scenarios_content

        # é€æ­¥ç§»é™¤æœ€åä¸€ä¸ªåœºæ™¯
        while current_tokens > max_tokens and "### åœºæ™¯" in scenarios_content:
            # æ‰¾åˆ°æœ€åä¸€ä¸ªåœºæ™¯çš„å¼€å§‹ä½ç½®
            last_scenario_start = scenarios_content.rfind("### åœºæ™¯")
            if last_scenario_start == -1:
                break

            # æ‰¾åˆ°è¿™ä¸ªåœºæ™¯çš„ç»“æŸä½ç½®ï¼ˆä¸‹ä¸€ä¸ªåœºæ™¯å¼€å§‹æˆ–æ–‡ä»¶ç»“æŸï¼‰
            next_scenario_start = scenarios_content.find("### åœºæ™¯", last_scenario_start + 1)
            if next_scenario_start != -1:
                scenarios_content = scenarios_content[:last_scenario_start] + scenarios_content[next_scenario_start:]
            else:
                scenarios_content = scenarios_content[:last_scenario_start]

            # æ·»åŠ æˆªæ–­æç¤º
            scenarios_content += "\n\n<!-- ç”±äºtokené™åˆ¶ï¼Œéƒ¨åˆ†åœºæ™¯æœªæ˜¾ç¤º -->\n"
            current_tokens = len(self.tokenizer.encode(scenarios_content))

        return scenarios_content
    def build_scenarios_with_recommend(self, all_scenarios: List[Dict[str, Any]], patient_info: PatientInfo,
                                       max_tokens: int = 2500):
        """æ„å»ºåœºæ™¯å†…å®¹ï¼Œé™åˆ¶åœ¨æŒ‡å®štokenæ•°å†…"""

        scenarios_text = "## å¯é€‰ä¸´åºŠåœºæ™¯åŠæ¨èé¡¹ç›®\n\n"

        # è®¡ç®—åˆå§‹tokenæ•°
        total_tokens = len(self.tokenizer.encode(scenarios_text))
        scenarios_added = 0
        recommendations_added = 0

        for scenario_idx, scenario_data in enumerate(all_scenarios, 1):
            scenario = scenario_data['scenario']
            recommendations = scenario_data.get('recommendations', [])

            # æ„å»ºå½“å‰åœºæ™¯çš„å®Œæ•´æ–‡æœ¬
            current_scenario_text = f"### åœºæ™¯{scenario_idx}: {scenario.description_zh}\n"
            current_scenario_text += f"- **åœºæ™¯ID**: {scenario.semantic_id}\n"
            current_scenario_text += f"- **é€‚ç”¨ç§‘å®¤**: {scenario.panel.name_zh if hasattr(scenario, 'panel') else 'æœªçŸ¥'}\n"
            current_scenario_text += f"- **é€‚ç”¨äººç¾¤**: {scenario.patient_population or 'æœªçŸ¥'}\n"
            # current_scenario_text += f"- **ä¸´åºŠèƒŒæ™¯**: {scenario.clinical_context or 'æ— '}\n\n"

            if not recommendations:
                current_scenario_text += "  æš‚æ— æ¨èé¡¹ç›®\n\n"
            else:
                current_scenario_text += "#### æ¨èé¡¹ç›®æ¸…å•:\n"

                for rec_idx, rec_data in enumerate(recommendations, 1):
                    recommendation = rec_data['recommendation']
                    procedure = rec_data['procedure']

                    # æ„å»ºæ¨èé¡¹ç›®æ–‡æœ¬
                    current_item_text = f"{rec_idx}. **{procedure.name_zh}**\n"

                    # æŠ€æœ¯ç»†èŠ‚ï¼ˆç®€åŒ–ï¼‰
                    # tech_details = []
                    # if procedure.modality:
                    #     tech_details.append(f"æ£€æŸ¥æ–¹å¼: {procedure.modality}")
                    # if procedure.body_part:
                    #     tech_details.append(f"æ£€æŸ¥éƒ¨ä½: {procedure.body_part}")

                    # å®‰å…¨æ€§å…³é”®ä¿¡æ¯
                    # safety_flags = []
                    # if procedure.contrast_used and any('è¿‡æ•' in allergy for allergy in getattr(patient_info, 'allergies', []) if allergy):
                    #         safety_flags.append("âš ï¸ ä½¿ç”¨å¯¹æ¯”å‰‚(æ³¨æ„è¿‡æ•å²)")
                    # elif procedure.contrast_used:
                    #         safety_flags.append("ä½¿ç”¨å¯¹æ¯”å‰‚")

                    # if (procedure.radiation_level and
                    #             getattr(patient_info, 'pregnancy_status', '') in ['å¦Šå¨ ', 'æ€€å­•']):
                    #         safety_flags.append("âš ï¸ æœ‰è¾å°„(å¦Šå¨ ç¦å¿Œ)")
                    # elif procedure.radiation_level:
                    #         safety_flags.append(f"è¾å°„ç­‰çº§: {procedure.radiation_level}")
                    #
                    # if safety_flags:
                    #         current_item_text += f"   - å®‰å…¨ä¿¡æ¯: {', '.join(safety_flags)}\n"
                    # å…³é”®ä¿¡æ¯ï¼šACRè¯„åˆ†å’Œå®‰å…¨æ€§
                    current_item_text += f"   - **ACRé€‚å®œæ€§è¯„åˆ†**: {recommendation.appropriateness_rating}/9\n"
                    if recommendation.appropriateness_category_zh:
                        current_item_text += f"   - æ¨èçº§åˆ«: {recommendation.appropriateness_category_zh}\n"

                    # critical_contraindications = []
                    # if (recommendation.pregnancy_safety and
                    #         getattr(patient_info, 'pregnancy_status', '') in ['å¦Šå¨ ', 'æ€€å­•'] and
                    #         'ç¦å¿Œ' in recommendation.pregnancy_safety):
                    #     critical_contraindications.append("å¦Šå¨ ç¦å¿Œ")
                    #
                    # if recommendation.contraindications:
                    #     # åªæ˜¾ç¤ºå‰50ä¸ªå­—ç¬¦çš„å…³é”®ç¦å¿Œ
                    #     contra_preview = recommendation.contraindications[:50]
                    #     if 'è‚¾åŠŸèƒ½' in contra_preview and any('è‚¾' in comorbidity for comorbidity in
                    #                                           getattr(patient_info, 'comorbidities', [])):
                    #         critical_contraindications.append("è‚¾åŠŸèƒ½é™åˆ¶")
                    #
                    # if critical_contraindications:
                    #     current_item_text += f"   - âš ï¸ ç¦å¿Œæç¤º: {', '.join(critical_contraindications)}\n"
                    # æ ¸å¿ƒæ¨èç†ç”±(ç²¾ç®€)
                    # if recommendation.reasoning_zh:
                    #     reasoning = recommendation.reasoning_zh[:50] + "..." if len(
                    #         recommendation.reasoning_zh) > 50 else recommendation.reasoning_zh
                    #     current_item_text += f"   - ä¸»è¦ä¼˜åŠ¿: {reasoning}\n"

                    current_item_text += "\n"
                    current_scenario_text += current_item_text
                    recommendations_added += 1

            # æ·»åŠ åœºæ™¯åˆ†éš”ç¬¦
            current_scenario_text += "---\n\n"

            # è®¡ç®—å½“å‰åœºæ™¯çš„æ€»tokenæ•°
            current_scenario_tokens = len(self.tokenizer.encode((current_scenario_text)))

            # æ£€æŸ¥æ·»åŠ æ•´ä¸ªåœºæ™¯åæ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
            if total_tokens + current_scenario_tokens <= max_tokens:
                scenarios_text += current_scenario_text
                total_tokens += current_scenario_tokens
                scenarios_added += 1
            else:
                # å¦‚æœè¶…è¿‡é™åˆ¶ï¼Œæ·»åŠ æˆªæ–­æç¤ºå¹¶è·³å‡ºå¾ªç¯
                remaining_scenarios = len(all_scenarios) - scenario_idx
                if remaining_scenarios > 0:
                    logger.info(f"### åœºæ™¯{scenario_idx}åŠåç»­{remaining_scenarios}ä¸ªåœºæ™¯ç”±äºtokené™åˆ¶æœªæ˜¾ç¤º\n")
                    # scenarios_text += f"---\n\n"
                break

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        stats_text = f"\n<!-- åœºæ™¯éƒ¨åˆ†ä½¿ç”¨token: {total_tokens}/{max_tokens}, æ˜¾ç¤ºåœºæ™¯: {scenarios_added}/{len(all_scenarios)}, æ˜¾ç¤ºæ¨èé¡¹ç›®: {recommendations_added} -->\n"
        # stats_tokens = qwen_token_counter.get_token_count(stats_text)
        logger.info(stats_text)

        return scenarios_text

    def build_task_instruction(self, direct_return: bool, max_scenarios: int,
                               max_recommendations_per_scenario: int):
        """æ„å»ºä»»åŠ¡æŒ‡ä»¤"""
        if direct_return:
            task_instruction = f"""

            ## ğŸ¯ ä»»åŠ¡ç›®æ ‡
            åŸºäºå¾ªè¯åŒ»å­¦åŸåˆ™ï¼Œä¸ºå½“å‰æ‚£è€…é€‰æ‹©æœ€åˆé€‚çš„å½±åƒå­¦æ£€æŸ¥æ–¹æ¡ˆã€‚

            ## ğŸ“‹ å†³ç­–æ¡†æ¶

            ### ç¬¬ä¸€çº§ï¼šåœºæ™¯ç­›é€‰
            ä»ç»™ä½ çš„ä¸Šä¸‹æ–‡çš„ä¸´åºŠåœºæ™¯ä¸­ï¼Œé€‰æ‹©{max_scenarios}ä¸ªæœ€ç›¸å…³çš„ä¸´åºŠåœºæ™¯ï¼š
            - **ä¸´åºŠåŒ¹é…åº¦**ï¼šåœºæ™¯æè¿°ä¸æ‚£è€…ä¸»è¯‰ã€è¯Šæ–­çš„å¥‘åˆç¨‹åº¦
            - **ç§‘å®¤é€‚ç”¨æ€§**ï¼šåœºæ™¯ä¸å°±è¯Šç§‘å®¤ä¸“ä¸šç‰¹é•¿çš„åŒ¹é…åº¦
            - **äººç¾¤é€‚åº”æ€§**ï¼šåœºæ™¯é€‚ç”¨äººç¾¤ä¸æ‚£è€…ç‰¹å¾çš„ç¬¦åˆåº¦

            ### ç¬¬äºŒçº§ï¼šæ£€æŸ¥é¡¹ç›®åˆ†çº§
            å¯¹æ¯ä¸ªé€‰ä¸­åœºæ™¯ï¼ŒæŒ‰ä»¥ä¸‹æ ‡å‡†åˆ†çº§ï¼š

            #### ğŸŸ¢ æå…¶æ¨è (Highly Recommended)
            - æ— æ˜ç¡®ç¦å¿Œç—‡
            - ä¸å½“å‰ä¸´åºŠé—®é¢˜é«˜åº¦ç›¸å…³
            - è¯Šæ–­ä»·å€¼æ˜ç¡®ä¸”é£é™©å¯æ§

            #### ğŸŸ¡ æ¨è (Recommended)  
            - æ— é‡å¤§ç¦å¿Œç—‡
            - ä¸´åºŠé€‚ç”¨æ€§è‰¯å¥½
            - å¯ä½œä¸ºè¾…åŠ©æˆ–æ›¿ä»£æ–¹æ¡ˆ

            #### ğŸ”´ ä¸å¤ªæ¨è (Less Recommended)
            - å­˜åœ¨æ˜ç¡®ç¦å¿Œç—‡
            - ä¸ä¸´åºŠéœ€æ±‚åŒ¹é…åº¦ä½
            - æœ‰æ›´ä¼˜çš„æ›¿ä»£æ–¹æ¡ˆ

            ## âš ï¸ å®‰å…¨ä¼˜å…ˆåŸåˆ™

            ### ç»å¯¹ç¦å¿Œ
            1. **å¦Šå¨ æœŸ**ï¼šä¸¥æ ¼é¿å…ç”µç¦»è¾å°„æ£€æŸ¥ï¼ˆCTã€Xçº¿ã€PET-CTï¼‰
            2. **å¯¹æ¯”å‰‚è¿‡æ•**ï¼šç¦ç”¨å«ç¢˜/é’†å¯¹æ¯”å‰‚çš„å¢å¼ºæ£€æŸ¥
            3. **è‚¾åŠŸèƒ½ä¸å…¨**ï¼šæ…ç”¨å¯¹æ¯”å‰‚ï¼Œè¯„ä¼°è‚¾ç—…é£é™©

            ### ç›¸å¯¹ç¦å¿Œ
            1. **å¹½é—­ææƒ§ç—‡**ï¼šMRIæ£€æŸ¥éœ€ç‰¹æ®Šå‡†å¤‡
            2. **é‡‘å±æ¤å…¥ç‰©**ï¼šéƒ¨åˆ†MRIå—é™
            3. **è‚¥èƒ–æ‚£è€…**ï¼šè€ƒè™‘è®¾å¤‡æ‰¿é‡å’Œå›¾åƒè´¨é‡é™åˆ¶

            ## ğŸ›ï¸ æŠ€æœ¯è€ƒé‡

            ### è¯Šæ–­æ•ˆèƒ½ä¼˜å…ˆçº§
            1. **æ•æ„Ÿæ€§/ç‰¹å¼‚æ€§**ï¼šç–¾ç—…çš„æ£€æµ‹å’Œæ’é™¤èƒ½åŠ›
            2. **ç©ºé—´åˆ†è¾¨ç‡**ï¼šè§£å‰–ç»†èŠ‚æ˜¾ç¤ºèƒ½åŠ›
            3. **åŠŸèƒ½ä¿¡æ¯**ï¼šé™¤å½¢æ€å­¦å¤–çš„åŠŸèƒ½è¯„ä¼°
            4. **æ£€æŸ¥æ—¶é•¿**ï¼šæ‚£è€…è€å—åº¦å’Œä¸´åºŠç´§è¿«æ€§

            ## ğŸ“Š è¾“å‡ºè¦æ±‚

            è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºæ¨èç»“æœï¼š

            ```json
            {{
                "selected_scenarios": [
                    {{
                        "scenario_index": 1,
                        "scenario_id": "åœºæ™¯è¯­ä¹‰ID",
                        "comprehensive_score": 85,
                        "scenario_reasoning": "åŸºäºæ‚£è€…æ€¥æ€§è…¹ç—›ä¸»è¯‰å’Œå¹´é¾„å› ç´ ï¼Œæ­¤è…¹éƒ¨æ€¥ç—‡åœºæ™¯æœ€ä¸ºåŒ¹é…",
                        "recommendation_grades": {{
                            "highly_recommended": [1, 2],
                            "recommended": [3],
                            "less_recommended": [4, 5]
                        }},
                        "grading_reasoning": "CTå¹³æ‰«ACRè¯„åˆ†9åˆ†ï¼Œå¯¹æ€¥è…¹ç—‡è¯Šæ–­ä»·å€¼æœ€é«˜ï¼›è¶…å£°æ— è¾å°„ï¼Œé€‚åˆåˆæ­¥ç­›æŸ¥"
                    }},
                    {{
                        "scenario_index": è¿™é‡Œæ˜¯ç´¢å¼•id(ä¾‹å¦‚ï¼š2),
                        "scenario_id": "åœºæ™¯è¯­ä¹‰ID",
                        "comprehensive_score": "0-100ç»¼åˆè¯„åˆ†",
                        "scenario_reasoning": "åœºæ™¯åŒ¹é…åº¦åˆ†æ",
                        "recommendation_grades": {{
                                      "highly_recommended": [1, 3],
                                      "recommended": [2, 4],
                                      "less_recommended": [5]
                                  }},
                        "grading_reasoning": "åˆ†çº§ä¸´åºŠç†ç”±"
                              }},
                ],
                "overall_choices":[è¿™æ˜¯æ€»ä½“çš„é€‰æ‹©é¡¹ç›®ï¼Œæ³¨æ„ï¼å¡«æ¨èé¡¹ç›®çš„åå­—,è¦æ±‚ä½ ç»¼åˆæ€§çš„è€ƒé‡ä¹‹åï¼Œé€‰æ‹©æœ€ç¬¦åˆæ‚£è€…ä¿¡æ¯å’Œä¸´åºŠåœºæ™¯çš„æ¨èé¡¹ç›®ï¼å¿…é¡»ä¸º{max_recommendations_per_scenario}ä¸ª]
                "overall_reasoning": "æ€»ä½“é€‰æ‹©ç­–ç•¥ï¼Œé‡ç‚¹è¯´æ˜å®‰å…¨æ€§è€ƒé‡å’Œè¯Šæ–­è·¯å¾„"
            }}
             **é‡è¦ï¼š
                  -è¯·åªè¾“å‡ºçº¯JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€è¯´æ˜æˆ–Markdownæ ‡è®°ï¼ç¡®ä¿JSONæ ¼å¼å®Œå…¨æ­£ç¡®ã€‚**
                  -æ³¨æ„é€‰æ‹©çš„ä¸´åºŠåœºæ™¯æ•°ä¸€å®šä¸èƒ½è¶…è¿‡{max_scenarios}ä¸ªï¼
            """

            return task_instruction
        return f"""
        ## ä»»åŠ¡è¯´æ˜
        åŸºäºæ‚£è€…ä¿¡æ¯ä¸ä¸´åºŠä¸Šä¸‹æ–‡ï¼Œä»¥åŠç»™å®šçš„åœºæ™¯ä¸‹å¯ä¾›é€‰æ‹©çš„æ¨èé¡¹ç›®ï¼Œç›´æ¥ç»™å‡ºæœ€ç»ˆæ¨èåŠå…¶åŸå› ã€‚

        ### è¾“å‡ºè¦æ±‚ï¼ˆçº¯æ–‡æœ¬ï¼Œä¸­æ–‡ï¼‰
        - ä»…è¾“å‡ºæ–‡æœ¬ï¼Œä¸è¦JSONæˆ–å…¶ä»–æ ‡è®°ï¼Œä¸è¦åŒ…å«é¢å¤–çš„è§£é‡Šæ€§æ®µè½ã€‚
        - 
          1) å…ˆè¾“å‡ºâ€œæ¨èé¡¹ç›®â€ï¼šåˆ—å‡ºæœ€é€‚åˆæ‚£è€…ä¿¡æ¯å’Œä¸´åºŠä¸Šä¸‹æ–‡{max_recommendations_per_scenario} ä¸ªé¡¹ç›®ï¼ŒæŒ‰ä¼˜å…ˆçº§ä»é«˜åˆ°ä½ï¼Œä»…å†™é¡¹ç›®åç§°ï¼Œç”¨é¡¿å·æˆ–é€—å·åˆ†éš”ã€‚
          2) å†è¾“å‡ºâ€œæ¨èç†ç”±â€ï¼šç®€è¦è¯´æ˜é€‰æ‹©ä¾æ®ï¼Œç»“åˆæ‚£è€…ä¸åœºæ™¯ä¿¡æ¯ï¼Œè¯­è¨€ç²¾ç‚¼ã€‚
        - ä¸¥æ ¼éµå®ˆâ€œå…ˆæ¨èé¡¹ç›®ï¼Œå†æ¨èç†ç”±â€çš„é¡ºåºã€‚

        ### æ–‡æœ¬ç¤ºä¾‹ï¼ˆç¤ºæ„ï¼‰ï¼š
        æ¨èé¡¹ç›®ï¼šé¡¹ç›®Aï¼Œé¡¹ç›®Bï¼Œé¡¹ç›®C
        æ¨èç†ç”±ï¼šâ€¦â€¦
        """


    def _log_decision_metrics(self, decision_metrics: Dict[str, Any], estimated_tokens: int):
        """è®°å½•å†³ç­–æŒ‡æ ‡"""
        metrics = decision_metrics['dimensions']

        logger.info("ğŸ“Š å†³ç­–åˆ†æ:")
        logger.info(
            f"  Tokenæ•°: {estimated_tokens}/{metrics['tokens']['threshold']} ({metrics['tokens']['ratio'] * 100:.1f}%)")
        logger.info(
            f"  åœºæ™¯æ•°: {metrics['scenarios']['value']}/{metrics['scenarios']['threshold']} ({metrics['scenarios']['ratio'] * 100:.1f}%)")
        logger.info(
            f"  æ€»æ¨èæ•°: {metrics['total_recommendations']['value']}/{metrics['total_recommendations']['threshold']} ({metrics['total_recommendations']['ratio'] * 100:.1f}%)")
        logger.info(
            f"  å¹³å‡æ¨èæ•°: {metrics['avg_recommendations']['value']:.1f}/{metrics['avg_recommendations']['threshold']} ({metrics['avg_recommendations']['ratio'] * 100:.1f}%)")
        logger.info(f"  ç»¼åˆè¯„åˆ†: {decision_metrics['composite_score']:.2f}")
        logger.info(f"  å†³ç­–ç†ç”±: {decision_metrics['decision_reason']}")

    async def _get_recommendations_single_call(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int,
            expected_scenario_count: int,
            single_prompt: str,
            direct_return:bool=False
    ) -> List[Dict[str, Any]]:
        """å•æ¬¡è°ƒç”¨å¤„ç†"""
        # è¿™é‡Œå®ç°å•æ¬¡LLMè°ƒç”¨é€»è¾‘
        # è¿”å›æ ¼å¼åŒ–çš„ç»“æœ
        # 2. æ ¹æ®tokenæ•°é€‰æ‹©ç­–ç•¥
        return await self._get_recommendations_single_call_by_llm(
                confirmed_scenarios,single_prompt,direct_return
            )

    async def _get_recommendations_for_confirmed_scenarios_concurrent(
            self,
            confirmed_scenarios: List[Dict[str, Any]],
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int,
            max_concurrent: int=3
    ) -> List[Dict[str, Any]]:
        """å¹¶å‘å¤„ç†"""
        """å¹¶å‘å¤„ç†å·²ç¡®è®¤åœºæ™¯çš„æ¨èé¡¹ç›®åˆ†çº§

            Args:
                confirmed_scenarios: å·²ç¡®è®¤çš„åœºæ™¯åˆ—è¡¨
                patient_info: æ‚£è€…ä¿¡æ¯
                clinical_context: ä¸´åºŠä¸Šä¸‹æ–‡
                max_recommendations_per_scenario: æ¯ä¸ªåœºæ™¯æœ€å¤§æ¨èé¡¹ç›®æ•°
                max_concurrent: æœ€å¤§å¹¶å‘æ•°

            Returns:
                æ ¼å¼åŒ–çš„æ¨èç»“æœï¼Œä¸åŸå‡½æ•°æ ¼å¼ç›¸åŒ
            """
        import asyncio
        from typing import List, Dict, Any

        async def process_single_scenario(scenario_data: Dict[str, Any], scenario_index: int) -> Dict[str, Any]:
            """å¤„ç†å•ä¸ªåœºæ™¯çš„æ¨èé¡¹ç›®åˆ†çº§"""
            try:
                # æ„å»ºå•ä¸ªåœºæ™¯çš„æç¤ºè¯
                prompt = self._build_single_scenario_prompt(
                    scenario_data,
                    scenario_index,
                    patient_info,
                    clinical_context,
                    max_recommendations_per_scenario
                )

                # è°ƒç”¨LLM
                response = await self.ai_service._call_llm(prompt)

                # è§£æJSONç»“æœ
                import re
                import json

                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if not json_match:
                    logger.error(f"åœºæ™¯{scenario_index} LLMè¿”å›æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
                    return self._fallback_single_scenario(scenario_data, scenario_index)

                try:
                    result = json.loads(json_match.group())
                except json.JSONDecodeError as e:
                    logger.error(f"åœºæ™¯{scenario_index} JSONè§£æé”™è¯¯: {e}")
                    return self._fallback_single_scenario(scenario_data, scenario_index)

                # å¤„ç†åˆ†çº§æ¨èç»“æœ
                return self._process_single_scenario_result(result, scenario_data, scenario_index,max_recommendations_per_scenario)

            except Exception as e:
                logger.error(f"å¤„ç†åœºæ™¯{scenario_index}æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
                return self._fallback_single_scenario(scenario_data, scenario_index)

        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)

        async def bounded_process(scenario_data, scenario_index):
            async with semaphore:
                return await process_single_scenario(scenario_data, scenario_index)

        # å¹¶å‘å¤„ç†æ‰€æœ‰åœºæ™¯
        tasks = [
            bounded_process(scenario_data, idx + 1)
            for idx, scenario_data in enumerate(confirmed_scenarios)
        ]

        single_scenario_results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†ç»“æœ
        final_results = []
        final_choices=[]
        for result in single_scenario_results:
            if isinstance(result, Exception):
                logger.error(f"åœºæ™¯å¤„ç†å¼‚å¸¸: {result}")
                continue
            if result:
                choices = result.get("final_choices", [])
                if not choices:
                    procedures = result.get('graded_recommendations')["highly_recommended"]
                    res = [p['procedure_details']['name_zh'] for p in procedures]
                    final_choices.append(res)
                # åªæ·»åŠ æœ‰æ•ˆç»“æœ
                final_results.append(result)

        # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
        final_results.sort(key=lambda x: x['comprehensive_score'], reverse=True)

        # ç”Ÿæˆæ€»ä½“æ¨ç†
        overall_choices = await self._generate_overall_reasoning(patient_info=patient_info,clinical_context=clinical_context,max_recommendations_per_scenario=max_recommendations_per_scenario,final_results=final_choices)


        res={"result":final_results,"overall_choices":overall_choices.get("final_choices"),"overall_reason":overall_choices.get("overall_reason")}

        return res

    def _build_single_scenario_prompt(
            self,
            scenario_data: Dict[str, Any],
            scenario_index: int,
            patient_info: PatientInfo,
            clinical_context: ClinicalContext,
            max_recommendations_per_scenario: int
    ) -> str:
        """ä¸ºå•ä¸ªåœºæ™¯æ„å»ºæç¤ºè¯"""

        scenario = scenario_data['scenario']
        recommendations = scenario_data.get('recommendations', [])

        patient_info_content = self.build_patient_context(patient_info)
        clinical_context_content = self.build_clinical_context(clinical_context)
        scenario_content = self._build_single_scenario_content(scenario_data, scenario_index)
        task_instruction = self._build_single_scenario_task_instruction(
            scenario_index,
            len(recommendations),
            max_recommendations_per_scenario
        )

        return f"""{patient_info_content}

        {clinical_context_content}

        {scenario_content}

        {task_instruction}"""

    def _build_single_scenario_content(self, scenario_data: Dict[str, Any], scenario_index: int) -> str:
        """æ„å»ºå•ä¸ªåœºæ™¯çš„å†…å®¹æè¿°"""
        scenario = scenario_data['scenario']
        recommendations = scenario_data.get('recommendations', [])

        content = f"""## åœºæ™¯ {scenario_index}: {scenario.description_zh}

        ### åœºæ™¯ä¿¡æ¯
        - **åœºæ™¯ID**: {scenario.semantic_id}
        - **é€‚ç”¨ç§‘å®¤**: {scenario.panel.name_zh if hasattr(scenario, 'panel') else 'æœªçŸ¥'}
        - **é€‚ç”¨äººç¾¤**: {scenario.patient_population or 'æœªçŸ¥'}
        - **ä¸´åºŠèƒŒæ™¯**: {scenario.clinical_context or 'æ— '}

        ### æ¨èé¡¹ç›®æ¸…å•
        """

        if not recommendations:
            content += "æš‚æ— æ¨èé¡¹ç›®\n"
            return content

        for rec_idx, rec_data in enumerate(recommendations, 1):
            recommendation = rec_data['recommendation']
            procedure = rec_data['procedure']

            # æ£€æŸ¥é¡¹ç›®åŸºæœ¬ä¿¡æ¯
            content += f"{rec_idx}. **{procedure.name_zh}** ({procedure.name_en})\n"

            # æ£€æŸ¥æŠ€æœ¯ç»†èŠ‚
            tech_details = []
            if procedure.modality:
                tech_details.append(f"æ£€æŸ¥æ–¹å¼: {procedure.modality}")
            if procedure.body_part:
                tech_details.append(f"æ£€æŸ¥éƒ¨ä½: {procedure.body_part}")
            # if procedure.exam_duration:
            #     tech_details.append(f"æ£€æŸ¥æ—¶é•¿: {procedure.exam_duration}åˆ†é’Ÿ")
            # if tech_details:
            #     content += f"   - æŠ€æœ¯ç»†èŠ‚: {', '.join(tech_details)}\n"

            # å®‰å…¨æ€§å’Œå‡†å¤‡ä¿¡æ¯
            safety_info = []
            if procedure.contrast_used:
                safety_info.append("ä½¿ç”¨å¯¹æ¯”å‰‚")
            if procedure.radiation_level:
                safety_info.append(f"è¾å°„ç­‰çº§: {procedure.radiation_level}")
            # if procedure.preparation_required:
            #     safety_info.append("éœ€è¦å‡†å¤‡")
            if safety_info:
                content += f"   - å®‰å…¨ä¿¡æ¯: {', '.join(safety_info)}\n"

            # ACRæ¨èä¿¡æ¯
            content += f"   - **ACRé€‚å®œæ€§è¯„åˆ†**: {recommendation.appropriateness_rating}/9\n"
            if recommendation.appropriateness_category_zh:
                content += f"   - é€‚å®œæ€§ç±»åˆ«: {recommendation.appropriateness_category_zh}\n"

            # è¯æ®å’Œå…±è¯†
            evidence_info = []
            if recommendation.evidence_level:
                evidence_info.append(f"è¯æ®å¼ºåº¦: {recommendation.evidence_level}")
            # if recommendation.consensus_level:
            #     evidence_info.append(f"å…±è¯†æ°´å¹³: {recommendation.consensus_level}")
            # if evidence_info:
            #     content += f"   - è¯æ®è´¨é‡: {', '.join(evidence_info)}\n"

            # è¾å°„å‰‚é‡ä¿¡æ¯
            dose_info = []
            if recommendation.adult_radiation_dose:
                dose_info.append(f"æˆäººå‰‚é‡: {recommendation.adult_radiation_dose}")
            if recommendation.pediatric_radiation_dose:
                dose_info.append(f"å„¿ç«¥å‰‚é‡: {recommendation.pediatric_radiation_dose}")
            if dose_info:
                content += f"   - è¾å°„å‰‚é‡: {', '.join(dose_info)}\n"

            # å®‰å…¨æ€§ä¿¡æ¯
            safety_info = []
            if recommendation.pregnancy_safety:
                safety_info.append(f"å¦Šå¨ å®‰å…¨: {recommendation.pregnancy_safety}")
            if recommendation.contraindications:
                contra = recommendation.contraindications[:60] + "..." if len(
                    recommendation.contraindications) > 60 else recommendation.contraindications
                safety_info.append(f"ç¦å¿Œç—‡: {contra}")
            if safety_info:
                content += f"   - å®‰å…¨è€ƒè™‘: {', '.join(safety_info)}\n"

            # æ¨èç†ç”±
            if recommendation.reasoning_zh:
                reasoning = recommendation.reasoning_zh[:50] + "..." if len(
                    recommendation.reasoning_zh) > 50 else recommendation.reasoning_zh
                content += f"   - æ¨èç†ç”±: {reasoning}\n"

            content += "\n"

        return content

    def _build_single_scenario_task_instruction(
            self,
            scenario_index: int,
            recommendation_count: int,
            max_recommendations_per_scenario: int
    ) -> str:
        """ä¸ºå•ä¸ªåœºæ™¯æ„å»ºä»»åŠ¡æŒ‡ä»¤"""

        task_instruction = f"""
        ## ä»»åŠ¡è¯´æ˜

        ä½œä¸ºç»éªŒä¸°å¯Œçš„ä¸´åºŠåŒ»ç”Ÿï¼Œè¯·åŸºäºæ‚£è€…ä¿¡æ¯å’Œä¸´åºŠä¸Šä¸‹æ–‡ï¼Œå¯¹**åœºæ™¯{scenario_index}**çš„{recommendation_count}ä¸ªæ¨èé¡¹ç›®è¿›è¡Œ**ä¸‰çº§æ¨èç­‰çº§åˆ’åˆ†**ã€‚

        ### æ¨èé¡¹ç›®ä¸‰çº§åˆ†çº§è¯„ä¼°
        å¯¹è¯¥åœºæ™¯çš„æ‰€æœ‰æ¨èé¡¹ç›®ï¼Œè¿›è¡Œ**ä¸‰çº§æ¨èç­‰çº§åˆ’åˆ†**ï¼š

        - **æå…¶æ¨è (Highly Recommended)**: è¯„åˆ†é«˜ï¼Œè¯æ®å……åˆ†ï¼Œä¸æ‚£è€…æƒ…å†µå®Œç¾åŒ¹é…ï¼Œå®‰å…¨æ€§å’Œè¯Šæ–­ä»·å€¼ä¿±ä½³ï¼Œæ— æ˜æ˜¾ç¦å¿Œ
        - **æ¨è (Recommended)**: è¯„åˆ†ä¸­ç­‰ï¼Œä¸´åºŠé€‚ç”¨æ€§è‰¯å¥½ï¼Œé£é™©æ”¶ç›Šæ¯”åˆç†ï¼Œå¯èƒ½å­˜åœ¨è½»å¾®é™åˆ¶  
        - **ä¸å¤ªæ¨è (Less Recommended)**: è¯„åˆ†ä½ï¼Œæˆ–å­˜åœ¨å®‰å…¨éšæ‚£ï¼Œæˆ–æœ‰æ˜ç¡®ç¦å¿Œç—‡ï¼Œæˆ–ä¸å½“å‰ä¸´åºŠéœ€æ±‚åŒ¹é…åº¦ä¸é«˜

        ### è¯„ä¼°è¦ç‚¹
        1. **æ‚£è€…åŒ¹é…åº¦**: è€ƒè™‘æ‚£è€…å¹´é¾„ã€æ€§åˆ«ã€ç—‡çŠ¶ã€ç—…å²ç­‰
        2. **ä¸´åºŠç›¸å…³æ€§**: ä¸å½“å‰ä¸´åºŠè¡¨ç°å’Œè¯Šæ–­éœ€æ±‚çš„åŒ¹é…ç¨‹åº¦
        3. **å®‰å…¨æ€§**: è¾å°„å‰‚é‡ã€å¯¹æ¯”å‰‚ä½¿ç”¨ã€ç¦å¿Œç—‡ç­‰å®‰å…¨å› ç´ 
        4. **è¯æ®å¼ºåº¦**: ACRè¯„åˆ†ã€è¯æ®ç­‰çº§ã€å…±è¯†æ°´å¹³
        5. **å®ç”¨æ€§**: æ£€æŸ¥å¯è¡Œæ€§ã€å‡†å¤‡è¦æ±‚ã€æ—¶é•¿ç­‰

        ## è¾“å‡ºæ ¼å¼
        è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦é¢å¤–è§£é‡Šï¼š

        ```json
        {{
            "scenario_index": {scenario_index},
            "scenario_id": "å¡«å†™åœºæ™¯è¯­ä¹‰ID",
            "comprehensive_score": "æ ¹æ®æ¨èé¡¹ç›®è´¨é‡ç»™å‡ºçš„0-100ç»¼åˆè¯„åˆ†",
            "scenario_reasoning": "è¯¥åœºæ™¯ä¸æ‚£è€…æƒ…å†µçš„åŒ¹é…åº¦åˆ†æï¼ˆ50å­—ï¼‰",
            "recommendation_grades": {{
                "highly_recommended": [æ¨èé¡¹ç›®ç´¢å¼•åˆ—è¡¨, ä»1å¼€å§‹],
                "recommended": [æ¨èé¡¹ç›®ç´¢å¼•åˆ—è¡¨, ä»1å¼€å§‹],
                "less_recommended": [æ¨èé¡¹ç›®ç´¢å¼•åˆ—è¡¨, ä»1å¼€å§‹]
            }},
            "final_choices":["è¿™é‡Œå¡«å…¥æœ€ç»ˆé€‰æ‹©çš„æœ€ç¬¦åˆå½“å‰æ‚£è€…ä¿¡æ¯çš„æ£€æŸ¥é¡¹ç›®æ¨è"]
            "grading_reasoning": "å¯¹è¯¥åœºæ™¯æ¨èé¡¹ç›®åˆ†çº§çš„ä¸´åºŠç†ç”±ï¼ˆ50å­—ï¼‰ï¼Œé‡ç‚¹è¯´æ˜åˆ†çº§ä¾æ®"
        }}"""
        return task_instruction

    def _process_single_scenario_result(
            self,
            result: Dict[str, Any],
            scenario_data: Dict[str, Any],
            scenario_index: int,
            top_k: int = 3  # æ–°å¢ top_k å‚æ•°ï¼Œé»˜è®¤é€‰æ‹©å‰3ä¸ª
    ) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªåœºæ™¯çš„LLMè¿”å›ç»“æœ"""

        scenario = scenario_data['scenario']
        original_recommendations = scenario_data.get('recommendations', [])
        grading_data = result.get('recommendation_grades', {})

        # æŒ‰æ¨èç­‰çº§ç»„ç»‡æ¨èé¡¹ç›®
        graded_recommendations = {
            "highly_recommended": [],
            "recommended": [],
            "less_recommended": []
        }

        # å¤„ç†å„ç­‰çº§æ¨èé¡¹ç›®
        recommendation_levels = [
            ('highly_recommended', 'æå…¶æ¨è'),
            ('recommended', 'æ¨è'),
            ('less_recommended', 'ä¸å¤ªæ¨è')
        ]

        for level_key, level_zh in recommendation_levels:
            for rec_idx in grading_data.get(level_key, []):
                if 1 <= rec_idx <= len(original_recommendations):
                    rec_data = original_recommendations[rec_idx - 1].copy()
                    rec_data['recommendation_level'] = level_key
                    rec_data['recommendation_level_zh'] = level_zh

                    # æ·»åŠ å®Œæ•´çš„æ£€æŸ¥é¡¹ç›®ä¿¡æ¯
                    procedure = rec_data['procedure']
                    recommendation = rec_data['recommendation']

                    # æ„å»ºè¯¦ç»†çš„æ£€æŸ¥é¡¹ç›®ä¿¡æ¯
                    rec_data['procedure_details'] = {
                        'semantic_id': procedure.semantic_id,
                        'name_zh': procedure.name_zh,
                        'name_en': procedure.name_en,
                        'modality': procedure.modality,
                        'body_part': procedure.body_part,
                        'contrast_used': procedure.contrast_used,
                        'radiation_level': procedure.radiation_level,
                        'exam_duration': procedure.exam_duration,
                        'preparation_required': procedure.preparation_required,
                        'standard_code': procedure.standard_code,
                        'description_zh': procedure.description_zh
                    }

                    # æ„å»ºè¯¦ç»†çš„æ¨èä¿¡æ¯
                    rec_data['recommendation_details'] = {
                        'appropriateness_rating': recommendation.appropriateness_rating,
                        'appropriateness_category_zh': recommendation.appropriateness_category_zh,
                        'evidence_level': recommendation.evidence_level,
                        'consensus_level': recommendation.consensus_level,
                        'adult_radiation_dose': recommendation.adult_radiation_dose,
                        'pediatric_radiation_dose': recommendation.pediatric_radiation_dose,
                        'pregnancy_safety': recommendation.pregnancy_safety,
                        'contraindications': recommendation.contraindications,
                        'reasoning_zh': recommendation.reasoning_zh,
                        'special_considerations': recommendation.special_considerations
                    }

                    graded_recommendations[level_key].append(rec_data)
                else:
                    logger.warning(f"åœºæ™¯{scenario_index}çš„æ— æ•ˆ{level_zh}ç´¢å¼•: {rec_idx}")

        # è·å– final_choicesï¼ŒæŒ‰ä¼˜å…ˆçº§é¡ºåºé€‰æ‹© top_k ä¸ªé¡¹ç›®
        final_choices = []

        # æ–¹æ³•1: å¦‚æœLLMè¿”å›äº†final_choicesï¼Œä½¿ç”¨å®ƒï¼ˆä½†é™åˆ¶æ•°é‡ï¼‰
        llm_final_choices = result.get('final_choices', [])
        if llm_final_choices:
            final_choices = llm_final_choices[:top_k]  # é™åˆ¶ä¸ºtop_kä¸ª
        else:
            # æ–¹æ³•2: é™çº§æ–¹æ¡ˆ - æŒ‰æ¨èç­‰çº§ä¼˜å…ˆçº§é€‰æ‹©top_kä¸ªé¡¹ç›®
            selected_recommendations = []

            # ä¼˜å…ˆé€‰æ‹©æå…¶æ¨èçš„é¡¹ç›®
            if graded_recommendations['highly_recommended']:
                selected_count = min(top_k, len(graded_recommendations['highly_recommended']))
                selected_recommendations.extend(graded_recommendations['highly_recommended'][:selected_count])

            # å¦‚æœè¿˜ä¸å¤Ÿï¼Œè¡¥å……æ¨èçš„é¡¹ç›®
            if len(selected_recommendations) < top_k and graded_recommendations['recommended']:
                remaining_slots = top_k - len(selected_recommendations)
                additional_count = min(remaining_slots, len(graded_recommendations['recommended']))
                selected_recommendations.extend(graded_recommendations['recommended'][:additional_count])

            # å¦‚æœè¿˜ä¸å¤Ÿï¼Œè¡¥å……ä¸å¤ªæ¨èçš„é¡¹ç›®ï¼ˆé€šå¸¸ä¸æ¨èï¼Œä½†ä½œä¸ºå¤‡é€‰ï¼‰
            if len(selected_recommendations) < top_k and graded_recommendations['less_recommended']:
                remaining_slots = top_k - len(selected_recommendations)
                additional_count = min(remaining_slots, len(graded_recommendations['less_recommended']))
                selected_recommendations.extend(graded_recommendations['less_recommended'][:additional_count])

            # æå–æ£€æŸ¥é¡¹ç›®åç§°
            final_choices = [rec['procedure_details']['name_zh'] for rec in selected_recommendations]

            # å¦‚æœæ²¡æœ‰é€‰æ‹©ä»»ä½•é¡¹ç›®ï¼Œæ·»åŠ æç¤º
            if not final_choices:
                final_choices = ["æ— åˆé€‚æ¨èé¡¹ç›®"]

        # æ„å»ºè¿”å›ç»“æœ - ç»Ÿä¸€æ ¼å¼
        return {
            'comprehensive_score': result.get('comprehensive_score', 0),
            'scenario_reasoning': result.get('scenario_reasoning', ''),
            'grading_reasoning': result.get('grading_reasoning', ''),
            'overall_reasoning': '',  # å°†åœ¨å¤–å±‚ç»Ÿä¸€è®¾ç½®
            'graded_recommendations': graded_recommendations,
            'recommendation_summary': {
                'highly_recommended_count': len(graded_recommendations['highly_recommended']),
                'recommended_count': len(graded_recommendations['recommended']),
                'less_recommended_count': len(graded_recommendations['less_recommended']),
                'total_recommendations': len(original_recommendations)
            },
            'final_choices': final_choices,  # ç°åœ¨åŒ…å«æœ€å¤štop_kä¸ªé¡¹ç›®
            'scenario_metadata': {
                'scenario_id': result.get('scenario_id') or scenario.semantic_id,
                'description': scenario.description_zh,
                'panel': scenario.panel.name_zh if hasattr(scenario, 'panel') else 'æœªçŸ¥',
                'patient_population': scenario.patient_population,
                'clinical_context': scenario.clinical_context,
                'original_index': scenario_index
            }
        }

    def _fallback_single_scenario(
            self,
            scenario_data: Dict[str, Any],
            scenario_index: int,
            top_k: int = 3  # æ–°å¢top_kå‚æ•°
    ) -> Dict[str, Any]:
        """å•ä¸ªåœºæ™¯å¤„ç†çš„é™çº§æ–¹æ¡ˆ"""

        scenario = scenario_data['scenario']
        original_recommendations = scenario_data.get('recommendations', [])

        # æ„å»ºç©ºçš„æ¨èåˆ†çº§
        graded_recommendations = {
            "highly_recommended": [],
            "recommended": [],
            "less_recommended": []
        }

        # å°†æ‰€æœ‰æ¨èé¡¹ç›®æ ‡è®°ä¸º"æ¨è"ä½œä¸ºé™çº§æ–¹æ¡ˆ
        for rec_data in original_recommendations:
            rec_copy = rec_data.copy()
            rec_copy['recommendation_level'] = 'recommended'
            rec_copy['recommendation_level_zh'] = 'æ¨è'

            # æ·»åŠ è¯¦ç»†ä¿¡æ¯çš„é™çº§å¤„ç†
            procedure = rec_copy['procedure']
            recommendation = rec_copy['recommendation']

            rec_copy['procedure_details'] = {
                'semantic_id': procedure.semantic_id,
                'name_zh': procedure.name_zh,
                'name_en': procedure.name_en,
                'modality': procedure.modality,
                'body_part': procedure.body_part,
                'contrast_used': procedure.contrast_used,
                'radiation_level': procedure.radiation_level,
                'exam_duration': procedure.exam_duration,
                'preparation_required': procedure.preparation_required,
                'standard_code': procedure.standard_code,
                'description_zh': procedure.description_zh
            }

            rec_copy['recommendation_details'] = {
                'appropriateness_rating': recommendation.appropriateness_rating,
                'appropriateness_category_zh': recommendation.appropriateness_category_zh,
                'evidence_level': recommendation.evidence_level,
                'consensus_level': recommendation.consensus_level,
                'adult_radiation_dose': recommendation.adult_radiation_dose,
                'pediatric_radiation_dose': recommendation.pediatric_radiation_dose,
                'pregnancy_safety': recommendation.pregnancy_safety,
                'contraindications': recommendation.contraindications,
                'reasoning_zh': recommendation.reasoning_zh,
                'special_considerations': recommendation.special_considerations
            }

            graded_recommendations['recommended'].append(rec_copy)

        # æ„å»º final_choices é™çº§æ–¹æ¡ˆï¼Œé€‰æ‹©å‰top_kä¸ª
        final_choices = []
        if original_recommendations:
            # é€‰æ‹©å‰top_kä¸ªæ¨èé¡¹ç›®ä½œä¸ºæœ€ç»ˆé€‰æ‹©
            selected_count = min(top_k, len(original_recommendations))
            final_choices = [original_recommendations[i]['procedure'].name_zh
                             for i in range(selected_count)]

        return {
            'comprehensive_score': 50,  # é»˜è®¤ä¸­ç­‰è¯„åˆ†
            'scenario_reasoning': 'ç³»ç»Ÿé™çº§å¤„ç†ï¼šæ— æ³•è·å–è¯¦ç»†åˆ†æ',
            'grading_reasoning': 'ç³»ç»Ÿé™çº§å¤„ç†ï¼šæ‰€æœ‰æ¨èé¡¹ç›®æ ‡è®°ä¸ºæ¨èçº§åˆ«',
            'overall_reasoning': '',
            'graded_recommendations': graded_recommendations,
            'recommendation_summary': {
                'highly_recommended_count': 0,
                'recommended_count': len(original_recommendations),
                'less_recommended_count': 0,
                'total_recommendations': len(original_recommendations)
            },
            'final_choices': final_choices,
            'scenario_metadata': {
                'scenario_id': scenario.semantic_id,
                'description': scenario.description_zh,
                'panel': scenario.panel.name_zh if hasattr(scenario, 'panel') else 'æœªçŸ¥',
                'patient_population': scenario.patient_population,
                'clinical_context': scenario.clinical_context,
                'original_index': scenario_index
            }
        }

    async def _generate_overall_reasoning(self,
                                          patient_info,
                                          clinical_context,
                                          max_recommendations_per_scenario,
                                          final_results: List[Dict[str, Any]]):
        """ç”Ÿæˆæ€»ä½“æ¨ç†è¯´æ˜"""
        patient_info_content = self.build_patient_context(patient_info)
        clinical_context_content = self.build_clinical_context(clinical_context)
        choices_content = ""
        a = []
        for choices in final_results:
            if isinstance(choices, list) and choices:
                a.extend(choices)
        choices_content = "\n".join(a)
        task_content = f"""
                          ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦å½±åƒä¸“å®¶ï¼Œè¯·ä½ æ ¹æ®æä¾›ç»™ä½ çš„åŒ»å­¦å½±åƒæ¨èé¡¹ç›®ä»¥åŠæ‚£è€…çš„ä¿¡æ¯å’Œä¸´åºŠä¸Šä¸‹æ–‡ï¼Œé€‰æ‹©{max_recommendations_per_scenario}ä¸ªæœ€é€‚åˆè¯¥ç—…äººçš„åŒ»å­¦å½±åƒæ¨èé¡¹ç›®
                          è¿™æ˜¯æ‚£è€…çš„ä¿¡æ¯ï¼š
                                 {patient_info_content}
                          è¿™æ˜¯ä¸´åºŠä¸Šä¸‹æ–‡ï¼š
                                 {clinical_context_content}
                          è¿™æ˜¯å¯¹åº”çš„åŒ»å­¦å½±åƒæ¨èï¼š
                                  {choices_content}               

                          è¯·ä½ åŠ¡å¿…é€‰æ‹©{max_recommendations_per_scenario}ä¸ªæ¨èé¡¹ç›®ã€‚
                          è¿™æ˜¯è¾“å‡ºæ ¼å¼
                          {{
                             "final_choices":[è¿™é‡Œæ˜¯é€‰æ‹©çš„{max_recommendations_per_scenario}ä¸ªåŒ»å­¦å½±åƒæ¨èé¡¹ç›®]
                             "overall_reason":..

                          }}
                          åŠ¡å¿…ä»¥jsonæ ¼å¼è¾“å‡ºï¼


                """
        response = await self.ai_service._call_llm(task_content)
        try:
            res = safe_parse_llm_response(response)
        except Exception as e:
            logger.info(f"è§£æjsonå‡ºé”™ï¼š{e}")

        choices = res.get("final_choices", [])
        reason = res.get("overall_reason", "")
        return {"final_choices": choices, "overall_reason": reason}

    def _fallback_for_confirmed_scenarios(self, confirmed_scenarios: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é™çº§æ–¹æ¡ˆ"""
        logger.info("ä½¿ç”¨é™çº§æ–¹æ¡ˆ...")
        return []

    def get_strategy_status(self) -> Dict[str, Any]:
        """è·å–ç­–ç•¥çŠ¶æ€"""
        status = {
            'environment': self.environment,
            'use_adaptive': self.use_adaptive,
            'threshold_config': self.strategy.threshold_config,
            'strategy_type': self.strategy.__class__.__name__
        }

        if isinstance(self.strategy, LearningThresholdStrategy):
            status.update({
                'learning_enabled': self.strategy.learning_enabled,
                'history_size': len(self.strategy.performance_history)
            })

        return status

    def set_adaptive_mode(self, enabled: bool):
        """åŠ¨æ€è®¾ç½®è‡ªé€‚åº”æ¨¡å¼"""
        old_mode = self.use_adaptive
        self.use_adaptive = enabled

        if enabled and not isinstance(self.strategy, LearningThresholdStrategy):
            self.strategy = LearningThresholdStrategy()
            self._initialize_strategy()
            logger.info("ğŸ”„ åˆ‡æ¢åˆ°è‡ªé€‚åº”å­¦ä¹ æ¨¡å¼")
        elif not enabled and isinstance(self.strategy, LearningThresholdStrategy):
            self.strategy = AdaptiveThresholdStrategy()
            self._initialize_strategy()
            logger.info("âš¡ åˆ‡æ¢åˆ°å›ºå®šé˜ˆå€¼æ¨¡å¼")

        logger.info(f"ğŸ“ è‡ªé€‚åº”æ¨¡å¼ä» {old_mode} æ”¹ä¸º {enabled}")

    def enable_learning(self, enabled: bool = True):
        """å¯ç”¨/ç¦ç”¨å­¦ä¹ åŠŸèƒ½ï¼ˆä»…å¯¹å­¦ä¹ ç­–ç•¥æœ‰æ•ˆï¼‰"""
        if isinstance(self.strategy, LearningThresholdStrategy):
            self.strategy.learning_enabled = enabled
            status = "å¯ç”¨" if enabled else "ç¦ç”¨"
            logger.info(f"ğŸ“š å­¦ä¹ åŠŸèƒ½å·²{status}")
        else:
            logger.warning("å½“å‰ä¸æ˜¯å­¦ä¹ ç­–ç•¥ï¼Œæ— æ³•å¯ç”¨å­¦ä¹ åŠŸèƒ½")

    def reset_learning(self):
        """é‡ç½®å­¦ä¹ æ•°æ®"""
        if isinstance(self.strategy, LearningThresholdStrategy):
            self.strategy.performance_history = []
            logger.info("ğŸ”„ å­¦ä¹ æ•°æ®å·²é‡ç½®")
        else:
            logger.warning("å½“å‰ä¸æ˜¯å­¦ä¹ ç­–ç•¥ï¼Œæ— æ³•é‡ç½®å­¦ä¹ æ•°æ®")

    async def _get_recommendations_single_call_by_llm(self, confirmed_scenarios,
                                                       single_prompt,direct_return):

            try:

                response = await self.ai_service._call_llm(single_prompt)

                if not direct_return:


                    # ä½¿ç”¨å¢å¼ºçš„JSONè§£æ
                    result = safe_parse_llm_response(response=response, expected_scenario_count=len(confirmed_scenarios))

                    if result is None:
                        logger.error("JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é™çº§æ–¹æ¡ˆ")
                        return self._fallback_for_confirmed_scenarios(confirmed_scenarios)

                    # å¤„ç†é€‰ä¸­çš„åœºæ™¯æ•°æ®
                    selected_scenarios_data = result.get('selected_scenarios', [])
                    overall_choices= result.get('overall_choices', ''),
                    final_results = []

                    for selected_data in selected_scenarios_data:
                        scenario_index = selected_data.get('scenario_index')
                        scenario_id = selected_data.get('scenario_id')
                        grading_data = selected_data.get('recommendation_grades', {})
                        # éªŒè¯åœºæ™¯ç´¢å¼•èŒƒå›´
                        if not (1 <= scenario_index <= len(confirmed_scenarios)):
                            logger.warning(f"æ— æ•ˆçš„åœºæ™¯ç´¢å¼•: {scenario_index}ï¼Œè·³è¿‡è¯¥åœºæ™¯")
                            continue

                        # è·å–åŸå§‹åœºæ™¯æ•°æ®
                        original_scenario_data = confirmed_scenarios[scenario_index - 1]
                        original_recommendations = original_scenario_data.get('recommendations', [])
                        scenario = original_scenario_data['scenario']

                        # å®‰å…¨å¤„ç†æ¨èåˆ†çº§
                        graded_recommendations = safe_process_recommendation_grades(
                            grading_data, original_recommendations, scenario_index
                        )

                        # æ„å»ºè¿”å›ç»“æœ
                        final_result = {
                            'comprehensive_score': selected_data.get('comprehensive_score', 0),
                            'scenario_reasoning': selected_data.get('scenario_reasoning', ''),
                            'grading_reasoning': selected_data.get('grading_reasoning', ''),
                            'graded_recommendations': graded_recommendations,
                            'recommendation_summary': {
                                'highly_recommended_count': len(graded_recommendations['highly_recommended']),
                                'recommended_count': len(graded_recommendations['recommended']),
                                'less_recommended_count': len(graded_recommendations['less_recommended']),
                                'total_recommendations': len(original_recommendations)
                            },
                            'scenario_metadata': {
                                'scenario_id': scenario_id or scenario.semantic_id,
                                'description': scenario.description_zh,
                                'panel': scenario.panel.name_zh if hasattr(scenario, 'panel') else 'æœªçŸ¥',
                                'patient_population': scenario.patient_population,
                                'clinical_context': scenario.clinical_context,
                                'original_index': scenario_index
                            }
                        }

                        final_results.append(final_result)

                    # æŒ‰ç»¼åˆè¯„åˆ†æ’åº
                    final_results.sort(key=lambda x: x['comprehensive_score'], reverse=True)

                    logger.info(f"âœ… å•æ¬¡è°ƒç”¨å®Œæˆï¼ŒæˆåŠŸå¤„ç†{len(final_results)}ä¸ªåœºæ™¯")
                    return {"result":final_results,"overall_choices":overall_choices,'overall_reasoning':result.get('overall_reasoning', '')}
                else:
                    return response
            except Exception as e:
                logger.error(f"âŒ å•æ¬¡è°ƒç”¨å¤±è´¥: {str(e)}")
                if not direct_return:
                    return self._fallback_for_confirmed_scenarios(confirmed_scenarios)
                return "æ‰§è¡Œå‡ºé”™"



